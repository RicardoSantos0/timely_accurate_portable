{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 2.1 Data Understanding and Preprocessing of Support Tables\n",
    "\n",
    "For all intents and purposes, this should be considered as the second real notebook that is part of the thesis work. In it, we will look at the support tables that are part of the NOVA IMS original database.\n",
    "\n",
    "#### 1. We are familiarized with the general structure of the logs\n",
    "\n",
    "Before going further, we should assess the remaining tables presented in the database. \n",
    "\n",
    "Recall, **logs record interactions with the system and we are looking for ways to determine whether these interactions can assist educators identify at risk students and high performing students.**\n",
    "\n",
    "Thus, to make the best out of the logs, we will need to perform different segmentations and it is likely that we will need perform some filtering. \n",
    "\n",
    "### To do that, we will take a look at all tables\n",
    "\n",
    "We will look at all tables and all columns to make a preliminary assessment of the utility of the available elements.\n",
    "In general, these are support elements that will be used sparsely, as most of the relevant information is present in the logs.\n",
    "\n",
    "The observation of each table will resort to the same chain of commands:\n",
    "\n",
    "info -> to observe count and datatype of each column, \n",
    "describe -> a command that that returns the most notable descriptive statistics of each column.\n",
    "The obeservation of each table ends with a look at the raw data (At least the visible rows).\n",
    "\n",
    "#### 2. We'll start this notebook by importing all relevant packages and data\n",
    "\n",
    "All data is stored in an excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.tseries.offsets import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other tables with support information\n",
    "support_table = pd.read_excel('../Data/Nova_IMS_logs_Moodle_cursos.xlsx', sheet_name = None,\n",
    "                             dtype = {\n",
    "                                 'cd_lectivo' : object,\n",
    "                                 'cd_curso' : object,\n",
    "                                 'cd_Discip': object,\n",
    "                                 'cd_discip': object,\n",
    "                                 'userId': object,\n",
    "                                 'dataExame': pd.datetime,\n",
    "                             })\n",
    "\n",
    "support_table['logs_Moodle_cursos'].rename(columns = {\n",
    "                                 'cd_Discip' : 'courseid',\n",
    "                                 'userId' : 'userid',\n",
    "                             }, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this cell to write any additional piece of code that may be required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the support table dict is composed by 2 distinct tables.\n",
    "\n",
    "Student performance is, in general, measured by the student's grade. So... how do we measure grades?\n",
    "In our data, we have access to different grades - Exam/assignment and Final Grades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table['logs_Moodle_cursos'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table['logs_Moodle_cursos'].describe(include = 'all', datetime_is_numeric = True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by removing identifiers of courses and programs. These will will provide no additional information for our analysis. Additionally, as all of the logs refer to the same school year, the reference to them will promptly be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd Lectivo is a single value column ~- other meaningless columns may also be\n",
    "support_table['logs_Moodle_cursos'].drop(['cd_lectivo', 'nm_curso_pt', 'nm_ramo', 'ds_discip_pt'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going forward, let us take a closer look at some columns - specifically the unique values each of these columns may take:\n",
    "1. Semestre: Will provide valuable insights into how to split the data, \n",
    "2. statusAvaliação: Teels us to which phase a grade refers to,\n",
    "3. statusEpoca: Refers to the grading status of a particular item,\n",
    "4. statusFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store columns of interest in list\n",
    "columns_of_interest = ['semestre', 'statusAvaliacao', 'statusEpoca', 'statusFinal']\n",
    "\n",
    "#print value counts of each of them:\n",
    "for i in tqdm(columns_of_interest):\n",
    "    print(f'Unique Values of Column {i}: \\n')\n",
    "    print(support_table['logs_Moodle_cursos'][i].value_counts())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know a lot of different things.\n",
    "\n",
    "**Next, we have the datasExames table**\n",
    "\n",
    "This table stores important information concerning the different curricular units and their exam dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table['datasExames'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table['datasExames'].describe(include = 'all', datetime_is_numeric = True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd Lectivo is a single value column - that is likely to refer to propbably referes\n",
    "support_table['datasExames'] = support_table['datasExames'].drop(['cd_lectivo'], axis = 1).rename(columns = {'cd_discip': 'courseid'})\n",
    "support_table['datasExames']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before going forward, we have to consider the following:\n",
    "\n",
    "In this instance, all courses have grades. Be it exam grade or final grade. As a first indicator, we will not consider grade improvements. \n",
    "\n",
    "These results come from students that have effectively completed the curricular unit. Likewise, we will not consider grades of special season - Reason being that these exams are only accessible to students that fulfill very strict conditions.\n",
    "\n",
    "In this instance, it does not make sense to distinguish between mandatory and optional assignments. We will keep the different assignments listed in the hope we can associate an assignment to a specific time-stamp.\n",
    "\n",
    "Additionally, we can already start to address course duration:\n",
    "1. We need a start date and an end date.\n",
    "2. Moodle Logs usually have a start date for the course: It is unclear, at this moment, whether the date presented therein is reflective of the actual start date or the course registry date. Regardless, it may possible for us to use the semester denomination to make a reasonable inference for duration using the weeks of start and finish.\n",
    "3. The end date is given by the Normal exam data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#we create a list of items to remove from the logs\n",
    "invalid_keys = ['Melhoria - Nota Parcial 1', #partial for improv\n",
    "                'Melhoria - Nota Parcial 2', #partial 2\n",
    "                'Melhoria - Nota Parcial 3', #partial 3          \n",
    "                'Melhoria - Nota Parcial 4', #...\n",
    "                'Melhoria - Nota Parcial 5',          \n",
    "                'Melhoria - Nota Parcial 6',          \n",
    "                'Época Especial', #special season\n",
    "                'Estatuto especial - Exame 1', #extraordinarily special season exam        \n",
    "                'Estatuto Especial - Exame 2', #extraordinarily special season exam 2 \n",
    "                'Melhoria (1ª Época)', #improv season 1\n",
    "                'Melhoria (2ª época)', #improv season 2\n",
    "                'Creditação', #credits with grade season\n",
    "               ]\n",
    "\n",
    "#we remove the entries from grades\n",
    "support_table['logs_Moodle_cursos'] = support_table['logs_Moodle_cursos'][~(support_table['logs_Moodle_cursos']['statusAvaliacao'].isin(invalid_keys))].reset_index(drop = True)\n",
    "\n",
    "#for course duration, we will only care about the date of the first season - as it is more indicative of duration than other options\n",
    "normal_season = ['Exame Época Final Normal', 'Normal']\n",
    "\n",
    "#and only keep normal season dates\n",
    "support_table['datasExames'] = support_table['datasExames'][support_table['datasExames']['epocaAvaliacao'].isin(normal_season)].reset_index(drop = True)\n",
    "\n",
    "#first, make distinction between passes and fails \n",
    "support_table['datasExames']['epocaAvaliacao'] = np.where(support_table['datasExames']['epocaAvaliacao'] == 'Exame Época Final Normal',\n",
    "                                                             'Normal', #getting rid of multiclassification\n",
    "                                                             support_table['datasExames']['epocaAvaliacao'])\n",
    "\n",
    "support_table['datasExames'].sort_values(by = ['semestre', 'courseid', 'dataExame']).drop_duplicates().reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our main target will be the exam grade** -> reason being that it is not directly computed from the grades of the different assignments. Whereas finalgrade results from these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, make distinction between passes and fails \n",
    "support_table['logs_Moodle_cursos']['statusEpoca'] = np.where(support_table['logs_Moodle_cursos']['statusEpoca'] != 'Aprovado',\n",
    "                                                             'Reprovado', #getting rid of multiclassification\n",
    "                                                             support_table['logs_Moodle_cursos']['statusEpoca'])\n",
    "\n",
    "#first, make distinction between passes and fails \n",
    "support_table['logs_Moodle_cursos']['statusAvaliacao'] = np.where(support_table['logs_Moodle_cursos']['statusAvaliacao'] == 'Exame Época Final Normal',\n",
    "                                                             'Normal', #getting rid of multiclassification\n",
    "                                                             support_table['logs_Moodle_cursos']['statusAvaliacao'])\n",
    "\n",
    "support_table['logs_Moodle_cursos'] = support_table['logs_Moodle_cursos'].sort_values(by = ['semestre', 'courseid', 'userid', 'statusAvaliacao', 'notaAvaliacao'])\n",
    "support_table['logs_Moodle_cursos'].drop_duplicates(subset = ['semestre', 'courseid', 'userid', 'statusAvaliacao'], inplace = True)\n",
    "\n",
    "support_table['logs_Moodle_cursos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exames = ['Normal', 'Recurso']\n",
    "\n",
    "#first, split exams from remaining rows\n",
    "exams = support_table['logs_Moodle_cursos'][support_table['logs_Moodle_cursos']['statusAvaliacao'].isin(exames)].filter(['cd_curso', 'courseid', 'semestre','userid', \n",
    "                                                                                                            'statusAvaliacao', 'notaAvaliacao',\n",
    "                                                                                                           'notaFinal'])\n",
    "\n",
    "#then, create a pivot table with the normal + recurso season exam grades \n",
    "exam_pivot = pd.pivot(exams, index = ['cd_curso', 'courseid', 'semestre','userid'], columns = 'statusAvaliacao', \n",
    "                 values = 'notaAvaliacao')\n",
    "\n",
    "exam_pivot.dropna(how = 'all', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We look at the exams at Normal and Recurso Season\n",
    "\n",
    "We have 1341 student /course pairs that have take the 2nd season exam. \n",
    "We also have 329 student who have not taken the 1st season exam.\n",
    "\n",
    "We can take a simple approach - fill all nans in Normal exam with the Recurso grade.\n",
    "\n",
    "For that, we will check whether both dfs are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is a relic from a previous formulation of the problem\n",
    "\n",
    "# #we need to look for instances of nans in normal and not nas in recurso\n",
    "# normal_nans = exam_pivot[exam_pivot['Normal'].isna()]\n",
    "# valid_recurso = exam_pivot[exam_pivot['Recurso'].notna()]\n",
    "\n",
    "# print(f'Are both dataframes exactly the same? \\n' +\n",
    "#       f'Answer: {normal_nans.equals(valid_recurso)}.')\n",
    "\n",
    "# #del normal_nans, valid_recurso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In light of the previous result**, we will look to join both columns together and get a valid targets_table - with exam grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fillnas with the recurso \n",
    "exam_pivot['Normal'].fillna(exam_pivot['Recurso'], inplace = True)\n",
    "\n",
    "#drop recurso table and rename column to describe what it refers to - an exam_mark\n",
    "exam_pivot = exam_pivot.drop('Recurso', axis = 1).rename(columns = {'Normal': 'exam_mark'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we remove useless columns and rename nota_final to final_mark\n",
    "exams = exams.drop(['statusAvaliacao', 'notaAvaliacao'], axis = 1).rename(columns = {'notaFinal' : 'final_mark'})\n",
    "exams = pd.merge(exams, exam_pivot, on = ['courseid', 'semestre', 'userid'], how = 'inner')\n",
    "\n",
    "#we finish by dropping the rows that have no final_mark\n",
    "exams = exams.drop_duplicates().fillna(0).reset_index(drop = True)\n",
    "exams['courseid'], exams['userid'] = exams['courseid'].astype(object), exams['userid'].astype(object)\n",
    "\n",
    "exams.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We now have a table with both of our potential targets**.\n",
    "\n",
    "Now, we need to go back to our main support table. In this table, we now deal with the remaining assignments - that is, the ones that are identified as N.\n",
    "\n",
    "One of the first issues is that we have no means to identify each assignment. We can start by giving every semester-course-status a unique identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, split exams from remaining rows\n",
    "assignments = support_table['logs_Moodle_cursos'][~(support_table['logs_Moodle_cursos']['statusAvaliacao'].isin(exames))].filter(['cd_curso','courseid', 'semestre','userid','statusAvaliacao', 'cd_final','notaAvaliacao'])\n",
    "\n",
    "#then we get all unique entries and assign them an index number\n",
    "assign_id = assignments[['cd_curso', 'semestre', 'courseid', 'statusAvaliacao']].drop_duplicates().reset_index(drop = True).reset_index()\n",
    "\n",
    "#then, we create a dict using the combination of index, courseid and status as keys\n",
    "assign_id = assign_id.set_index(['cd_curso', 'semestre', 'courseid', 'statusAvaliacao']).to_dict()['index']\n",
    "\n",
    "#set index of df to match same index\n",
    "assignments.set_index(['cd_curso', 'semestre', 'courseid', 'statusAvaliacao'], drop = True, inplace = True)\n",
    "\n",
    "#use index as key for dict\n",
    "assignments['assign_id'] = assignments.index.map(assign_id)\n",
    "\n",
    "#reset_index\n",
    "assignments = assignments.reset_index().drop(['cd_final'], axis = 1)\n",
    "assignments['assign_id'], assignments['courseid'] = assignments['assign_id'].astype(object), assignments['courseid'].astype(object)\n",
    "\n",
    "#before finishing, we still need to make sure we only consider students for which there are, at least, examgrades\n",
    "assignments = pd.merge(assignments, exams[['cd_curso','courseid', 'semestre', 'userid']], on = ['cd_curso','courseid', 'semestre', 'userid'], how = 'right')\n",
    "\n",
    "#convert both to objects\n",
    "assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now what?\n",
    "\n",
    "At the start of this notebook, we aimed at a couple of things:\n",
    "\n",
    "1. Student Performance -> we will either use exam_mark or the final_mark for this\n",
    "2. The duration of the course -> for this we need the start date and the end date of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As a final step, we will store the start date of each course to the support table.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform inner join between support table and courses with grades\n",
    "support_table = pd.merge(assignments, support_table['datasExames'], on = ['semestre','courseid'], how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will only touch the course Start Date in the next notebook.**\n",
    "\n",
    "For now, we will make the following concessions concerning end-date:\n",
    "\n",
    "1. For every Semester class: S1, S2, T1, T2, T3 and T4, we will find the mean date (which corresponds to the mean of the dates of the normal exam).\n",
    "\n",
    "2. All courses in each semester class will have an end-date that is equal to the Friday of the week in question.\n",
    "\n",
    "This will ensure that all courses in the class have the same duration - which will ease our work later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then, we cumulative sum all in-group members \n",
    "support_table['end_date'] = pd.to_datetime((support_table.groupby('semestre')['dataExame'].transform(pd.Series.mean)).dt.date)\n",
    "\n",
    "#setting up duration threshold to be on friday -> weekday 4\n",
    "support_table['end_date'] = support_table['end_date'].where( support_table['end_date'] == (( support_table['end_date'] + Week(weekday=4)) - Week()), support_table['end_date'] + Week(weekday=4))\n",
    "\n",
    "#this also allows us to remove the pesky columns that serve no additional purpose\n",
    "support_table = support_table.drop(['epocaAvaliacao', 'dataExame'], axis = 1).drop_duplicates().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an end-date that is suitable - calculated as the mean date of the Normal season exams of the trimester-semester the course is part of.\n",
    "However, we do not have, yet, a start date.\n",
    "\n",
    "The logs have start and end dates that are, at the very least, unreliable. A way to address the issue is to look at the curricular unit start dates of MDSAA (which I attended) and extrapolate those start weeks.\n",
    "\n",
    "As such, we will pre-emptively give courses the following Start Dates:\n",
    "\n",
    "T1 and S1: Week from 7 of September to 13th of September 2020\n",
    "\n",
    "T2: 2nd of November 2020\n",
    "\n",
    "S2 and T3: Week February 8th 2021\n",
    "\n",
    "T4: 12th of April\n",
    "\n",
    "**As such, we will go forward with creating a suitable dict to reflect the expected start dates.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Step 1, create dict\n",
    "startdates = {\n",
    "            'T1' : pd.to_datetime('2020-09-07'),\n",
    "            'S1' : pd.to_datetime('2020-09-07'),\n",
    "            'T2' : pd.to_datetime('2020-11-02'),\n",
    "            'T3' : pd.to_datetime('2021-02-08'),\n",
    "            'S2' : pd.to_datetime('2021-02-08'),\n",
    "            'T4' : pd.to_datetime('2021-04-12'),\n",
    "            }\n",
    "                                  \n",
    "#step 2: assign start date to course\n",
    "support_table['startdate'] = support_table['semestre'].map(startdates)\n",
    "support_table.rename(columns = {'notaAvaliacao' : 'assignment_mark'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exams.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 2 distinct table that will be invaluable for our work in future notebooks.\n",
    "\n",
    "targets_table has stored every final mark obtain by each student attending the different courses of the university:\n",
    "\n",
    "- From final_mark or exam_mar, we finally are able to calculate our target variables:\n",
    "    - We can label students as at-risk or as overachievers depending on their mark,\n",
    "\n",
    "\n",
    "- From support_table, we will need more robust sets of information to be used for feature extraction and engineering:\n",
    "    - The endate and the expected startdate of each course,\n",
    "    - The individual mark of each assignment - tbd if usable on a later stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save tables \n",
    "exams.to_csv('../Data/Modeling Stage/Nova_IMS_targets_table.csv') \n",
    "\n",
    "support_table.drop(['statusAvaliacao'], axis = 1).to_csv('../Data/Nova_IMS_support_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Done for now\n",
    "\n",
    "In notebook 2.2. we will rely on the activity logs and our support table to perform the necessary filtering and preprocessing of the data in order to make it compliant with our necessities. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
