{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 2.1 Data Understanding and Preprocessing of Support Tables\n",
    "\n",
    "For all intents and purposes, this should be considered as the second real notebook that is part of the thesis work. In it, we will look at the support tables that are part of the NOVA IMS original database.\n",
    "\n",
    "#### 1. We are familiarized with the general structure of the logs\n",
    "\n",
    "Before going further, we should assess the remaining tables presented in the database. \n",
    "\n",
    "Recall, **logs record interactions with the system and we are looking for ways to determine whether these interactions can assist educators identify at risk students and high performing students.**\n",
    "\n",
    "Thus, to make the best out of the logs, we will need to perform different segmentations and it is likely that we will need perform some filtering. \n",
    "\n",
    "### To do that, we will take a look at all tables\n",
    "\n",
    "We will look at all tables and all columns to make a preliminary assessment of the utility of the available elements.\n",
    "In general, these are support elements that will be used sparsely, as most of the relevant information is present in the logs.\n",
    "\n",
    "The observation of each table will resort to the same chain of commands:\n",
    "\n",
    "info -> to observe count and datatype of each column, \n",
    "describe -> a command that that returns the most notable descriptive statistics of each column.\n",
    "The obeservation of each table ends with a look at the raw data (At least the visible rows).\n",
    "\n",
    "#### 2. We'll start this notebook by importing all relevant packages and data\n",
    "\n",
    "All data is stored in an excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.tseries.offsets import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other tables with support information\n",
    "support_table = pd.read_excel('../Data/Nova_IMS_logs_Moodle_cursos.xlsx', sheet_name = None,\n",
    "                             dtype = {\n",
    "                                 'cd_lectivo' : object,\n",
    "                                 'cd_curso' : object,\n",
    "                                 'cd_Discip': object,\n",
    "                                 'cd_discip': object,\n",
    "                                 'userId': object,\n",
    "                                 'dataExame': pd.datetime,\n",
    "                             })\n",
    "\n",
    "support_table['logs_Moodle_cursos'].rename(columns = {\n",
    "                                 'cd_Discip' : 'courseid',\n",
    "                                 'userId' : 'userid',\n",
    "                             }, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this cell to write any additional piece of code that may be required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the support table dict is composed by 2 distinct tables.\n",
    "\n",
    "Student performance is, in general, measured by the student's grade. So... how do we measure grades?\n",
    "In our data, we have access to different grades - Exam/assignment and Final Grades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table['logs_Moodle_cursos'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table['logs_Moodle_cursos'].describe(include = 'all', datetime_is_numeric = True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by removing identifiers of courses and programs. These will will provide no additional information for our analysis. Additionally, as all of the logs refer to the same school year, the reference to them will promptly be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd Lectivo is a single value column ~- other meaningless columns may also be\n",
    "support_table['logs_Moodle_cursos'].drop(['cd_lectivo', 'nm_curso_pt', 'nm_ramo', 'ds_discip_pt'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going forward, let us take a closer look at some columns - specifically the unique values each of these columns may take:\n",
    "1. Semestre: Will provide valuable insights into how to split the data, \n",
    "2. statusAvaliação: Teels us to which phase a grade refers to,\n",
    "3. statusEpoca: Refers to the grading status of a particular item,\n",
    "4. statusFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store columns of interest in list\n",
    "columns_of_interest = ['semestre', 'statusAvaliacao', 'statusEpoca', 'statusFinal']\n",
    "\n",
    "#print value counts of each of them:\n",
    "for i in tqdm(columns_of_interest):\n",
    "    print(f'Unique Values of Column {i}: \\n')\n",
    "    print(support_table['logs_Moodle_cursos'][i].value_counts())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know a lot of different things.\n",
    "\n",
    "**Next, we have the datasExames table**\n",
    "\n",
    "This table stores important information concerning the different curricular units and their exam dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table['datasExames'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table['datasExames'].describe(include = 'all', datetime_is_numeric = True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd Lectivo is a single value column - that is likely to refer to propbably referes\n",
    "support_table['datasExames'] = support_table['datasExames'].drop(['cd_lectivo'], axis = 1).rename(columns = {'cd_discip': 'courseid'})\n",
    "support_table['datasExames']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before going forward, we have to consider the following:\n",
    "\n",
    "In this instance, all courses have grades. Be it exam grade or final grade. As a first indicator, we will not consider grade improvements. \n",
    "\n",
    "These results come from students that have effectively completed the curricular unit. Likewise, we will not consider grades of special season - Reason being that these exams are only accessible to students that fulfill very strict conditions.\n",
    "\n",
    "In this instance, it does not make sense to distinguish between mandatory and optional assignments. We will keep the different assignments listed in the hope we can associate an assignment to a specific time-stamp.\n",
    "\n",
    "Additionally, we can already start to address course duration:\n",
    "1. We need a start date and an end date.\n",
    "2. Moodle Logs usually have a start date for the course: It is unclear, at this moment, whether the date presented therein is reflective of the actual start date or the course registry date. Regardless, it may possible for us to use the semester denomination to make a reasonable inference for duration using the weeks of start and finish.\n",
    "3. The end date is given by the Normal exam data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#we create a list of items to remove from the logs\n",
    "invalid_keys = ['Melhoria - Nota Parcial 1', #partial for improv\n",
    "                'Melhoria - Nota Parcial 2', #partial 2\n",
    "                'Melhoria - Nota Parcial 3', #partial 3          \n",
    "                'Melhoria - Nota Parcial 4', #...\n",
    "                'Melhoria - Nota Parcial 5',          \n",
    "                'Melhoria - Nota Parcial 6',          \n",
    "                'Época Especial', #special season\n",
    "                'Estatuto especial - Exame 1', #extraordinarily special season exam        \n",
    "                'Estatuto Especial - Exame 2', #extraordinarily special season exam 2 \n",
    "                'Melhoria (1ª Época)', #improv season 1\n",
    "                'Melhoria (2ª época)', #improv season 2\n",
    "                'Creditação', #credits with grade season\n",
    "               ]\n",
    "\n",
    "#we remove the entries from grades\n",
    "support_table['logs_Moodle_cursos'] = support_table['logs_Moodle_cursos'][~(support_table['logs_Moodle_cursos']['statusAvaliacao'].isin(invalid_keys))].reset_index(drop = True)\n",
    "\n",
    "#for course duration, we will only care about the date of the first season - as it is more indicative of duration than other options\n",
    "normal_season = ['Exame Época Final Normal', 'Normal']\n",
    "\n",
    "#and only keep normal season dates\n",
    "support_table['datasExames'] = support_table['datasExames'][support_table['datasExames']['epocaAvaliacao'].isin(normal_season)].reset_index(drop = True)\n",
    "\n",
    "#first, make distinction between passes and fails \n",
    "support_table['datasExames']['epocaAvaliacao'] = np.where(support_table['datasExames']['epocaAvaliacao'] == 'Exame Época Final Normal',\n",
    "                                                             'Normal', #getting rid of multiclassification\n",
    "                                                             support_table['datasExames']['epocaAvaliacao'])\n",
    "\n",
    "support_table['datasExames'].sort_values(by = ['semestre', 'courseid', 'dataExame']).drop_duplicates().reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table['datasExames']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our main target will be the exam grade** -> reason being that it is not directly computed from the grades of the different assignments. Whereas finalgrade results from these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, make distinction between passes and fails \n",
    "support_table['logs_Moodle_cursos']['statusEpoca'] = np.where(support_table['logs_Moodle_cursos']['statusEpoca'] != 'Aprovado',\n",
    "                                                             'Reprovado', #getting rid of multiclassification\n",
    "                                                             support_table['logs_Moodle_cursos']['statusEpoca'])\n",
    "\n",
    "#first, make distinction between passes and fails \n",
    "support_table['logs_Moodle_cursos']['statusAvaliacao'] = np.where(support_table['logs_Moodle_cursos']['statusAvaliacao'] == 'Exame Época Final Normal',\n",
    "                                                             'Normal', #getting rid of multiclassification\n",
    "                                                             support_table['logs_Moodle_cursos']['statusAvaliacao'])\n",
    "\n",
    "support_table['logs_Moodle_cursos'] = support_table['logs_Moodle_cursos'].sort_values(by = ['semestre', 'courseid', 'userid', 'statusAvaliacao', 'notaAvaliacao'])\n",
    "support_table['logs_Moodle_cursos'].drop_duplicates(subset = ['semestre', 'courseid', 'userid', 'statusAvaliacao'], inplace = True)\n",
    "\n",
    "support_table['logs_Moodle_cursos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exames = ['Normal', 'Recurso']\n",
    "\n",
    "#first, split exams from remaining rows\n",
    "exams = support_table['logs_Moodle_cursos'][support_table['logs_Moodle_cursos']['statusAvaliacao'].isin(exames)].filter(['courseid', 'semestre','userid', \n",
    "                                                                                                            'statusAvaliacao', 'notaAvaliacao',\n",
    "                                                                                                           'notaFinal'])\n",
    "\n",
    "#then, create a pivot table with the normal + recurso season exam grades \n",
    "exam_pivot = pd.pivot(exams, index = ['courseid', 'semestre','userid'], columns = 'statusAvaliacao', \n",
    "                 values = 'notaAvaliacao')\n",
    "\n",
    "exam_pivot.dropna(how = 'all', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We look at the exams at Normal and Recurso Season\n",
    "\n",
    "We have 1341 student /course pairs that have take the 2nd season exam. \n",
    "We also have 329 student who have not taken the 1st season exam.\n",
    "\n",
    "We can take a simple approach - fill all nans in Normal exam with the Recurso grade.\n",
    "\n",
    "For that, we will check whether both dfs are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is a relic from a previous formulation of the problem\n",
    "\n",
    "# #we need to look for instances of nans in normal and not nas in recurso\n",
    "# normal_nans = exam_pivot[exam_pivot['Normal'].isna()]\n",
    "# valid_recurso = exam_pivot[exam_pivot['Recurso'].notna()]\n",
    "\n",
    "# print(f'Are both dataframes exactly the same? \\n' +\n",
    "#       f'Answer: {normal_nans.equals(valid_recurso)}.')\n",
    "\n",
    "# #del normal_nans, valid_recurso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In light of the previous result**, we will look to join both columns together and get a valid targets_table - with exam grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fillnas with the recurso \n",
    "exam_pivot['Normal'].fillna(exam_pivot['Recurso'], inplace = True)\n",
    "\n",
    "#drop recurso table and rename column to describe what it refers to - an exam_mark\n",
    "exam_pivot = exam_pivot.drop('Recurso', axis = 1).rename(columns = {'Normal': 'exam_mark'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we remove useless columns and rename nota_final to final_mark\n",
    "exams = exams.drop(['statusAvaliacao', 'notaAvaliacao'], axis = 1).rename(columns = {'notaFinal' : 'final_mark'})\n",
    "exams = pd.merge(exams, exam_pivot, on = ['courseid', 'semestre', 'userid'], how = 'inner')\n",
    "\n",
    "#we finish by dropping the rows that have no final_mark\n",
    "exams.dropna(inplace = True)\n",
    "exams.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We now have a table with both of our potential targets**.\n",
    "\n",
    "Now, we need to go back to our main support table. In this table, we now deal with the remaining assignments - that is, the ones that are identified as N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, split exams from remaining rows\n",
    "assignments = support_table['logs_Moodle_cursos'][~(support_table['logs_Moodle_cursos']['statusAvaliacao'].isin(exames))].filter(['courseid', 'semestre','userid', \n",
    "                                                                                                            'statusAvaliacao', 'cd_final','notaAvaliacao'])\n",
    "assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exams.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table['datasExames'].drop_duplicates(inplace = True)\n",
    "support_table['datasExames'][support_table['datasExames'].duplicated(subset = ['cd_discip'], keep = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table['logs_Moodle_cursos'][support_table['logs_Moodle_cursos']['statusAvaliacao'] == 'Exame Época Final Normal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will only touch the course Start Date in the next notebook.**\n",
    "\n",
    "For now, we will make the following concessions concerning end-date:\n",
    "\n",
    "1. For every Semester class: S1, S2, T1, T2, T3 and T4, we will find the mean date (which corresponds to the mean of the dates of the normal exam).\n",
    "\n",
    "2. All courses in each semester class will have an end-date that is equal to the Friday of the week in question.\n",
    "\n",
    "This will ensure that all courses in the class have the same duration - which will ease our work later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then, we cumulative sum all in-group members \n",
    "support_table['datasExames']['End Date'] = pd.to_datetime((support_table['datasExames'].groupby('semestre')['dataExame'].transform(pd.Series.mean)).dt.date)\n",
    "\n",
    "#setting up duration threshold to be on friday -> weekday 4\n",
    "support_table['datasExames']['End Date'] = support_table['datasExames']['End Date'].where( support_table['datasExames']['End Date'] == (( support_table['datasExames']['End Date'] + Week(weekday=4)) - Week()), support_table['datasExames']['End Date'] + Week(weekday=4))\n",
    "\n",
    "#this also allows us to remove the pesky columns that serve no additional purpose\n",
    "support_table['datasExames'].drop(['epocaAvaliacao', 'dataExame'], axis = 1, inplace = True)\n",
    "support_table['datasExames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then, we cumulative sum all in-group members \n",
    "support_table['datasExames']['End Date'] = pd.to_datetime((support_table['datasExames'].groupby('semestre')['dataExame'].transform(pd.Series.mean)).dt.date)\n",
    "support_table['datasExames']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. To business\n",
    "\n",
    "The information stored in these tables is pivotal for our work with the logs. Ignoring all other noise potential insights that may arise from this data we are, for the most part, interested in 3 things:\n",
    "\n",
    "1. Identify the student population - implicitly achieved via\n",
    "2. Compute Student Performance - our target\n",
    "3. Get course duration - or find a way to compute those - the courses to that we will take forward.\n",
    "\n",
    "We've been discussing continuously that we want to, in some capacity, predict student performance. As we do not have access to the final grades, we will need to infer it from graded Moodle assignments. The first, and almost immediate observation is that we will can only use courses that use Moodle in this capacity -> which will reduce the number of courses we have to work with.\n",
    "\n",
    "We will follow the formula adopted by the authors of the Riestra-González paper:\n",
    "\n",
    "#### Student Performance and Course Duration\n",
    "\n",
    "The authors got to student performance and course duration by performing inner joins across multiple tables and filtered across different conditions:\n",
    "\n",
    "course_mod_table,\n",
    "grades_table,\n",
    "grade_item_table\n",
    "\n",
    "We will replicate their steps and hopefully, reach suport tables that return comparable results. The first step is to perform the removal of rows that will be unnecessary for us. We can only construct a solution for items that are graded and for which we have the means to estimate the course duration. \n",
    "\n",
    "Thus, in the grades_table, we will look to only keep rows that can, simultaneuously, fulfill the following pre-requisite:\n",
    "1. Have a valid final grade,\n",
    "\n",
    "The second phase will be to perform inner joins of the different tables:\n",
    "1. course_mod_table with grade_item_table on iteminstance and courseid\n",
    "2. grade_item_table.id with grades_table.itemid\n",
    "3. The merge of the previous 2 merged tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Step 1, removing all rows that have no interest to us\n",
    "\n",
    "# grades_table.dropna(subset = ['finalgrade','timecreated', 'timemodified'], inplace = True)\n",
    "\n",
    "#Step 2: Create temporary tables that associate courses and assignments\n",
    "placeholder_1 = pd.merge(course_mod_table, grade_item_table, on=['iteminstance','courseid'], how='inner')\n",
    "\n",
    "#Step 3: Create second temporary table that associates grades with assignments\n",
    "placeholder_2 = pd.merge(placeholder_1, grades_table, on ='itemid', how='inner')\n",
    "\n",
    "#step 3: merge both placeholder tables\n",
    "support_table = placeholder_2[:]\n",
    "support_table['sup_time'] = np.where(support_table['timecreated'] > support_table['timemodified'],\n",
    "                                support_table['timecreated'], support_table['timemodified'])\n",
    "\n",
    "#step 4: only keep graded items, which means nonzero max grades\n",
    "support_table = support_table[(support_table['rawgrademax'] > 0) & (support_table['sup_time'] >= '2014-08-24')]\n",
    "\n",
    "del placeholder_1, placeholder_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As a final step, we will store the start date of each course - as it will provide us with the means to, further down the line, perform the inference for course duration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep rows worth merging - this cell can only be run once\n",
    "course_table = course_table.filter(['id', 'startdate']).rename(columns = {'id': 'courseid'})\n",
    "\n",
    "#perform inner join between support table and courses with grades\n",
    "support_table = pd.merge(support_table, course_table, on = 'courseid', how = 'inner')\n",
    "\n",
    "#only keep the final result\n",
    "support_table = support_table[(support_table['startdate'] <= support_table['sup_time']) & (support_table['startdate'].dt.year >= 2014)].filter(['assign_id', 'courseid', 'startdate', 'userid', 'finalgrade', \n",
    "                                      'rawgrademax', 'sup_time'])\n",
    "\n",
    "#we will, by default, consider the start date to be Monday\n",
    "support_table['startdate'] = support_table['startdate'].dt.to_period('W').dt.start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "support_table.describe(include = 'all', datetime_is_numeric = True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will finish this section by filtering the features to keep and, afterward, export the support table to use with the LMS logs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2: \n",
    "\n",
    "By now, we know, generally:\n",
    "\n",
    "- all courses that had graded assignments (i.e. whose max assignment grade was not 0) - courseid,\n",
    "- all students that were registered in the curricular unit - userid,\n",
    "- if a student delivered an assignment or not and the assignment's grade - finalgrade.\n",
    "\n",
    "This information is especially useful because it will assist us in the proper filtering of the moodle activity logs and, additionally, assist us in the achievement of valuable information needed for the project: course duration and target.\n",
    "\n",
    "#### 1. Course duration: \n",
    "\n",
    "We have the start date for each course. The authors of original paper inferred course end to occur at the 95% log threshold. That is to say, 5% of the logs were registered after the end of course. We have no way to obtain a better estimate so we will accept the postulation. When we deal with the logs, we will be able to calculate end of course date.\n",
    "\n",
    "#### 2. Targets - finalgrade:\n",
    "\n",
    "The authors calculated final grade as a construct computed from the assignment grades. Again, we will accept the author's methods for this. \n",
    "\n",
    "**First**: to classify whether different assignments were mandatory or not\n",
    "\n",
    "The authors of the paper focused made a split between mandatory and optional assignments. In their view, any assignment whose submittal rate (relative to the number of students attending the course) is 40% or under would be considered an optional assignment. \n",
    "\n",
    "1. We will need to know which assignments are optional and which are mandatory. We have, from our support table, the ability to list the courses and students that attending the course. From here, we can get the number of students attending each course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get to create a pivot-table that associates students and the courses they are attending\n",
    "student_list = pd.pivot_table(support_table, index='userid', columns = 'courseid', values = 'assign_id',\n",
    "                    aggfunc='count')\n",
    "\n",
    "# we use the describe command to get the course-level aggregate statistics\n",
    "# count -> number of students attending, mean is the average number of clicks performed by each student \n",
    "student_count = student_list.describe(include = 'all').T.sort_values(by = 'count', ascending = False)['count'].reset_index()\n",
    "\n",
    "#from here, we can create a dict that associates each course to the number of students attending the course\n",
    "student_count = student_count.set_index('courseid').to_dict()['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We can, in some capacity, partially repeat the steps performed in the previous pivot-table and make the option/mandatory classification of each assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get to create a pivot-table that associates assignments and the courses are asked on\n",
    "assign_number = pd.pivot_table(support_table.dropna(), index= 'userid', columns = ['courseid', 'assign_id'], values = 'finalgrade',\n",
    "                    aggfunc='count')\n",
    "\n",
    "# we use the describe command to get the course-level aggregate statistics\n",
    "# count -> number of students delivering the assignment, mean is the average number of students delivering the assignment \n",
    "assign_number = assign_number.describe(include = 'all').T.sort_values(by = 'count', ascending = False)['count'].reset_index()\n",
    "\n",
    "#from her, we can create 2 columns: i) one with the number of students attending the course\n",
    "assign_number['registered_students'] = assign_number['courseid'].map(student_count)\n",
    "\n",
    "#then, we can calculate the percentage of assignments delivered relative to the number of attending students\n",
    "assign_number['%_submissions'] = assign_number['count'] / assign_number['registered_students']\n",
    "\n",
    "#finally, we classify each assignment as mandatory vs non-mandatory (over 40% submission rates)\n",
    "assign_number['mandatory_status'] = np.where(assign_number['%_submissions'] > 0.4, 1, 0)\n",
    "\n",
    "#from here, we can create a dict that associates each course to the number of students attending the course\n",
    "mandatory_status = assign_number.set_index('assign_id').to_dict()['mandatory_status']\n",
    "\n",
    "#from here, we can now map the mandatory vs non mandatory status of \n",
    "support_table['mandatory_status'] = support_table['assign_id'].map(mandatory_status)\n",
    "\n",
    "del assign_number, student_count, mandatory_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have assigned the mandatory status to different assignments. We will not use this knowledge immediatly, but we will need it later. What it allows us is the ability to perform new computations.\n",
    "\n",
    "**3. Now, we can clean unnecessary assignments and courses. We can now perform the following operations:**\n",
    "\n",
    "1. identify whether the students made the delivery of the assignment or not - nans vs non nans\n",
    "\n",
    "2. give every nan the classification of 0.\n",
    "\n",
    "3. verify whether any courses have average finalgrade of 0. By extension, every course that only has 0 mean finalgrades will also excluded.\n",
    "\n",
    "4. Another variant we considered was the removal of all assignments with average finalgrade = 0. Ultimately, we opted to keep these in courses where there are assignments with finalgrade > 0,\n",
    "\n",
    "5. Additionally, we considered the removal of average finalgrade equal to the rawgrademax . However, we opted to keep these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether the assignment was delivered by the student or not\n",
    "support_table['delivered'] = np.where(support_table['finalgrade'].isna(), 0, 1)\n",
    "\n",
    "#now, we fill the nas of finalgrade with 0\n",
    "support_table.fillna(0, inplace = True)\n",
    "\n",
    "#as a final note, we can now verify which courses we can exclude\n",
    "#criteria 1: if all assignments have average grade 0, the course can be excluded\n",
    "assignments_keep = support_table.groupby(['courseid']).agg({\n",
    "                                                    'userid': 'count',\n",
    "                                                    'finalgrade' : 'mean',\n",
    "                                                    'rawgrademax' : 'mean',\n",
    "                                                    },\n",
    "                                                    )\n",
    "\n",
    "#now we select assignments that fulfill the criteria avg finalgrade = 0 and store it in a list\n",
    "assignments_keep = list(assignments_keep[assignments_keep['finalgrade'] > 0].reset_index()['courseid'])\n",
    "\n",
    "#so, we keep assignments who have a positive finalgrade\n",
    "support_table = support_table[support_table['courseid'].isin(assignments_keep)]\n",
    "\n",
    "del assignments_keep\n",
    "\n",
    "#check\n",
    "support_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table.describe(include = 'all', datetime_is_numeric = True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Before finishing this notebook, there is still one thing we need to do:**\n",
    "\n",
    "So far, the students registered in each course, and their results in graded assignments.\n",
    "\n",
    "The authors of the Riestra González paper used the following equation to calculate target with several different possible values between 0 and 1 for $\\alpha$:\n",
    "\n",
    "$$\\hat{Y} = 10(\\alpha \\frac{\\sum{} mandatory\\:assignment\\:marks}{number\\:of\\:mandatory\\:assignments} + (1 - \\alpha) \\frac{\\sum{} optional\\:assignment\\:marks}{number\\:of\\:optional\\:assignments})$$\n",
    "\n",
    "We will now calculate our results for final marks using the same value used by the authors of the R. Gonzalez paper:\n",
    "\n",
    "$\\alpha = 0.5$\n",
    "\n",
    "In order to do that, we will start by normalizing all grades relative to their max possible grade to a 0 to be on a 0 to 1 scale.\n",
    "\n",
    "Then, we will compute the final mark, which will allow us to compute the targets for our classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hwe will start by defining a function that will assist us in dealing with the multiindex\n",
    "\n",
    "def flattenHierarchicalCol(col,sep = '_'):\n",
    "    '''converts multiindex columns into single index columns while retaining the hierarchical components'''\n",
    "    if not type(col) is tuple:\n",
    "        return col\n",
    "    else:\n",
    "        new_col = ''\n",
    "        for leveli,level in enumerate(col):\n",
    "            if not level == '':\n",
    "                if not leveli == 0:\n",
    "                    new_col += sep\n",
    "                new_col += level\n",
    "        return new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1: current grade scales are varying between 0.7 and 1001. The fastest way to account for this \n",
    "support_table['assignment_mark'] = support_table['finalgrade'] / support_table['rawgrademax']\n",
    "\n",
    "#step 2: For every student and every course, we can obtain the sum and number of both mandatory and optional assignments:\n",
    "grade_estimation = support_table.groupby(['courseid', 'userid', 'mandatory_status']).agg({\n",
    "                                                                                    'assignment_mark' : ['sum', 'count'],\n",
    "                                                                                        })\n",
    "\n",
    "#applies the function that removes multiindex\n",
    "grade_estimation.columns = grade_estimation.columns.map(flattenHierarchicalCol)\n",
    "grade_estimation.reset_index(inplace = True)\n",
    "\n",
    "#now we can create an optional and a mandatory sum column for each sum \n",
    "grade_estimation['Optional'] = 0.5 * np.where(grade_estimation['mandatory_status'] == 0, #1 - alpha = 0.5,  \n",
    "                                              grade_estimation['assignment_mark_sum'] / grade_estimation['assignment_mark_count'],\n",
    "                                             np.nan)\n",
    "\n",
    "grade_estimation['Mandatory'] = 0.5 * np.where(grade_estimation['mandatory_status'] == 1, # alpha = 0.5,  \n",
    "                                              grade_estimation['assignment_mark_sum'] / grade_estimation['assignment_mark_count'],\n",
    "                                              np.nan)\n",
    "\n",
    "#for all intents and purposes, we can now remove the columns assignment_mark_sum\n",
    "grade_estimation.drop(['assignment_mark_sum', 'assignment_mark_count'], axis = 1, inplace = True)\n",
    "\n",
    "#we can now create a new pivot_table that perfectly arranges our intended result\n",
    "targets_table = grade_estimation.pivot_table(index=['courseid','userid'], \n",
    "                                         columns=['mandatory_status'],\n",
    "                                         values=['Optional', 'Mandatory'],aggfunc='sum')\n",
    "\n",
    "#next we remove the columns that we do not want to keep, final result being: for each discipline, the optional and the mandatory grades\n",
    "targets_table.columns.set_levels(['optional','mandatory'],level=1,inplace=True)\n",
    "targets_table.columns = targets_table.columns.map(flattenHierarchicalCol)\n",
    "targets_table.drop(['Mandatory_optional', 'Optional_mandatory'], axis = 1, inplace = True)\n",
    "\n",
    "#next, we sum the columns and multiply by 10:\n",
    "targets_table['final_mark'] = 10 * (targets_table['Mandatory_mandatory'].fillna(0) + targets_table['Optional_optional'].fillna(0))\n",
    "\n",
    "#if there are no optional assignments on the course, we will double the ponderation of the mandatory course\n",
    "targets_table['final_mark'] = np.where(targets_table['Optional_optional'].isna(), 2 * targets_table['final_mark'],\n",
    "                                                                                  targets_table['final_mark'])\n",
    "\n",
    "#if there are no optional assignments on the course, we will double the ponderation of the mandatory course\n",
    "targets_table['final_mark'] = np.where(targets_table['Mandatory_mandatory'].isna(), 2 * targets_table['final_mark'],\n",
    "                                                                                  targets_table['final_mark'])\n",
    "\n",
    "#before finishing this cell, we will now drop the unncecessary columns and keep the final mark\n",
    "targets_table.dropna(subset = ['Mandatory_mandatory']).reset_index(inplace = True)\n",
    "targets_table = targets_table.rename(columns = {'Mandatory_mandatory': 'Grade Mandatory', 'Optional_optional' : 'Grade Optional'})\n",
    "\n",
    "del grade_estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started the last cell with the normalization of the mark of each assignment. \n",
    "\n",
    "The cell finishes with a dataframe containing each curricular unit, each student attending it and the final mark of the student according to the formula we had placed previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_table.dropna(how = 'all', inplace = True)\n",
    "targets_table.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 2 distinct table that will be invaluable for our work in future notebooks.\n",
    "\n",
    "targets_table has stored every final mark obtain by each student attending the different courses of the university:\n",
    "\n",
    "- From final_mark, we finally are able to calculate our target variables:\n",
    "    - We can label students as at-risk or as overachievers depending on their mark,\n",
    "\n",
    "\n",
    "- From support_table, we will need more robust sets of information to be used for feature extraction and engineering:\n",
    "    - The startdate of each course,\n",
    "    - The individual mark of each assignment and at what time the assignment was delivered,\n",
    "    - The mandatory status of an assignment and whether it was or not delivered by the student in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save tables \n",
    "targets_table.to_csv('../Data/Modeling Stage/Nova_IMS_targets_table.csv') \n",
    "\n",
    "support_table.drop(['finalgrade', 'rawgrademax'], axis = 1).to_csv('../Data/Nova_IMS_support_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Done for now\n",
    "\n",
    "In notebook 2.2. we will rely on the activity logs and our support table to perform the necessary filtering and preprocessing of the data in order to make it compliant with our necessities. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
