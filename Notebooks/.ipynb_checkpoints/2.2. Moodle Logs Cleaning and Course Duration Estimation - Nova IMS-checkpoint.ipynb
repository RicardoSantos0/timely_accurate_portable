{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 2.2. Understanding and Preprocessing of Moodle Logs\n",
    "\n",
    "For all intents and purposes, this should be considered as the first real notebook that is part of the thesis work. In it, we will take the original student log file and perform the necessary manipulations to ensure that we have a dataset with the potential to be useful.\n",
    "\n",
    "We will use this notebook to filter the Moodle logs to only include the courses of our interest and estimate course duration.\n",
    "\n",
    "#### 1. A Small overview of the logs and each column\n",
    "\n",
    "The presented logs report to interactions with the Moodle LMS:\n",
    "\n",
    "    - Each interaction with the LMS is recorded sequentially:\n",
    "        When is the action performed,\n",
    "        What is the nature of the interaction,\n",
    "        Where is the actor when the action is performed,\n",
    "        Who performed the interaction,\n",
    "        In the context of which course page,\n",
    "        What is the specific link,\n",
    "                \n",
    "    - Each user is uniquely identified by the userID,\n",
    "    - Each course is uniquely identified by the courseID,\n",
    "    - Each specific interaction is recorded -> action performed and clicked url, \n",
    "    - Each click is timestamped,\n",
    "    - The actor's IP is recorded,\n",
    "\n",
    "A brief description of each column follows:\n",
    "\n",
    "##### component\n",
    "An identifier of the component,\n",
    "\n",
    "##### TStamp\t\n",
    "A timestamp of the event,\n",
    "\n",
    "##### userid\n",
    "Unique numerical identifier of user -> be it student, faculty or other,\n",
    "\n",
    "##### ip\n",
    "ip adress used by the user when interactiong with the LMS system,\n",
    "\n",
    "##### course\n",
    "Unique numerical identifier of a course,\n",
    "\n",
    "##### objecttable\n",
    "meaning unclear at the moment - to check with other Moodle Sources,\n",
    "\n",
    "##### action\n",
    "categorization of nature of the interaction\n",
    "\n",
    "#### target\t\n",
    "category of the page the student is accessing,\n",
    "\n",
    "##### cd_discip\n",
    "The identifier of the course in the other institutional software\n",
    "\n",
    "\n",
    "#### 2. We'll start this notebook by importing all relevant packages and data\n",
    "\n",
    "All data is stored in the csv files that were exported in the previous notebook. \n",
    "\n",
    "In order to minimize unecessary steps, as we import these csv files we will immediatly remove, from each dataset:\n",
    "1. The first unnamed column,\n",
    "2. All columns that are entirely made of missing values - we have detected some.\n",
    "3. All numerical columns that are immediatly recognied as categorical (or likely to be categorical values) are also immediatly declared as categoricals - this does not mean that, upon further assessment, other features may be converted to objects,\n",
    "4. All features that display no null values and have a single value are promptly removed as well, \n",
    "5. No preprocessing of time related features is performed at this stage - namely because the features realted with time may require further assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.tseries.offsets import *\n",
    "import re\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "#viz related tools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm, trange\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "sns.set()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#additionally, we will also define preemptively some golbal variables that may come in handy\n",
    "\n",
    "#colors for vizualizations\n",
    "nova_ims_colors = ['#BFD72F', '#5C666C']\n",
    "\n",
    "#standard color for student aggregates\n",
    "student_color = '#474838'\n",
    "\n",
    "#standard color for course aggragates\n",
    "course_color = '#1B3D2F'\n",
    "\n",
    "#standard continuous colormap\n",
    "standard_cmap = 'viridis_r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading student log data \n",
    "student_logs = pd.concat(pd.read_excel('../Data/Nova_IMS_logs_Moodle.xlsx', sheet_name = None,\n",
    "                           dtype = {\n",
    "                                   'userid': float,\n",
    "                                   'courseid': object,\n",
    "                                   'TStamp' : pd.datetime,\n",
    "                           })).drop(['eventname', 'CourseShortname', 'startdate', 'enddate'], axis = 1).dropna(how = 'all', axis = 1) #logs\n",
    "\n",
    "#other tables with support information\n",
    "support_table = pd.read_csv('../Data/Nova_IMS_support_table.csv',\n",
    "                             dtype = {\n",
    "                                 'cd_curso' : object,\n",
    "                                 'courseid' : float,\n",
    "                                 'userid' : float,\n",
    "                                 'assign_id': object,\n",
    "                             }, parse_dates = ['startdate', 'end_date']).drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "#after checking, we note that time and stime report to the same date and differ in 1 hour, hence, we will only keep the time column\n",
    "#additionally, we will make the immediate conversion of time\n",
    "student_logs = student_logs.rename(columns = {\n",
    "                    'TStamp': 'time', #readjusting names to match other information I already have\n",
    "                    'courseid': 'course', #moodle courseid\n",
    "                    'cd_discip' : 'courseid', #netpa course id\n",
    "                    }).reset_index(drop = True).sort_values(by = 'time')\n",
    "\n",
    "student_logs['userid'], support_table['courseid'], support_table['userid'] = student_logs['userid'].astype(object), support_table['courseid'].astype(object), support_table['userid'].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We start by taking a preliminary look at the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_logs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_logs.describe(include ='all', datetime_is_numeric = True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_logs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am unable to convert courseid to an object, which hints at some of these courses as different. We've identified 2 instances:\n",
    "\n",
    "1. 100012-100013\n",
    "2. 200032-400007\n",
    "\n",
    "In the support table, all of the 4 courses are represented. We can get the students attending each individual course and make the proper assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this cell to write any additional piece of code that may be required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And follow-up by looking at the support table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table.describe(include ='all', datetime_is_numeric = True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correcting instances where the logs recorded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting list of courses\n",
    "students_course_1 = support_table[support_table['courseid'] == 100012.0]['userid'].unique()\n",
    "students_course_2 = support_table[support_table['courseid'] == 100013.0]['userid'].unique()\n",
    "students_course_3 = support_table[support_table['courseid'] == 200032.0]['userid'].unique()\n",
    "students_course_4 = support_table[support_table['courseid'] == 400007.0]['userid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting \n",
    "student_logs['courseid'] = np.where(student_logs['courseid'] == '100012-100013',\n",
    "                                np.where(student_logs['userid'].isin(students_course_1),\n",
    "                                   100012.0, #course 1\n",
    "                                   100013.0), #course 2,    \n",
    "                                np.where(student_logs['courseid'] == '200032-400007',\n",
    "                                  np.where(student_logs['userid'].isin(students_course_3),\n",
    "                                   200032.0, #course 3\n",
    "                                   400007.0), #course 4,\n",
    "                                student_logs['courseid'] #remain the same in all others      \n",
    "                                           ))\n",
    "\n",
    "#converting to float to get the .0 and back to object again\n",
    "student_logs['courseid'] = student_logs['courseid'].astype(float)\n",
    "student_logs['courseid'] = student_logs['courseid'].astype(object)\n",
    "\n",
    "del students_course_1, students_course_2, students_course_3, students_course_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this cell to write any additional piece of code that may be required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal 1: \n",
    "\n",
    "One of the first thing to do is to consider the set of students and courses we intend to use. We have, from our support table, a list of the courses and students that we are interested in.\n",
    "\n",
    "Unlike in the situation of R. Gonz, we have to account for semesters, as there are instances of the same course - better said different courses with the same internal reference in Netpa have different course reference codes on Moodle.\n",
    "\n",
    "We need to start by making sure that we have a real way to properly sinchronize both databases - as to avoid joining together students attending different versions of a course.\n",
    "\n",
    "A first, preliminary approach is to only retains logs from courses for which we have records. We  will not perform an inner pairs on the logs and see how they match up to programid-semester-courseid. We would expect there to be a reasonable match between both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We start by filtering by all courses that are in our support table\n",
    "course_array = support_table['courseid'].unique()\n",
    "\n",
    "#We start by filtering by all courses that are in our support table\n",
    "students = support_table['userid'].unique()\n",
    "\n",
    "#then, we keep logs of the courses of interest   \n",
    "student_logs = student_logs[student_logs['courseid'].isin(course_array)].sort_values(by = 'time')\n",
    "\n",
    "#and the students\n",
    "student_logs = student_logs[student_logs['userid'].isin(students)].reset_index(drop = True)\n",
    "\n",
    "#and get the complete list of students interacting with the system - graded or not\n",
    "student_courses = student_logs.filter(['courseid', 'course', 'userid']).drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "#take a look at slices dataset\n",
    "student_logs.describe(include ='all', datetime_is_numeric = True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this filtering process, we get **4 400 020 recorded interactions**, performed by **2 140** unique students in the context of **231 curricular units**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove courses such as Research methodologies, Thesis and the doctoral discipline experimental design. It seems that there are instances of distinct classes of the same curricular unit - just different classes.\n",
    "\n",
    "We will treat different courses differently. The following courses will be removed outright:\n",
    "1. Research Methodologies - 200163.0,\n",
    "2. Experimental Design - 200086.0, \n",
    "3. Thesis - 200131.0\n",
    "4. Dissertação - 200040.0\n",
    "5. Thesis follow-up - 200050.0\n",
    "6. Thesis Seminars - 200263.0\n",
    "7. Methodology of Legal Research - 200250.0\n",
    "8. Research Seminar - 300005.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with pd.option_context('display.max_rows', None,):\n",
    "#    print(student_logs[['courseid', 'course', 'CourseFullname']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting a list of courses to eliminate\n",
    "courseid_to_eliminate = [200086.0, \n",
    "                         200163.0,\n",
    "                         200131.0,\n",
    "                         200040.0,\n",
    "                         200150.0, \n",
    "                         200263.0,\n",
    "                         200250.0,\n",
    "                         300005.0,\n",
    "                        ]\n",
    "\n",
    "#adapt student_logs and support_table to match courses to eliminate\n",
    "student_logs = student_logs[~student_logs['courseid'].isin(courseid_to_eliminate)]\n",
    "support_table = support_table[~support_table['courseid'].isin(courseid_to_eliminate)]\n",
    "student_courses = student_courses[~student_courses['courseid'].isin(courseid_to_eliminate)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have to deal, to the best of our ability with mismatches between courseid and course - instances where a specific courseid refers to more than one course.\n",
    "\n",
    "These will be relevant if and when we have students a student attending multiple courses within a courseid. Because in these instances we will need an additional identifier in order to promote a verifiable association between course a(the finer grained resolution) and courseid - the Netpa reference we have.\n",
    "\n",
    "We can start by counting the number of courses, each courseid student pair is registered to - we have no other option than to verifyu each course id individually and deal with it in the most approppriate manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create a pivot_table\n",
    "student_courses_piv = pd.pivot_table(student_courses, index = ['courseid', 'userid'], values = 'course',\n",
    "                                aggfunc = 'count')\n",
    "\n",
    "#and only keep courseid-student pairs for whom there is more than 1 occurrence of course\n",
    "student_courses_piv = student_courses_piv[student_courses_piv['course'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#with pd.option_context('display.max_rows', None,): #uncomment to see \n",
    "#    display(student_courses_piv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following Courseids have the same student attending in different semesters:**\n",
    "1. **Course 100008.0**,\n",
    "2. **Course 100010.0**\n",
    "3. **Course 200070.0**\n",
    "4. **Course 200014.0**\n",
    "\n",
    "Each version is easily mergeable with netpa data via a semester-courseid pairing,\n",
    "\n",
    "**The following Courseids have the same student displaying activity in versions of the same NetPa course whithin a semester:**\n",
    "\n",
    "1. **Course 200197.0** - 1 student in these conditions,\n",
    "2. **Course 200195.0** - Multiple students\n",
    "3. **Course 200193.0** - 1 student, \n",
    "4. **Course 200166.0** - a couple of students\n",
    "5. **Course 200165.0** - 1 student, \n",
    "6. **Course 200146.0** - different classes, same student,\n",
    "7. **Course 200049.0** - 1 student\n",
    "8. **Course 200013.0** - 5 students, \n",
    "9. **Course 200012.0** - 1 student\n",
    "\n",
    "In this instance, we can treat the unique courseid pairing as sufficient for identification of the course. It seems that the different courses result from registration in different classes of the same version of the course.\n",
    "\n",
    "**The following courses need to be verified more thoroughly:**\n",
    "1. **Course 200194.0** -> different versions occur (T3 and T4), with different classes being registered in T4\n",
    "2. **Course 200170.0** -> no id on the semester - will need to cross with support table\n",
    "3. **Course 200167.0** -> no id on semester, will need to check further -> problably different programs are in the mix -> 1488 is S2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section for additional verification using Support Table\n",
    "\n",
    "This small section will assist us in the decision on how to deal with 3 courses.\n",
    "Courses to verify:\n",
    "\n",
    "1. **Course 200194.0** -> different versions occur (T3 and T4), with different classes being registered in T4\n",
    "2. **Course 200170.0** -> no id on the semester - will need to cross with support table\n",
    "3. **Course 200167.0** -> no id on semester, will need to check further -> problably different programs are in the mix -> 1488 is S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#courses\n",
    "to_verify = [\n",
    "            200194.0,\n",
    "            200170.0, \n",
    "            200167.0\n",
    "            ]\n",
    "\n",
    "#filtering support table \n",
    "verification = support_table[support_table['courseid'].isin(to_verify)]\n",
    "verification[['cd_curso', 'nm_curso_pt', 'courseid', 'semestre', 'ds_discip_pt']].drop_duplicates(\n",
    "    subset = ['cd_curso', 'courseid', 'semestre']).sort_values(by = 'nm_curso_pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, from the data in the support table, that there are multiple instances of different programs having similar curricular units - that have, each, a different version of the same curricular unit.\n",
    "\n",
    "Therefore, we will be required to perform the merger at the level of resolution we can: That is using Course ID, Semester and UserID.\n",
    "\n",
    "Additionally, we see that most of the of the courses have the semester indication in their name. We can use this knowledge to extract the number semester and store it in a columns.\n",
    "\n",
    "Then, we will need to take care of duplicates that may surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we get all nonduplicate rows \n",
    "synch_df = student_logs.filter(['courseid', 'userid', 'course', 'CourseFullname']).drop_duplicates(subset = ['course', 'courseid', 'userid'],\n",
    "                                                                                        keep = 'first') #will allow us to understand when the first student interaction occurs\n",
    "\n",
    "#extracts an S or T followed by a digit - not perferct, but workable\n",
    "synch_df['semester'] = synch_df['CourseFullname'].str.extract(pat = '([ST]\\d)') #matches for capitol S or T followed by digit\n",
    "synch_df.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now, we get to perform an inner merge - we not expecting an increase in rows\n",
    "synch_df = pd.merge(synch_df, support_table.filter(['cd_curso','semestre', 'courseid', 'nm_curso_pt', 'ds_discip_pt', 'userid']).drop_duplicates(), on = ['courseid', 'userid'])\n",
    "\n",
    "#previous step will definitely generate immediate duplicates -> multiple courses for same course id\n",
    "synch_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have correctly managed to perform the merger, we must take into account the fact that we were not able to merge semester-wise.\n",
    "\n",
    "We will find semestral mismatches (i.e. Semester and Semestre have different values) of 2 kinds:\n",
    "\n",
    "1. Courses whose Moodle name has no indication of semester -> making the column derived from moodle a Nan,\n",
    "    \n",
    "2. Courses who have the same students attending in the first semester and second semester version of the course,\n",
    "    - This occurs on courseids 100008.0, 100010.0 and 200070.0 (course 200014.0 did not come through the filtering process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell allows us to verify which courses (according to Moodle) are registered to a semester differently to NetPA\n",
    "semester_mismatch =  synch_df[synch_df['semester'] != synch_df['semestre']]['course'].unique()\n",
    "synch_df[synch_df['course'].isin(semester_mismatch)].drop_duplicates(['course', 'semester', 'semestre']).sort_values('courseid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#we can use np.where to make the necessary adjustments\n",
    "synch_df['semester'] = np.where(synch_df['semester'].isna(),\n",
    "                                synch_df['semestre'],  #fill nas with the netpa semester reference\n",
    "                                np.where(synch_df['semester'] != synch_df['semestre'],  #in other instances of difference\n",
    "                                         'delete this row', #we understand the duplication to be a by product of the merger\n",
    "                                         synch_df['semester'])\n",
    "                               )\n",
    "#this way, we ensure only the correct semester-courseid pairings is kept                               \n",
    "synch_df = synch_df[~(synch_df['semester'] == 'delete this row')].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are expecting all of these filtering and cleaning results in our ability to, at the exception of instances where different classes of the same course are registered differently on Moodle,  draw a 1 to 1 between Moodle's course-userid pairing and Netpa's courseid-semester-userid grouping.\n",
    "\n",
    "**Course-userid pairs generate no duplicates**, but **Programid-Semester-CourseID-userid generate 222 duplicate rows**.\n",
    "\n",
    "We'll need check whether any duplicate user entries refer to:\n",
    "1. same curricular unit and different class,\n",
    "2. Different course entirely with the same courseid. \n",
    "\n",
    "**Uncomment next cells to verify duplicates.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# with pd.option_context('display.max_rows', None,): #uncomment to see \n",
    "#     display(synch_df[synch_df.duplicated(subset = ['cd_curso', 'courseid', 'semester', 'userid'], keep = False)].sort_values('courseid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#synch_df[synch_df.duplicated(subset = ['cd_curso', 'courseid', 'semester', 'userid'], keep = False)]['CourseFullname'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#course-user pairs generate no duplicates\n",
    "#with pd.option_context('display.max_rows', None,): #uncomment to see \n",
    "#     display(synch_df[synch_df.duplicated(subset = ['course', 'userid'], keep = False)].sort_values('courseid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#synch_df[synch_df.duplicated(subset = ['cd_curso', 'courseid', 'semester', 'userid'], keep = False)]['CourseFullname'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Going for mergers\n",
    "\n",
    "These results suggest that duplicate entries using programid-semester-courseid-userid pairing result from the fact that, in the moment of registration on Moodle, different classes (TP1 vs TP2 e.g.) of the same curricular unit were registered differently. \n",
    "\n",
    "**Now, we are confident in performing an inner merge between our current synchronization of Moodle and SIS data.**\n",
    "\n",
    "To do this, we will go back to the logs and, to each course-user id pairing map the respective courseid-programid-semesterid. We'll do that by performing an inner merge between synch_df and the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter\n",
    "synch_df = synch_df.filter(['course', 'userid', 'courseid', 'cd_curso', 'semestre', 'nm_curso_pt', 'ds_discip_pt'])\n",
    "\n",
    "#merge\n",
    "student_logs = pd.merge(student_logs.drop('courseid', axis = 1), synch_df, on = ['course', 'userid'])\n",
    "student_logs.drop('course', axis = 1, inplace = True)\n",
    "\n",
    "#and update complete list of students interacting with the system - graded or not\n",
    "student_courses = student_logs.filter(['cd_curso', 'courseid', 'semestre', 'userid']).drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "#likewise, we can now update the support table to only contain students that are present in the logs\n",
    "support_table = pd.merge(support_table, student_courses, on= ['cd_curso', 'courseid', 'semestre', 'userid'])\n",
    "\n",
    "#clearing space\n",
    "del synch_df\n",
    "\n",
    "#check our updated dateset\n",
    "student_logs.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small visualization: Weekly clicks per course\n",
    "We know that the conditions from course to course vary wildly. \n",
    "For the purposes of a more thorough understanding of the data, we can see how clicks for each course vary, from course to course, through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, we sort the courses by the start date. Then, we'll get the index of each \n",
    "sorting_hat = support_table[['cd_curso', 'semestre', 'courseid', 'startdate']].drop_duplicates().sort_values(by = 'startdate').reset_index(drop = True)\n",
    "sorting_hat = sorting_hat.set_index(['cd_curso', 'semestre', 'courseid']).to_dict()['startdate'] \n",
    "\n",
    "#second, we sort the courses by the start date. Then, we'll get the index of each \n",
    "ending_hat = support_table[['cd_curso', 'semestre', 'courseid', 'end_date']].drop_duplicates().reset_index(drop = True)\n",
    "ending_hat = ending_hat.set_index(['cd_curso', 'semestre', 'courseid']).to_dict()['end_date'] \n",
    "\n",
    "#Then, when it comes to logs, we aggregate by week\n",
    "data_grouper = student_logs.groupby([pd.Grouper(key='time', freq='W'), 'cd_curso', 'semestre', 'courseid']).agg({\n",
    "                                                                             'action': 'count',\n",
    "                                                                             }).reset_index().sort_values('time')\n",
    "\n",
    "\n",
    "#Weekly Interactions overall\n",
    "grouped_data = deepcopy(data_grouper)\n",
    "\n",
    "#change for better reading\n",
    "grouped_data['Date (week)'] = grouped_data['time'].astype(str)\n",
    "\n",
    "#creating pivot table to create heatmap\n",
    "grouped_data = grouped_data.pivot_table(index =['cd_curso', 'semestre', 'courseid'], \n",
    "                       columns = 'Date (week)',\n",
    "                        values = 'action', \n",
    "                       aggfunc =np.sum,\n",
    "                        fill_value=np.nan)\n",
    "\n",
    "#now, we will sort the courses according to the starting date\n",
    "grouped_data['sort'] = grouped_data.index.map(sorting_hat)\n",
    "grouped_data = grouped_data.reset_index().rename(columns = {'courseid': 'Course',\n",
    "                                                            'cd_curso': 'Program',\n",
    "                                                            'semestre': 'Semester',\n",
    "                                                           })\n",
    "\n",
    "grouped_data['Course'] = pd.to_numeric(grouped_data['Course']).astype(int)\n",
    "\n",
    "#finally we create the pivot_table that we will use to create our heatmap\n",
    "grouped_data = grouped_data.set_index(['Program', 'Semester', 'Course'], drop = True).sort_values('sort').drop('sort', axis = 1)\n",
    "grouped_data.T.describe(include = 'all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set_theme(context='paper', style='whitegrid', font='Calibri', rc={\"figure.figsize\":(20, 12)}, font_scale=2)\n",
    "\n",
    "#here, we are plotting the first\n",
    "heat1 = sns.heatmap(grouped_data, robust=True, norm=LogNorm(), xticklabels = 2, yticklabels= False,\n",
    "            cmap = standard_cmap, cbar_kws={'label': 'Weekly interactions'})\n",
    "\n",
    "fig = heat1.get_figure()\n",
    "fig.savefig('../Images/NovaIMS_exploratory_course_weekly_clicks_heatmap.png', transparent=True, dpi=300)\n",
    "\n",
    "#delete to remove from memory\n",
    "del fig, heat1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a first glance, the interaction patterns seem to be consistent with the Semester duration. We will need to take a closer look at each Semester type separately.\n",
    "\n",
    "**Starting with Semester 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = deepcopy(data_grouper[data_grouper['semestre'] == 'S1'])\n",
    "\n",
    "#change for better reading\n",
    "grouped_data['Date (week)'] = grouped_data['time'].astype(str)\n",
    "\n",
    "#creating pivot table to create heatmap\n",
    "grouped_data = grouped_data.pivot_table(index =['cd_curso', 'semestre', 'courseid'], \n",
    "                       columns = 'Date (week)',\n",
    "                        values = 'action', \n",
    "                       aggfunc =np.sum,\n",
    "                        fill_value=np.nan)\n",
    "\n",
    "#now, we will sort the courses according to the starting date\n",
    "grouped_data['sort'] = grouped_data.index.map(sorting_hat)\n",
    "grouped_data = grouped_data.reset_index().rename(columns = {'courseid': 'Course',\n",
    "                                                            'cd_curso': 'Program',\n",
    "                                                            'semestre': 'Semester',\n",
    "                                                           })\n",
    "\n",
    "grouped_data['Course'] = pd.to_numeric(grouped_data['Course']).astype(int)\n",
    "\n",
    "#finally we create the pivot_table that we will use to create our heatmap\n",
    "grouped_data = grouped_data.set_index(['Program', 'Semester', 'Course'], drop = True).sort_values('sort').drop('sort', axis = 1)\n",
    "grouped_data.T.describe(include = 'all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context='paper', style='whitegrid', font='Calibri', rc={\"figure.figsize\":(20, 12)}, font_scale=2)\n",
    "\n",
    "#here, we are plotting the first\n",
    "heat2 = sns.heatmap(grouped_data, robust=True, norm=LogNorm(), xticklabels = 2, yticklabels= False,\n",
    "            cmap = standard_cmap, cbar_kws={'label': 'Weekly interactions'})\n",
    "\n",
    "fig = heat2.get_figure()\n",
    "fig.savefig('../Images/NovaIMS_S1_weekly_interactions.png', transparent=True, dpi=300)\n",
    "\n",
    "#delete to remove from memory\n",
    "del fig, heat2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trimester 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = deepcopy(data_grouper[data_grouper['semestre'] == 'T1'])\n",
    "\n",
    "#change for better reading\n",
    "grouped_data['Date (week)'] = grouped_data['time'].astype(str)\n",
    "\n",
    "#creating pivot table to create heatmap\n",
    "grouped_data = grouped_data.pivot_table(index =['cd_curso', 'semestre', 'courseid'], \n",
    "                       columns = 'Date (week)',\n",
    "                        values = 'action', \n",
    "                       aggfunc =np.sum,\n",
    "                        fill_value=np.nan)\n",
    "\n",
    "#now, we will sort the courses according to the starting date\n",
    "grouped_data['sort'] = grouped_data.index.map(sorting_hat)\n",
    "grouped_data = grouped_data.reset_index().rename(columns = {'courseid': 'Course',\n",
    "                                                            'cd_curso': 'Program',\n",
    "                                                            'semestre': 'Semester',\n",
    "                                                           })\n",
    "\n",
    "grouped_data['Course'] = pd.to_numeric(grouped_data['Course']).astype(int)\n",
    "\n",
    "#finally we create the pivot_table that we will use to create our heatmap\n",
    "grouped_data = grouped_data.set_index(['Program', 'Semester', 'Course'], drop = True).sort_values('sort').drop('sort', axis = 1)\n",
    "grouped_data.T.describe(include = 'all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context='paper', style='whitegrid', font='Calibri', rc={\"figure.figsize\":(20, 12)}, font_scale=2)\n",
    "\n",
    "#here, we are plotting the first\n",
    "heat3 = sns.heatmap(grouped_data, robust=True, norm=LogNorm(), xticklabels = 2, yticklabels= False,\n",
    "            cmap = standard_cmap, cbar_kws={'label': 'Weekly interactions'})\n",
    "\n",
    "fig = heat3.get_figure()\n",
    "fig.savefig('../Images/NovaIMS_T1_weekly_interactions.png', transparent=True, dpi=300)\n",
    "\n",
    "#delete to remove from memory\n",
    "del fig, heat3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trimester 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = deepcopy(data_grouper[data_grouper['semestre'] == 'T2'])\n",
    "\n",
    "#change for better reading\n",
    "grouped_data['Date (week)'] = grouped_data['time'].astype(str)\n",
    "\n",
    "#creating pivot table to create heatmap\n",
    "grouped_data = grouped_data.pivot_table(index =['cd_curso', 'semestre', 'courseid'], \n",
    "                       columns = 'Date (week)',\n",
    "                        values = 'action', \n",
    "                       aggfunc =np.sum,\n",
    "                        fill_value=np.nan)\n",
    "\n",
    "#now, we will sort the courses according to the starting date\n",
    "grouped_data['sort'] = grouped_data.index.map(sorting_hat)\n",
    "grouped_data = grouped_data.reset_index().rename(columns = {'courseid': 'Course',\n",
    "                                                            'cd_curso': 'Program',\n",
    "                                                            'semestre': 'Semester',\n",
    "                                                           })\n",
    "\n",
    "grouped_data['Course'] = pd.to_numeric(grouped_data['Course']).astype(int)\n",
    "\n",
    "#finally we create the pivot_table that we will use to create our heatmap\n",
    "grouped_data = grouped_data.set_index(['Program', 'Semester', 'Course'], drop = True).sort_values('sort').drop('sort', axis = 1)\n",
    "grouped_data.T.describe(include = 'all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context='paper', style='whitegrid', font='Calibri', rc={\"figure.figsize\":(20, 12)}, font_scale=2)\n",
    "\n",
    "#here, we are plotting the first\n",
    "heat4 = sns.heatmap(grouped_data, robust=True, norm=LogNorm(), xticklabels = 2, yticklabels= False,\n",
    "            cmap = standard_cmap, cbar_kws={'label': 'Weekly interactions'})\n",
    "\n",
    "fig = heat4.get_figure()\n",
    "fig.savefig('../Images/NovaIMS_T2_weekly_interactions.png', transparent=True, dpi=300)\n",
    "\n",
    "#delete to remove from memory\n",
    "del fig, heat4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semester 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = deepcopy(data_grouper[data_grouper['semestre'] == 'S2'])\n",
    "\n",
    "#change for better reading\n",
    "grouped_data['Date (week)'] = grouped_data['time'].astype(str)\n",
    "\n",
    "#creating pivot table to create heatmap\n",
    "grouped_data = grouped_data.pivot_table(index =['cd_curso', 'semestre', 'courseid'], \n",
    "                       columns = 'Date (week)',\n",
    "                        values = 'action', \n",
    "                       aggfunc =np.sum,\n",
    "                        fill_value=np.nan)\n",
    "\n",
    "#now, we will sort the courses according to the starting date\n",
    "grouped_data['sort'] = grouped_data.index.map(sorting_hat)\n",
    "grouped_data = grouped_data.reset_index().rename(columns = {'courseid': 'Course',\n",
    "                                                            'cd_curso': 'Program',\n",
    "                                                            'semestre': 'Semester',\n",
    "                                                           })\n",
    "\n",
    "grouped_data['Course'] = pd.to_numeric(grouped_data['Course']).astype(int)\n",
    "\n",
    "#finally we create the pivot_table that we will use to create our heatmap\n",
    "grouped_data = grouped_data.set_index(['Program', 'Semester', 'Course'], drop = True).sort_values('sort').drop('sort', axis = 1)\n",
    "grouped_data.T.describe(include = 'all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context='paper', style='whitegrid', font='Calibri', rc={\"figure.figsize\":(20, 12)}, font_scale=2)\n",
    "\n",
    "#here, we are plotting the first\n",
    "heat5 = sns.heatmap(grouped_data, robust=True, norm=LogNorm(), xticklabels = 2, yticklabels= False,\n",
    "            cmap = standard_cmap, cbar_kws={'label': 'Weekly interactions'})\n",
    "\n",
    "fig = heat5.get_figure()\n",
    "fig.savefig('../Images/NovaIMS_S2_weekly_interactions.png', transparent=True, dpi=300)\n",
    "\n",
    "#delete to remove from memory\n",
    "del fig, heat5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trimester 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = deepcopy(data_grouper[data_grouper['semestre'] == 'T3'])\n",
    "\n",
    "#change for better reading\n",
    "grouped_data['Date (week)'] = grouped_data['time'].astype(str)\n",
    "\n",
    "#creating pivot table to create heatmap\n",
    "grouped_data = grouped_data.pivot_table(index =['cd_curso', 'semestre', 'courseid'], \n",
    "                       columns = 'Date (week)',\n",
    "                        values = 'action', \n",
    "                       aggfunc =np.sum,\n",
    "                        fill_value=np.nan)\n",
    "\n",
    "#now, we will sort the courses according to the starting date\n",
    "grouped_data['sort'] = grouped_data.index.map(sorting_hat)\n",
    "grouped_data = grouped_data.reset_index().rename(columns = {'courseid': 'Course',\n",
    "                                                            'cd_curso': 'Program',\n",
    "                                                            'semestre': 'Semester',\n",
    "                                                           })\n",
    "\n",
    "grouped_data['Course'] = pd.to_numeric(grouped_data['Course']).astype(int)\n",
    "\n",
    "#finally we create the pivot_table that we will use to create our heatmap\n",
    "grouped_data = grouped_data.set_index(['Program', 'Semester', 'Course'], drop = True).sort_values('sort').drop('sort', axis = 1)\n",
    "grouped_data.T.describe(include = 'all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context='paper', style='whitegrid', font='Calibri', rc={\"figure.figsize\":(20, 12)}, font_scale=2)\n",
    "\n",
    "#here, we are plotting the first\n",
    "heat6 = sns.heatmap(grouped_data, robust=True, norm=LogNorm(), xticklabels = 2, yticklabels= False,\n",
    "            cmap = standard_cmap, cbar_kws={'label': 'Weekly interactions'})\n",
    "\n",
    "fig = heat6.get_figure()\n",
    "fig.savefig('../Images/NovaIMS_T3_weekly_interactions.png', transparent=True, dpi=300)\n",
    "\n",
    "#delete to remove from memory\n",
    "del fig, heat6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trimester 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = deepcopy(data_grouper[data_grouper['semestre'] == 'T4'])\n",
    "\n",
    "#change for better reading\n",
    "grouped_data['Date (week)'] = grouped_data['time'].astype(str)\n",
    "\n",
    "#creating pivot table to create heatmap\n",
    "grouped_data = grouped_data.pivot_table(index =['cd_curso', 'semestre', 'courseid'], \n",
    "                       columns = 'Date (week)',\n",
    "                        values = 'action', \n",
    "                       aggfunc =np.sum,\n",
    "                        fill_value=np.nan)\n",
    "\n",
    "#now, we will sort the courses according to the starting date\n",
    "grouped_data['sort'] = grouped_data.index.map(sorting_hat)\n",
    "grouped_data = grouped_data.reset_index().rename(columns = {'courseid': 'Course',\n",
    "                                                            'cd_curso': 'Program',\n",
    "                                                            'semestre': 'Semester',\n",
    "                                                           })\n",
    "\n",
    "grouped_data['Course'] = pd.to_numeric(grouped_data['Course']).astype(int)\n",
    "\n",
    "#finally we create the pivot_table that we will use to create our heatmap\n",
    "grouped_data = grouped_data.set_index(['Program', 'Semester', 'Course'], drop = True).sort_values('sort').drop('sort', axis = 1)\n",
    "grouped_data.T.describe(include = 'all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context='paper', style='whitegrid', font='Calibri', rc={\"figure.figsize\":(20, 12)}, font_scale=2)\n",
    "\n",
    "#here, we are plotting the first\n",
    "heat7 = sns.heatmap(grouped_data, robust=True, norm=LogNorm(), xticklabels = 2, yticklabels= False,\n",
    "            cmap = standard_cmap, cbar_kws={'label': 'Weekly interactions'})\n",
    "\n",
    "fig = heat7.get_figure()\n",
    "fig.savefig('../Images/NovaIMS_T4_weekly_interactions.png', transparent=True, dpi=300)\n",
    "\n",
    "#delete to remove from memory\n",
    "del fig, heat7, grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can, additionally**, make some additional observations that may come in handy in the future:\n",
    "\n",
    "a. How many students are attending each course,\n",
    "\n",
    "b. How many courses is each student attending,\n",
    "\n",
    "This knowledge will allow us make additional filtering decisions to enhance our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can compute the number of students attending each course, and the number of courses each student is attending\n",
    "class_list = student_courses.groupby(['cd_curso', 'semestre', 'courseid'])['userid'].count().to_frame().rename(columns = {'userid' : 'Users per course'})\n",
    "enrollment_size = student_courses.groupby('userid')['courseid'].count().to_frame().rename(columns = {'courseid' : 'Courses per User'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. How many students are attending each course?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settub\n",
    "sns.set_theme(context='paper', style='whitegrid', font='Calibri', rc={\"figure.figsize\":(16, 10)}, font_scale=2)\n",
    "\n",
    "#a number of students per course\n",
    "#student_courses.rename(columns = {'userid' : 'Students per course'}, inplace = True)\n",
    "\n",
    "#then we plot an histogram with all courses, we are not interested in keeping courses with a number of students inferior to 10\n",
    "hist1 = sns.histplot(data=class_list, x='Users per course', kde=True, color= student_color, binwidth = 5,)\n",
    "\n",
    "fig = hist1.get_figure()\n",
    "fig.savefig('../Images/Nova_IMS_hist1_students_per_course_bin_5.png', transparent=True, dpi=300)\n",
    "\n",
    "#delete to remove from memory\n",
    "del fig, hist1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is a very significant number of courses with between 1 and 10 students**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. In how many courses is each student enrolled?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then we plot an histogram with all courses, we are not interested in keeping courses with a number of students inferior to 10\n",
    "hist2 = sns.histplot(data= enrollment_size, \n",
    "        x='Courses per User', color= course_color, discrete = True, fill = True)\n",
    "\n",
    "fig = hist2.get_figure()\n",
    "fig.savefig('../Images/Nova_IMS_hist2_courses_per_student course_bin_1.png', transparent=True, dpi=300)\n",
    "\n",
    "#delete to remove from memory\n",
    "del fig, hist2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the course in question, it is possible for it to have 1 registered user vs almost 175.\n",
    "Additionally, we can also see that there is a significant number of students attending a single course (over 150).\n",
    "\n",
    "To some extent, most courses have some degree of interaction with Moodle, no matter how small.\n",
    "\n",
    "We can see that interactions with a Course will usually start as the starting date approaches - regardless of the semester in question. Usually, accesses past the end of course date still continue occuring - liklely to access some educational content.\n",
    "\n",
    "Additionally, course interactions are consistent with the intentional splits many courses seem, at least on a preliminary level, be consistent with the split between semesters and trimesters.\n",
    "\n",
    "### So, what's next?\n",
    "\n",
    "Well, we are going back to the original script. We arte going to complete our course list. We'll need to add The start-date, end date, course duration, course size and, finally, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start date\n",
    "class_list['Start Date'] = class_list.index.to_series().map(sorting_hat)\n",
    "\n",
    "#end date\n",
    "class_list['End Date'] = class_list.index.to_series().map(ending_hat)\n",
    "class_list['End Date'] = class_list['End Date'].where( class_list['End Date'] == (( class_list['End Date'] + Week(weekday=4) ) - Week()), class_list['End Date'] + Week(weekday=4))\n",
    "\n",
    "#additionally, we will look at our estimated course duration\n",
    "class_list['Course duration days'] = class_list['End Date'] - class_list['Start Date']\n",
    "class_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we will fininsh our work by removing all logs outside the following conditions.**\n",
    "\n",
    "We will build 2 cutoff points:\n",
    "\n",
    "1. One week before the start date of the course, \n",
    "2. After the perceived end of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a new look into class list\n",
    "class_list['cuttoff_point'] = pd.to_datetime((class_list['Start Date'] - pd.to_timedelta(1, unit = 'W')).dt.date)\n",
    "\n",
    "#convert to date\n",
    "class_list['Start Date'] = pd.to_datetime(class_list['Start Date'].dt.date)\n",
    "class_list['End Date'] = pd.to_datetime(class_list['End Date'].dt.date)\n",
    "class_list['Course duration days'] = class_list['End Date'] - class_list['Start Date']\n",
    "class_list['Course duration days'] = (class_list['Course duration days'].dt.total_seconds() // 3600 // 24) + 1\n",
    "\n",
    "#we will create a new dict with the start date\n",
    "cuttoff_point = class_list.to_dict()['cuttoff_point'] \n",
    "\n",
    "#we'll create a new column that will signal whether we are whithin our course boundaries or not\n",
    "student_logs.set_index(['cd_curso', 'semestre', 'courseid'], drop = True, inplace = True)\n",
    "\n",
    "student_logs['start_bound'] = student_logs.index.map(cuttoff_point)\n",
    "student_logs['end_bound'] = student_logs.index.map(ending_hat)\n",
    "\n",
    "#convert to date\n",
    "student_logs['start_bound'] = pd.to_datetime(student_logs['start_bound'].dt.date)\n",
    "student_logs['end_bound'] = pd.to_datetime(student_logs['end_bound'].dt.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we only keep rows that are inside between the dates inside the start and end bounds.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_logs = student_logs[student_logs['time'].between(student_logs['start_bound'], student_logs['end_bound'], inclusive = True)].reset_index()\n",
    "student_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After finishing, we will now take a new look at the weekly interactions.**\n",
    "\n",
    "We are expecting a cleaner view at the weekly interactions performed by students in the context of their courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then, when it comes to logs, we aggregate by week\n",
    "grouped_data = student_logs.groupby([pd.Grouper(key='time', freq='W'), 'cd_curso', 'semestre', 'courseid']).agg({\n",
    "                                                                             'action': 'count',\n",
    "                                                                             }).reset_index().sort_values('time')\n",
    "\n",
    "#change for better reading\n",
    "grouped_data['Date (week)'] = grouped_data['time'].astype(str)\n",
    "\n",
    "#creating pivot table to create heatmap\n",
    "grouped_data = grouped_data.pivot_table(index =['cd_curso', 'semestre', 'courseid'], \n",
    "                       columns = 'Date (week)',\n",
    "                        values = 'action', \n",
    "                       aggfunc =np.sum,\n",
    "                        fill_value=np.nan)\n",
    "\n",
    "#now, we will sort the courses according to the starting date\n",
    "grouped_data['sort'] = grouped_data.index.map(sorting_hat)\n",
    "grouped_data = grouped_data.reset_index().rename(columns = {'courseid': 'Course',\n",
    "                                                            'cd_curso': 'Program',\n",
    "                                                            'semestre': 'Semester',\n",
    "                                                           })\n",
    "\n",
    "grouped_data['Course'] = pd.to_numeric(grouped_data['Course']).astype(int)\n",
    "\n",
    "#finally we create the pivot_table that we will use to create our heatmap\n",
    "grouped_data = grouped_data.set_index(['Program', 'Semester', 'Course'], drop = True).sort_values('sort').drop('sort', axis = 1)\n",
    "grouped_data.T.describe(include = 'all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, we are plotting the nex\n",
    "heat8 = sns.heatmap(grouped_data, robust=True, norm=LogNorm(), xticklabels = 2, yticklabels= False,\n",
    "            cmap = standard_cmap, cbar_kws={'label': 'Weekly interactions'})\n",
    "\n",
    "fig = heat8.get_figure()\n",
    "fig.savefig('../Images/Nova_IMS_cleaned_weekly_clicks.png', transparent=True, dpi=300)\n",
    "\n",
    "#delete to remove from memory\n",
    "del fig, heat8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finish the notebook by saving the cleaned logs and the list of the courses with which we will be going forward in our analysis. \n",
    "\n",
    "A very important factor to take into account is the fact that, as our targets, we will only have access to the student-pairt courses that we were able to identify in our targets table - which are the same as the ones present iin our support_table.\n",
    "\n",
    "It is, therefore, wise to perform a last filtering step before going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save tables \n",
    "class_list.to_csv('../Data/Modeling Stage/NovaIMS_class_duration.csv') \n",
    "\n",
    "student_logs.drop(['start_bound', 'end_bound'], axis = 1).to_csv('../Data/Modeling Stage/NovaIMS_cleaned_logs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Done\n",
    "\n",
    "From here on out, we will continue with feature engineering and extraction for modeling purposes in Notebooks 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
