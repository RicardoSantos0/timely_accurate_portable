{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc58434",
   "metadata": {},
   "source": [
    "In this notebook, we will finally create predictive features using the logs we cleaned on notebook 2.2. Our focus, for now, will be prediction using an aggregate non-temporal representation of each student.\n",
    "\n",
    "Throughout the notebook, we will start with the import of logs and remaining tables that we consider to be relevant for feature engineering and extraction.\n",
    "\n",
    "#### 1. Importing the relevant packages, setting global variables and importing the relevant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e2e9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.tseries.offsets import *\n",
    "\n",
    "#viz related tools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "#tqdm to monitor progress\n",
    "from tqdm.notebook import tqdm, trange\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "#time related features\n",
    "from datetime import timedelta\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "#starting with other tools\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "#to save\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00622359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables that may come in handy\n",
    "#course threshold sets the % duration that will be considered (1 = 100%)\n",
    "duration_threshold = [0.1, 0.25, 0.33, 0.5, 1]\n",
    "\n",
    "#colors for vizualizations\n",
    "nova_ims_colors = ['#BFD72F', '#5C666C']\n",
    "\n",
    "#standard color for student aggregates\n",
    "student_color = '#474838'\n",
    "\n",
    "#standard color for course aggragates\n",
    "course_color = '#1B3D2F'\n",
    "\n",
    "#standard continuous colormap\n",
    "standard_cmap = 'viridis_r'\n",
    "\n",
    "#Function designed to deal with multiindex and flatten it\n",
    "def flattenHierarchicalCol(col,sep = '_'):\n",
    "    '''converts multiindex columns into single index columns while retaining the hierarchical components'''\n",
    "    if not type(col) is tuple:\n",
    "        return col\n",
    "    else:\n",
    "        new_col = ''\n",
    "        for leveli,level in enumerate(col):\n",
    "            if not level == '':\n",
    "                if not leveli == 0:\n",
    "                    new_col += sep\n",
    "                new_col += level\n",
    "        return new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1209428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading student log data \n",
    "student_logs = pd.read_csv('../Data/Modeling Stage/R_Gonz_cleaned_logs.csv', \n",
    "                           dtype = {\n",
    "                                   'id': object,\n",
    "                                   'itemid': object,\n",
    "                                   'userid': object,\n",
    "                                   'course': object,\n",
    "                                   'cmid': object,\n",
    "                                   },\n",
    "                                   parse_dates = ['time'],).drop(['Unnamed: 0', 'id', 'url', 'info'], axis = 1).dropna(how = 'all', axis = 1) #logs\n",
    "\n",
    "#loading support table\n",
    "support_table = pd.read_csv('../Data/R_Gonz_support_table.csv', \n",
    "                           dtype = {\n",
    "                                   'assign_id': object,\n",
    "                                   'courseid': object,\n",
    "                                   'userid': object,\n",
    "                                   }, \n",
    "                            parse_dates = ['sup_time', 'startdate']).drop('Unnamed: 0', axis = 1).dropna(how = 'all', axis = 1)\n",
    "\n",
    "#save tables \n",
    "class_list = pd.read_csv('../Data/Modeling Stage/R_Gonz_class_duration.csv', \n",
    "                         dtype = {\n",
    "                                   'course': object,                                   \n",
    "                                   },\n",
    "                        parse_dates = ['Start Date','End Date', 'cuttoff_point']).drop('Unnamed: 0', axis = 1).rename(columns = {'cuttoff_point' : 'Week before start'})\n",
    "\n",
    "#targets tables \n",
    "targets_table = pd.read_csv('../Data/Modeling Stage/R_Gonz_targets_table.csv',\n",
    "                           dtype = {\n",
    "                                   'userid': object,\n",
    "                                   'courseid': object,\n",
    "                                   },)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6218f",
   "metadata": {},
   "source": [
    "We'll start with the general verification of the different datasets we've imported. \n",
    "\n",
    "**Starting with the targets table, which includes all valid student-course logs with Final-Grade.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5389476d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30510 entries, 0 to 30509\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   courseid         30510 non-null  object \n",
      " 1   userid           30510 non-null  object \n",
      " 2   Grade Mandatory  26676 non-null  float64\n",
      " 3   Grade Optional   16485 non-null  float64\n",
      " 4   final_mark       30510 non-null  float64\n",
      "dtypes: float64(3), object(2)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "#get info\n",
    "targets_table.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0797724a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>courseid</th>\n",
       "      <th>userid</th>\n",
       "      <th>Grade Mandatory</th>\n",
       "      <th>Grade Optional</th>\n",
       "      <th>final_mark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30510</td>\n",
       "      <td>30510</td>\n",
       "      <td>26676.000000</td>\n",
       "      <td>16485.000000</td>\n",
       "      <td>30510.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>732</td>\n",
       "      <td>14111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2271.0</td>\n",
       "      <td>36779.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>667</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.303199</td>\n",
       "      <td>0.086236</td>\n",
       "      <td>4.715297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.151369</td>\n",
       "      <td>0.144909</td>\n",
       "      <td>3.121882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.210688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.194832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.339517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.666666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.424005</td>\n",
       "      <td>0.125375</td>\n",
       "      <td>7.423107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       courseid   userid  Grade Mandatory  Grade Optional    final_mark\n",
       "count     30510    30510     26676.000000    16485.000000  30510.000000\n",
       "unique      732    14111              NaN             NaN           NaN\n",
       "top      2271.0  36779.0              NaN             NaN           NaN\n",
       "freq        667       10              NaN             NaN           NaN\n",
       "mean        NaN      NaN         0.303199        0.086236      4.715297\n",
       "std         NaN      NaN         0.151369        0.144909      3.121882\n",
       "min         NaN      NaN         0.000000        0.000000      0.000000\n",
       "25%         NaN      NaN         0.210688        0.000000      2.194832\n",
       "50%         NaN      NaN         0.339517        0.000000      4.666666\n",
       "75%         NaN      NaN         0.424005        0.125375      7.423107\n",
       "max         NaN      NaN         0.500000        0.500000     10.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_table.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4270b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_table.rename(columns = {'courseid' : 'course'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530eb58f",
   "metadata": {},
   "source": [
    "Then, we repeat the same for the list of courses and their respective start and end dates. We know that the number of students attending each course is the number found in the logs. We will need to make further cuts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3910ba5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 573 entries, 0 to 572\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   course                573 non-null    object        \n",
      " 1   Users per course      573 non-null    float64       \n",
      " 2   Start Date            573 non-null    datetime64[ns]\n",
      " 3   End Date              573 non-null    datetime64[ns]\n",
      " 4   Course duration days  573 non-null    float64       \n",
      " 5   Week before start     573 non-null    datetime64[ns]\n",
      "dtypes: datetime64[ns](3), float64(2), object(1)\n",
      "memory usage: 27.0+ KB\n"
     ]
    }
   ],
   "source": [
    "class_list.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f27ad93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course</th>\n",
       "      <th>Users per course</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>End Date</th>\n",
       "      <th>Course duration days</th>\n",
       "      <th>Week before start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>573</td>\n",
       "      <td>573.000000</td>\n",
       "      <td>573</td>\n",
       "      <td>573</td>\n",
       "      <td>573.000000</td>\n",
       "      <td>573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>573</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>50.436300</td>\n",
       "      <td>2014-11-17 01:45:32.984293376</td>\n",
       "      <td>2015-03-21 03:51:12.251308800</td>\n",
       "      <td>125.087260</td>\n",
       "      <td>2014-11-10 01:45:32.984293120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2014-08-25 00:00:00</td>\n",
       "      <td>2014-10-03 00:00:00</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>2014-08-18 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>2014-09-08 00:00:00</td>\n",
       "      <td>2015-01-16 00:00:00</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>2014-09-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>2014-10-20 00:00:00</td>\n",
       "      <td>2015-03-06 00:00:00</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>2014-10-13 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>2015-01-19 00:00:00</td>\n",
       "      <td>2015-05-29 00:00:00</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>2015-01-12 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>642.000000</td>\n",
       "      <td>2015-06-29 00:00:00</td>\n",
       "      <td>2015-07-31 00:00:00</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>2015-06-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>56.346253</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.300804</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        course  Users per course                     Start Date  \\\n",
       "count      573        573.000000                            573   \n",
       "unique     573               NaN                            NaN   \n",
       "top     1000.0               NaN                            NaN   \n",
       "freq         1               NaN                            NaN   \n",
       "mean       NaN         50.436300  2014-11-17 01:45:32.984293376   \n",
       "min        NaN          1.000000            2014-08-25 00:00:00   \n",
       "25%        NaN         16.000000            2014-09-08 00:00:00   \n",
       "50%        NaN         33.000000            2014-10-20 00:00:00   \n",
       "75%        NaN         69.000000            2015-01-19 00:00:00   \n",
       "max        NaN        642.000000            2015-06-29 00:00:00   \n",
       "std        NaN         56.346253                            NaN   \n",
       "\n",
       "                             End Date  Course duration days  \\\n",
       "count                             573            573.000000   \n",
       "unique                            NaN                   NaN   \n",
       "top                               NaN                   NaN   \n",
       "freq                              NaN                   NaN   \n",
       "mean    2015-03-21 03:51:12.251308800            125.087260   \n",
       "min               2014-10-03 00:00:00             33.000000   \n",
       "25%               2015-01-16 00:00:00            103.000000   \n",
       "50%               2015-03-06 00:00:00            124.000000   \n",
       "75%               2015-05-29 00:00:00            138.000000   \n",
       "max               2015-07-31 00:00:00            306.000000   \n",
       "std                               NaN             49.300804   \n",
       "\n",
       "                    Week before start  \n",
       "count                             573  \n",
       "unique                            NaN  \n",
       "top                               NaN  \n",
       "freq                              NaN  \n",
       "mean    2014-11-10 01:45:32.984293120  \n",
       "min               2014-08-18 00:00:00  \n",
       "25%               2014-09-01 00:00:00  \n",
       "50%               2014-10-13 00:00:00  \n",
       "75%               2015-01-12 00:00:00  \n",
       "max               2015-06-22 00:00:00  \n",
       "std                               NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_list.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c42fa21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course</th>\n",
       "      <th>Users per course</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>End Date</th>\n",
       "      <th>Course duration days</th>\n",
       "      <th>Week before start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2014-08-25</td>\n",
       "      <td>2015-01-23</td>\n",
       "      <td>152.0</td>\n",
       "      <td>2014-08-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2014-09-22</td>\n",
       "      <td>2015-03-06</td>\n",
       "      <td>166.0</td>\n",
       "      <td>2014-09-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1010.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2014-09-08</td>\n",
       "      <td>2014-12-26</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2014-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1013.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2014-09-22</td>\n",
       "      <td>2015-02-06</td>\n",
       "      <td>138.0</td>\n",
       "      <td>2014-09-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1020.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>2015-06-05</td>\n",
       "      <td>152.0</td>\n",
       "      <td>2014-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>961.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2015-02-02</td>\n",
       "      <td>2015-05-22</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2015-01-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>984.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2014-08-25</td>\n",
       "      <td>2015-01-30</td>\n",
       "      <td>159.0</td>\n",
       "      <td>2014-08-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>992.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2015-02-09</td>\n",
       "      <td>2015-05-15</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2015-02-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>993.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-09-08</td>\n",
       "      <td>2015-01-16</td>\n",
       "      <td>131.0</td>\n",
       "      <td>2014-09-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>999.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2014-09-15</td>\n",
       "      <td>2015-01-30</td>\n",
       "      <td>138.0</td>\n",
       "      <td>2014-09-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>573 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     course  Users per course Start Date   End Date  Course duration days  \\\n",
       "0    1000.0              12.0 2014-08-25 2015-01-23                 152.0   \n",
       "1    1002.0              22.0 2014-09-22 2015-03-06                 166.0   \n",
       "2    1010.0              48.0 2014-09-08 2014-12-26                 110.0   \n",
       "3    1013.0              71.0 2014-09-22 2015-02-06                 138.0   \n",
       "4    1020.0              64.0 2015-01-05 2015-06-05                 152.0   \n",
       "..      ...               ...        ...        ...                   ...   \n",
       "568   961.0              12.0 2015-02-02 2015-05-22                 110.0   \n",
       "569   984.0              13.0 2014-08-25 2015-01-30                 159.0   \n",
       "570   992.0              16.0 2015-02-09 2015-05-15                  96.0   \n",
       "571   993.0              55.0 2014-09-08 2015-01-16                 131.0   \n",
       "572   999.0              59.0 2014-09-15 2015-01-30                 138.0   \n",
       "\n",
       "    Week before start  \n",
       "0          2014-08-18  \n",
       "1          2014-09-15  \n",
       "2          2014-09-01  \n",
       "3          2014-09-15  \n",
       "4          2014-12-29  \n",
       "..                ...  \n",
       "568        2015-01-26  \n",
       "569        2014-08-18  \n",
       "570        2015-02-02  \n",
       "571        2014-09-01  \n",
       "572        2014-09-08  \n",
       "\n",
       "[573 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e00340",
   "metadata": {},
   "source": [
    "We still note a significant presence of courses with small numbers of students. The first step we will take is the removal of all courses whose number of attending students is below 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09e9cb00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course</th>\n",
       "      <th>Users per course</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>End Date</th>\n",
       "      <th>Course duration days</th>\n",
       "      <th>Week before start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>226</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>226</td>\n",
       "      <td>226</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>226</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>1013.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>95.951327</td>\n",
       "      <td>2014-11-12 13:41:56.814159360</td>\n",
       "      <td>2015-03-24 03:36:38.230088448</td>\n",
       "      <td>132.579646</td>\n",
       "      <td>2014-11-05 13:41:56.814159360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>2014-08-25 00:00:00</td>\n",
       "      <td>2014-10-10 00:00:00</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>2014-08-18 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>2014-09-08 00:00:00</td>\n",
       "      <td>2015-01-23 00:00:00</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>2014-09-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>2014-10-20 00:00:00</td>\n",
       "      <td>2015-03-06 00:00:00</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>2014-10-13 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>106.500000</td>\n",
       "      <td>2015-01-19 00:00:00</td>\n",
       "      <td>2015-05-29 00:00:00</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>2015-01-12 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>642.000000</td>\n",
       "      <td>2015-05-18 00:00:00</td>\n",
       "      <td>2015-07-03 00:00:00</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>2015-05-11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>66.286398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.575954</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        course  Users per course                     Start Date  \\\n",
       "count      226        226.000000                            226   \n",
       "unique     226               NaN                            NaN   \n",
       "top     1013.0               NaN                            NaN   \n",
       "freq         1               NaN                            NaN   \n",
       "mean       NaN         95.951327  2014-11-12 13:41:56.814159360   \n",
       "min        NaN         50.000000            2014-08-25 00:00:00   \n",
       "25%        NaN         62.000000            2014-09-08 00:00:00   \n",
       "50%        NaN         77.000000            2014-10-20 00:00:00   \n",
       "75%        NaN        106.500000            2015-01-19 00:00:00   \n",
       "max        NaN        642.000000            2015-05-18 00:00:00   \n",
       "std        NaN         66.286398                            NaN   \n",
       "\n",
       "                             End Date  Course duration days  \\\n",
       "count                             226            226.000000   \n",
       "unique                            NaN                   NaN   \n",
       "top                               NaN                   NaN   \n",
       "freq                              NaN                   NaN   \n",
       "mean    2015-03-24 03:36:38.230088448            132.579646   \n",
       "min               2014-10-10 00:00:00             33.000000   \n",
       "25%               2015-01-23 00:00:00            110.000000   \n",
       "50%               2015-03-06 00:00:00            131.000000   \n",
       "75%               2015-05-29 00:00:00            145.000000   \n",
       "max               2015-07-03 00:00:00            299.000000   \n",
       "std                               NaN             47.575954   \n",
       "\n",
       "                    Week before start  \n",
       "count                             226  \n",
       "unique                            NaN  \n",
       "top                               NaN  \n",
       "freq                              NaN  \n",
       "mean    2014-11-05 13:41:56.814159360  \n",
       "min               2014-08-18 00:00:00  \n",
       "25%               2014-09-01 00:00:00  \n",
       "50%               2014-10-13 00:00:00  \n",
       "75%               2015-01-12 00:00:00  \n",
       "max               2015-05-11 00:00:00  \n",
       "std                               NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_list = class_list[class_list['Users per course'] >= 50]\n",
    "\n",
    "#updating student logs\n",
    "student_logs = student_logs[student_logs['course'].isin(class_list['course'])]\n",
    "\n",
    "\n",
    "#additionally updating targets_table\n",
    "targets_table = targets_table[targets_table['course'].isin(class_list['course'])]\n",
    "class_list.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac374a",
   "metadata": {},
   "source": [
    "We'll follow up with taking a closer look logs we cleaned in the previous section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3d0d86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5192790 entries, 5132 to 6911838\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Dtype         \n",
      "---  ------  -----         \n",
      " 0   time    datetime64[ns]\n",
      " 1   userid  object        \n",
      " 2   ip      object        \n",
      " 3   course  object        \n",
      " 4   module  object        \n",
      " 5   cmid    object        \n",
      " 6   action  object        \n",
      "dtypes: datetime64[ns](1), object(6)\n",
      "memory usage: 316.9+ MB\n"
     ]
    }
   ],
   "source": [
    "student_logs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a53ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_logs.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d533a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45520e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#then we plot an histogram with all courses, we are not interested in keeping courses with a number of students inferior to 10\n",
    "sns.set_theme(context='paper', style='whitegrid', font='Calibri', rc={\"figure.figsize\":(16, 10)}, font_scale=2)\n",
    "hist4 = sns.histplot(data=class_list, x='Users per course', kde=True, color= student_color, binwidth = 5,)\n",
    "\n",
    "fig = hist4.get_figure()\n",
    "fig.savefig('../Images/hist4_students_per_course_bin_5.png', transparent=True, dpi=300)\n",
    "\n",
    "#delete to remove from memory\n",
    "del fig, hist4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd3d387",
   "metadata": {},
   "source": [
    "\n",
    "Likewise, there is some attention to be found on courses with abnormally high numbers of attending students in a face-to-face context (over 200). We will pay closer attention to those courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb5550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create df only with high affluence courses\n",
    "most_affluent_courses = class_list[class_list['Users per course'] >= 200]\n",
    "\n",
    "#separate logs accordingly\n",
    "high_attendance_logs = student_logs[student_logs['course'].isin(most_affluent_courses['course'])]\n",
    "high_attendance_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2e0b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_attendance_logs.describe(include = 'all', datetime_is_numeric = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881df3a",
   "metadata": {},
   "source": [
    "We can plot the weekly interactions of these courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13e1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then, when it comes to logs, we aggregate by week\n",
    "grouped_data = high_attendance_logs.groupby([pd.Grouper(key='time', freq='W'), 'course']).agg({\n",
    "                                                                             'action': 'count',\n",
    "                                                                             }).reset_index().sort_values('time')\n",
    "#change for better reading\n",
    "grouped_data['Date (week)'] = grouped_data['time'].astype(str)\n",
    "\n",
    "#creating pivot table to create heatmap\n",
    "grouped_data = grouped_data.pivot_table(index =['course'], \n",
    "                       columns = 'Date (week)',\n",
    "                        values = 'action', \n",
    "                       aggfunc =np.sum,\n",
    "                        fill_value=np.nan).reset_index().rename(columns = {'course' : 'Course'})\n",
    "\n",
    "#now, we will sort the courses according to the starting date\n",
    "grouped_data['Course'] = pd.to_numeric(grouped_data['Course']).astype(int)\n",
    "\n",
    "#finally we create the pivot_table that we will use to create our heatmap\n",
    "grouped_data = grouped_data.set_index('Course', drop = True)\n",
    "grouped_data.T.describe(include = 'all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context='paper', style='whitegrid', font='Calibri', rc={\"figure.figsize\":(20, 12)}, font_scale=2)\n",
    "\n",
    "#here, we are plotting the nex\n",
    "heat4 = sns.heatmap(grouped_data, robust=True, norm=LogNorm(), xticklabels = 2, yticklabels= 1,\n",
    "            cmap = standard_cmap, cbar_kws={'label': 'Weekly interactions'})\n",
    "\n",
    "fig = heat4.get_figure()\n",
    "fig.savefig('../Images/highest_attendance_weekly_clicks_heat4.png', transparent=True, dpi=300)\n",
    "\n",
    "#delete to remove from memory\n",
    "del fig, heat4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b59e4c",
   "metadata": {},
   "source": [
    "While most courses are very clearly restricted to their semester, there are courses that have interactions occurring across the entire year. \n",
    "\n",
    "For these courses, we just want to undestand whether all students are interacting continuously or we are speaking of different co-horts of students. As such, we will look more deeply at the following courses:\n",
    "\n",
    "3022, 3069 and 3151"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_long_high_attendance = ['3022.0', '3069.0', '3151.0']\n",
    "\n",
    "#Then, when it comes to logs, we aggregate by week\n",
    "grouped_data = high_attendance_logs[high_attendance_logs['course'].isin(year_long_high_attendance)].groupby([pd.Grouper(key='time', freq='W'), 'course', 'userid']).agg({\n",
    "                                                                             'action': 'count',\n",
    "                                                                             }).reset_index().sort_values('time')\n",
    "#change for better reading\n",
    "grouped_data['Date (week)'] = grouped_data['time'].astype(str)\n",
    "\n",
    "#creating pivot table to create heatmap\n",
    "grouped_data = grouped_data.pivot_table(index =['course', 'userid'], \n",
    "                       columns = 'Date (week)',\n",
    "                        values = 'action', \n",
    "                       aggfunc =np.sum,\n",
    "                        fill_value=np.nan).reset_index().rename(columns = {'course' : 'Course'})\n",
    "\n",
    "#now, we will sort the courses according to the starting date\n",
    "grouped_data['Course'] = pd.to_numeric(grouped_data['Course']).astype(int)\n",
    "\n",
    "#finally we create the pivot_table that we will use to create our heatmap\n",
    "grouped_data = grouped_data.set_index(['Course', 'userid'], drop = True)\n",
    "grouped_data.T.describe(include = 'all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context='paper', style='whitegrid', font='Calibri', rc={\"figure.figsize\":(20, 12)}, font_scale=2)\n",
    "\n",
    "#here, we are plotting the nex\n",
    "heat5 = sns.heatmap(grouped_data, robust=True, norm=LogNorm(), xticklabels = 2, yticklabels= 0,\n",
    "            cmap = standard_cmap, cbar_kws={'label': 'Weekly interactions'})\n",
    "\n",
    "fig = heat5.get_figure()\n",
    "fig.savefig('../Images/high_attend_yearlong_weekly_heat5.png', transparent=True, dpi=300)\n",
    "\n",
    "#delete to remove from memory\n",
    "del fig, heat5, grouped_data, high_attendance_logs, year_long_high_attendance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0d0177",
   "metadata": {},
   "source": [
    "After consideration, we find that the student interactions seem to be consistent with the course duration. \n",
    "\n",
    "We note, however, that the accesses to course 4923 seem to be inconsistent at best. We will monitor this course (and others) in the following steps. For now, we will proceed with the analysis over targets and support table.\n",
    "\n",
    "**1. First, we filter by our current list of valid courses.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384a3ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representation of different targets depending \n",
    "g = sns.PairGrid(targets_table, diag_sharey=False, corner=True)\n",
    "g.map_diag(sns.histplot)\n",
    "g.map_lower(sns.scatterplot)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816df8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a larger overlook at the different courses\n",
    "targets_table.groupby('course').agg({\n",
    "                                    'userid' : 'count', \n",
    "                                    'Grade Mandatory' : ['min', 'mean', 'max'],\n",
    "                                    'Grade Optional' : ['min', 'mean', 'max'],                                    \n",
    "                                    'final_mark' : ['min', 'mean', 'max'],\n",
    "                                    }).describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7ba06",
   "metadata": {},
   "source": [
    "#### Finally, we will take a look at the support table we have and repeat the same steps performed thus far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd19e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate logs accordingly\n",
    "support_table = support_table[support_table['assign_id'].isin(student_logs['cmid'])].rename(columns = {\n",
    "                                                                                            'courseid' : 'course'\n",
    "                                                                                            })\n",
    "#filter student logs_approppriately\n",
    "student_logs = student_logs[student_logs['course'].isin(support_table['course'])].reset_index(drop = True)\n",
    "\n",
    "#get info\n",
    "support_table.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f85ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77969b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a926381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a larger overlook at the different courses\n",
    "support_table.groupby('course').agg({\n",
    "                                    'userid' : 'count',\n",
    "                                    'assign_id' : 'count', \n",
    "                                    'mandatory_status' : 'mean',\n",
    "                                    'delivered' : 'mean',                                    \n",
    "                                    'assignment_mark' : 'mean',\n",
    "                                    }).describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b1761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representation of different targets depending \n",
    "g = sns.PairGrid(support_table, diag_sharey=False, corner=True)\n",
    "g.map_diag(sns.histplot)\n",
    "g.map_lower(sns.scatterplot)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d2168",
   "metadata": {},
   "source": [
    "**Going forward**.\n",
    "\n",
    "After this preliminary look, we will go forward with extracting features from the Moodle logs. \n",
    "\n",
    "In this notebook, we will consider a static non-temporal representation that considers each student-course pair as a row. We will, however, construct different datasets - 1 for each relevant timestep. \n",
    "\n",
    "We will rely on features that are regular presences in the literature. Some of these features may appear in more than one work:\n",
    "\n",
    "**From Macfadyen et al. (2010)**\n",
    "\n",
    "Count related features:\n",
    "- Discussion messages posted, \n",
    "- Online Sessions, \n",
    "- File views,\n",
    "- Assessments finished, \n",
    "- Assessments started, \n",
    "- Replies to discussion messages, \n",
    "- Mail messages sent, \n",
    "- Assignments submitted, \n",
    "- Discussion MEssages read, \n",
    "- Web link views\n",
    "\n",
    "Time related features:\n",
    "- Total time online, \n",
    "- Time spent on assignments,\n",
    "\n",
    "\n",
    "**From Romero et al. (2013)**\n",
    "\n",
    "Number Accesses to:\n",
    "- Assignments done,\n",
    "- Quizzes passed,\n",
    "- Quizzes failed,\n",
    "- Forum messages posted, \n",
    "- Forum messages read,\n",
    "\n",
    "Time related features:\n",
    "- Total time on assignments, \n",
    "- Total time on quizzes, \n",
    "- Total time on forums\n",
    "\n",
    "**From Gasevic et. al (2016)**\n",
    "\n",
    "Number of Accesses of the following variables:\n",
    "- course logins,\n",
    "- forum,\n",
    "- resources,\n",
    "- Turnitin file submission,\n",
    "- assignments,\n",
    "- book,\n",
    "- quizzes, \n",
    "- feedback,\n",
    "- lessons,\n",
    "- virtual classroom\n",
    "- chat,\n",
    "\n",
    "- etc...\n",
    "\n",
    "**From Conijn et. al (2017)**\n",
    "\n",
    "Click count related features:\n",
    "- Clicks,\n",
    "- Online sessions, \n",
    "- Course page views,\n",
    "- Resources viewed,\n",
    "- Links viewed, \n",
    "- Discussion post views,\n",
    "- Content page views,\n",
    "- Quizzes,\n",
    "- Quizzes passed,\n",
    "- Assignments submitted, \n",
    "- Wiki edits,\n",
    "- Wiki views,\n",
    "\n",
    "Time related features:\n",
    "- Total time online,\n",
    "- Largest period of inactivity,\n",
    "- Time until first action, \n",
    "- Averages session time,\n",
    "\n",
    "Performance related features:\n",
    "- Average assignment grade,\n",
    "\n",
    "**Chen and Cui (2020)**\n",
    "\n",
    "Click count related features:\n",
    "- Total clicks, \n",
    "- Clicks on campus, \n",
    "- Online sessions,\n",
    "- Clicks during weekdays,\n",
    "- Clicks on weekend,\n",
    "- Assignments, \n",
    "- File,\n",
    "- Forum,\n",
    "- Overview Report,\n",
    "- Quizz,\n",
    "- System, \n",
    "- User Report\n",
    "\n",
    "Time related features\n",
    "- Total time of online sessions, \n",
    "- Mean duration of online sessions, \n",
    "- SD of time between sessions, \n",
    "- Total time on Quiz, \n",
    "- Total time on File, \n",
    "- SD of time on File, \n",
    "\n",
    "Other statistics\n",
    "- Ratio between on-campups and off-campus clicks\n",
    "\n",
    "**Nuno Rosário Thesis**\n",
    "- number of forum messages read, \n",
    "- number of forum messages posted, \n",
    "- number of pages,\n",
    "- number of clicks, \n",
    "- number of submissions,\n",
    "- number of files accessed,\n",
    "\n",
    "As stated, some of the features are calculated across multiple works - and these only address the course level. They are not designed specifically for course-agnostic purposes.\n",
    "\n",
    "First, **we will split the logs by the difference courses and, for each student calculate the different features we intend to calculate.** The are calculated via aggregate operations, from the most common (appear more times in our literature).\n",
    "\n",
    "**But before that**, we will look at different columns of our logs and, when appropriate, keep the different values they may take. The first, and most immediate correction is IP - 127.0.0.1 hints at local connection, while other IPs suggest an out of Campus connection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ba0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts up to on_campus\n",
    "student_logs['on_campus'] = np.where(student_logs['ip'] == '127.0.0.1', 1, 0)\n",
    "student_logs.drop('ip', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae1251e",
   "metadata": {},
   "source": [
    "Secondly, we take a look at the actions and the modules. Here, we can find the most common actions and modules.\n",
    "\n",
    "We are familiar with the most common features: course, resources, assignments, quiz, forums, etc...\n",
    "\n",
    "There are, however, other less common labels whose usage is not very common. We can, start by grouping together the less common modules together in a way that, at least intuitively, makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#other pages with unclear meaning will be grouped together\n",
    "other_modules = ['oublog', 'data', 'data', 'bigbluebuttonbn', 'nanogong', 'role', 'notes', 'calendar', 'recordingsbn', 'bookmark']\n",
    "\n",
    "#converts discussion points to forum or, alternatively,groups other elements to other category\n",
    "student_logs['module'] = np.where(student_logs['module'] == 'discussion', 'forum',\n",
    "                                  np.where(student_logs['module'] == 'imscp', 'resource', #imscp is what allows content packages to be posted\n",
    "                                  np.where(student_logs['module'] == 'glossary', 'course', #usually, glossaries refer to course\n",
    "                                  np.where(student_logs['module'].isin(other_modules), 'others', student_logs['module']))))\n",
    "\n",
    "del other_modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beecaa6f",
   "metadata": {},
   "source": [
    "Likewise, we will need to take a look at the different actions in order to understand how common these may be. \n",
    "\n",
    "Again, we will look at different actions and see how we can group them together in a way that, at least, makes intuitive sense. There is use in keeping the distinction between different types of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#updates and edits related to making editions on presented information:\n",
    "update_related = ['edit post', 'edit override',  'edit report', 'update mod'\n",
    "                  'update submission', 'update entry', 'editsection', 'update switch phase',\n",
    "               'update assessment',   'update post', 'editquestions', 'update submission', 'edit', 'update mod']\n",
    "\n",
    "#additions \n",
    "addition = ['add assessment', 'add category', 'add item', 'add submission', 'add page', 'add mod', 'add entry', 'add post', 'add discussion',\n",
    "            'add', 'save report','add comment']\n",
    "\n",
    "#deletion\n",
    "deletion = ['delete override', 'record delete', 'delete entry', 'delete', 'delete post',\n",
    "            'delete discussion', 'delete attempt','delete mod',]\n",
    "\n",
    "#likewise, we will also join other moderation related tasks together,\n",
    "moderation = ['restore', 'save', 'unlock submission', 'stop tracking', 'assign', 'lock submission', 'usage report',\n",
    "              'start tracking', 'view subscribers', 'grade submission', 'grant extension', 'manualgrade']\n",
    "\n",
    "#other reporting actions\n",
    "report = ['report live', 'report participation', 'report', 'report log', 'report outline', 'view report', 'user report']\n",
    "\n",
    "#messages/files\n",
    "messages = ['message saved', 'message sent', 'files']\n",
    "\n",
    "#other actions\n",
    "other_actions = ['preview', 'unsubscribe', 'download all submissions', 'mark read', 'subscribeall', 'diff', 'comment', \n",
    "                 'unsubscribeall', 'search', 'history', 'map', 'subscribe', 'submissioncopied', 'comments']\n",
    "\n",
    "#smaller view commands to join main view\n",
    "small_view = ['view entry', 'view edit']\n",
    "\n",
    "#converts discussion points to forum or, alternatively,groups other elements to other category\n",
    "student_logs['action'] = np.where(student_logs['action'].isin(update_related), 'update', #edit list\n",
    "                                  np.where(student_logs['action'].isin(addition), 'addition', #addition list\n",
    "                                  np.where(student_logs['action'].isin(deletion), 'delete', #deletion list\n",
    "                                  np.where(student_logs['action'].isin(moderation), 'other admin actions', #other admin actions\n",
    "                                  np.where(student_logs['action'].isin(report), 'report', #reporting related\n",
    "                                  np.where(student_logs['action'].isin(other_actions), 'other actions', #other actions\n",
    "                                  np.where(student_logs['action'].isin(messages), 'messages and files', #messages\n",
    "                                  np.where(student_logs['action'].isin(small_view), 'view others', #main view files\n",
    "                                  np.where(student_logs['action'] == 'choose again','choose', #choose again to choose\n",
    "                                  \n",
    "                                  #finishing with splitting the view command to different subgroups according to the module - will make it easier later\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'assign'),'view assignment',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'choice'),'view choice',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'course'),'view course',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'folder'),'view folder',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'glossary'),'view glossary',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'others'),'view others',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'page'),'view page',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'questionnaire'),'view questionnaire',\n",
    "                                  np.where(((student_logs['action'] == 'view') | (student_logs['action'] == 'view all')) & (student_logs['module'] == 'resource'),'view resource',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'url'),'view url',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'user'),'view user',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'wiki'),'view wiki',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'workshop'),'view workshop',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'quiz'),'view quiz',                             \n",
    "                                  np.where((student_logs['action'] == 'view forums') & (student_logs['module'] == 'forum'),'view forum',\n",
    "                                  np.where((student_logs['action'] == 'submit for grading') & (student_logs['module'] == 'assign'),'submit',\n",
    "                                  np.where((student_logs['action'] == 'view submit assignment form') & (student_logs['module'] == 'assign'),'view assignment',\n",
    "                                  student_logs['action']))))))))))))))))))))))))))\n",
    "\n",
    "#we finish by ending these lists we've created\n",
    "del update_related, addition, deletion, moderation, messages, other_actions, small_view "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad0e00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#uncomment to verify pairings\n",
    "with pd.option_context('display.max_rows', None,):\n",
    "     display(student_logs.groupby(['module', 'action']).size().to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40729acb",
   "metadata": {},
   "source": [
    "We have addressed the most obvious possible aggregations. Now, we will go forward with our intended feature extraction and selection.\n",
    "\n",
    "For this step, we will create 5 distinct dicts of dataframes. Each dict refers to a certain course duration threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc5abe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#additionally, we will look at our estimated course duration\n",
    "for i in tqdm(duration_threshold):\n",
    "    #create, for each desired threshold, the appropriate cutoff date \n",
    "    class_list[f'Date_threshold_{int(i*100)}'] = pd.to_datetime((class_list['Start Date'] + pd.to_timedelta(class_list['Course duration days'] * i, unit = 'Days')).dt.date)\n",
    "    \n",
    "    #setting up duration threshold to be on friday -> reason being that it will be easier to \n",
    "    class_list[f'Date_threshold_{int(i*100)}'] = class_list[f'Date_threshold_{int(i*100)}'].where( class_list[f'Date_threshold_{int(i*100)}'] == (( class_list[f'Date_threshold_{int(i*100)}'] + Week(weekday=4)) - Week()), class_list[f'Date_threshold_{int(i*100)}'] + Week(weekday=4))\n",
    "\n",
    "#then, we will create a dictionary of dictionaries, each main dictionary storing and a version of the logs\n",
    "logs_dict = {}\n",
    "assignment_dict = {}\n",
    "\n",
    "for i in tqdm(duration_threshold):\n",
    "    #create, for each desired threshold, a different dictionary of dataframes wherein we will perform the different operations\n",
    "    print(f'Date_threshold_{int(i*100)}\\n' +\n",
    "          f'Logs')\n",
    "    logs_dict[f'Date_threshold_{int(i*100)}'] = {course: student_logs.loc[student_logs['course'] == course].reset_index(drop = True) for course in tqdm(student_logs['course'].unique())}\n",
    "    \n",
    "    #Assignments\n",
    "    print(f'Assignments')\n",
    "    assignment_dict[f'Date_threshold_{int(i*100)}'] = {course: support_table.loc[support_table['course'] == course].reset_index(drop = True) for course in tqdm(support_table['course'].unique())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d90d8b1",
   "metadata": {},
   "source": [
    "Now, we have a nested dictionary with different dataframes inside it. We will use this data structure to perform the most of the operations we are interested in.\n",
    "\n",
    "**First, we will add, to each dataframe, a column with the corresponding threshold date**\n",
    "\n",
    "After this cleaning procedure, we will all different columns referring to our features of interest. These will be:\n",
    "1. Number of assignments submitted, \n",
    "2. Number of online sessions,\n",
    "3. Discussion messages read,\n",
    "4. Resource views, \n",
    "5. Assessments started,\n",
    "6. Total time online,\n",
    "7. Assignment views,\n",
    "8. Average duration of session,\n",
    "9. Messages posted,\n",
    "10. Clicks on Forum,  \n",
    "11. On-campus clicks, \n",
    "12. On-campus/off-campus clicks,\n",
    "13. Total number of clicks\n",
    "14. Number of links viewed\n",
    "15. Largest period of inactivity\n",
    "\n",
    "To check difference between inclusion and not inclusion\n",
    "\n",
    "16. Average grade of assignments (optional)\n",
    "\n",
    "A double loop is not very efficient but, to the best of my ability, is the obvious solution to perform these operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d0504b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for each intended course duration threshold\n",
    "for i in tqdm(logs_dict):\n",
    "    #start with creating a dictionary of course and intended cuttoff date\n",
    "    cut = class_list.set_index('course').to_dict()[i] \n",
    "    \n",
    "    #for each dataframe\n",
    "    for j in tqdm(logs_dict[i]):\n",
    "        #where the course is the same as in the class_list, get the corresponding value of the appropriate column,\n",
    "        logs_dict[i][j]['Date Threshold'] = logs_dict[i][j]['course'].map(cut)\n",
    "        logs_dict[i][j] = logs_dict[i][j][logs_dict[i][j]['time'] <= logs_dict[i][j]['Date Threshold']].reset_index(drop = True).drop('Date Threshold', axis = 1)\n",
    "        \n",
    "        #doing the same for the assignments list\n",
    "        try:\n",
    "            assignment_dict[i][j]['Date Threshold'] = assignment_dict[i][j]['course'].map(cut)\n",
    "            assignment_dict[i][j] = assignment_dict[i][j][assignment_dict[i][j]['sup_time'] <= assignment_dict[i][j]['Date Threshold']].reset_index(drop = True).drop('Date Threshold', axis = 1)\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        #calculates the difference between previous within group row and current\n",
    "        logs_dict[i][j]['t_diff'] = logs_dict[i][j].sort_values(['userid', 'time']).groupby('userid')['time'].diff()\n",
    "        \n",
    "        #will need to ignore dictionaries where there is no lenght\n",
    "        if len(logs_dict[i][j]) > 0:\n",
    "            #the nans will be correspond to the first interaction made by each student - also signaling the start of the first session\n",
    "            logs_dict[i][j]['session'] = np.where(logs_dict[i][j]['t_diff'].isna(), 1, #the first session is started by nans\n",
    "                                             np.where(logs_dict[i][j]['t_diff'] > pd.to_timedelta(40, unit = 'minutes'), 1, #also identify the starting point of new sessions\n",
    "                                                      0))\n",
    "            \n",
    "            #then, we cumulative sum all in-group members \n",
    "            logs_dict[i][j]['session'] = logs_dict[i][j].groupby('userid')['session'].transform(pd.Series.cumsum)\n",
    "            \n",
    "            #before finishing this step, we will calculate the accumulated duration of a session\n",
    "            logs_dict[i][j]['mask'] = np.where(logs_dict[i][j]['t_diff'].isna(), 0, #the first session is started by nans\n",
    "                                      np.where(logs_dict[i][j]['t_diff'] > pd.to_timedelta(40, unit = 'minutes'), 0, #also identify the starting point of new sessions\n",
    "                                      logs_dict[i][j]['t_diff'].dt.total_seconds()))\n",
    "            #fillnas in t_diff\n",
    "            logs_dict[i][j]['t_diff'].fillna(pd.to_timedelta(0), inplace = True)\n",
    "\n",
    "            #then, we cumulative sum all in-group members \n",
    "            logs_dict[i][j]['session_cumul_time'] = logs_dict[i][j].groupby(['userid','session'])['mask'].transform(pd.Series.cumsum)\n",
    "            logs_dict[i][j]['session_cumul_time'] = pd.to_timedelta(logs_dict[i][j]['session_cumul_time'], unit = 'seconds')\n",
    "            #drop mask\n",
    "            logs_dict[i][j].drop('mask', axis = 1, inplace = True)\n",
    "        \n",
    "        else:\n",
    "            logs_dict[i][j] = pd.DataFrame(columns=['time', 'userid', 'course', 'module', 'cmid', 'action', 'on_campus', 't_diff',\n",
    "                                                    'session', 'session_cumul_time'])\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1abf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create backup of logs dict, we will need it for later\n",
    "backup = deepcopy(logs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7815b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dict = deepcopy(backup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a02de1",
   "metadata": {},
   "source": [
    "Now, we'll go forward with the creation of the features using these datasets. We will do it, using groupby commands.\n",
    "\n",
    "After this cleaning procedure, we will all different columns referring to our features of interest.\n",
    "\n",
    "**We cannot perform all steps at once, unfortunately.** (at least not in a capacity I can manage)\n",
    "\n",
    "We will need to create multiple dfs to ensure that all features are accounted for:\n",
    "\n",
    "1. We start with features that relate to raw aggregate counts of clicks and sessions - a general set of features, \n",
    "2. We continue by computing features related with time -  total time online and average duration of session,\n",
    "3. Then, we go into finer grained features using specific pairs of modules and actions.\n",
    "4. Then, we finish by merging these features with the final mark we have previously calculated and the average grade of assignments delivered up to the threshold date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f033e29",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#we will need to perform the same double loop we have done before\n",
    "for i in tqdm(logs_dict):\n",
    "     \n",
    "    for j in tqdm(logs_dict[i]):    \n",
    "        #as it is very difficult to we will need to create multiple placeholders\n",
    "        #placeholder 1 - general features\n",
    "        general_features = logs_dict[i][j].groupby(['course', 'userid']).agg(\n",
    "                                                {'action' : [('N_clicks','count')], #number of clicks\n",
    "                                                'session' : [('N_sessions', 'nunique')], #number of sessions\n",
    "                                                 'on_campus' : [('Clicks on Campus', np.sum)], #number of clicks on campus\n",
    "                                                 't_diff' : [('Largest_period_of Inactivity' , 'max')] #largest period of inactivity \n",
    "                                                })\n",
    "        \n",
    "        #the second group will deal with session related time features\n",
    "        session_features = logs_dict[i][j].groupby(['userid', 'session'])['session_cumul_time'].max().to_frame().reset_index() #the accumulated time up to last click of each session identifies the duration \n",
    "        \n",
    "        #now, we get to our intended features\n",
    "        session_features = session_features.groupby(['userid']).agg(\n",
    "                                                {'session_cumul_time' : [np.sum, #The total time online is the sum of the time spent in all sessions \n",
    "                                                                        np.mean], #mean duration across all sessions made by the student\n",
    "                                                 })\n",
    "        \n",
    "        #the third relies on clicks of multiple types and modules. An elegant way is to deal with these is pivot_tables of the counts\n",
    "        pivot = pd.pivot_table(logs_dict[i][j], index = 'userid', \n",
    "                              columns = ['module', 'action'],\n",
    "                              values='cmid',\n",
    "                              aggfunc = 'count').fillna(0)\n",
    "        \n",
    "        #applies the function that removes multiindex\n",
    "        pivot.columns = pivot.columns.map(flattenHierarchicalCol)\n",
    "        pivot.reset_index(inplace = True)\n",
    "        \n",
    "        #now, we filter the pivot table to only keep the features that we are interested in - specifically, the counts\n",
    "        pivot = pivot.filter([\n",
    "                               'userid',\n",
    "                               'assign_submit', #Number of assignments submitted\n",
    "                               'resource_view resource', #resource views,\n",
    "                               'assign_view assignment', #view assignment\n",
    "                               'forum_view discussion', #view discussion,\n",
    "                               'quiz_attempt', #quizzes started\n",
    "                               'forum_addition', #forum messages posted\n",
    "                                ], \n",
    "                               )\n",
    "        \n",
    "        #drop columns unnecessary columns and rename others \n",
    "        pivot = pivot.rename(columns = {'forum_addition' : 'Forum posts', \n",
    "                                            'forum_view discussion' : 'Discussions viewed',\n",
    "                                            'assign_submit' : 'Assignments submitted', \n",
    "                                            'resource_view resource' : 'Resources viewed',\n",
    "                                            'quiz_attempt' : 'Quizzes started',  \n",
    "                                            'assign_view assignment' : 'Assignments viewed'\n",
    "                                           })\n",
    "        \n",
    "        #the third relies on clicks of multiple types and modules. An elegant way is to deal with these is pivot_tables of the counts\n",
    "        pivot_1 = pd.pivot_table(logs_dict[i][j], index = 'userid', \n",
    "                              columns = 'module',\n",
    "                              values='cmid',\n",
    "                              aggfunc = 'count').fillna(0).reset_index()\n",
    "        \n",
    "        #now, we filter the pivot table to only keep the features that we are interested in - specifically, the counts\n",
    "        pivot_1 = pivot_1.filter([\n",
    "                               'userid',\n",
    "                               'forum'\n",
    "                                'url'\n",
    "                                'folder'\n",
    "                                ], \n",
    "                               ).rename(columns = {'forum' : 'Clicks on Forum',\n",
    "                                                    'url': 'Links viewed',\n",
    "                                                   'folder' : 'Clicks on folder'})\n",
    "\n",
    "\n",
    "        #applies the function that removes multiindex\n",
    "        general_features.columns = general_features.columns.map(flattenHierarchicalCol)\n",
    "        general_features.reset_index(inplace = True)\n",
    "        \n",
    "        #same for session features\n",
    "        session_features.columns = session_features.columns.map(flattenHierarchicalCol)\n",
    "        session_features.reset_index(inplace = True)\n",
    "        \n",
    "        #we finish this section by wrapping everything together\n",
    "        general_features = pd.merge(general_features, session_features, on = 'userid', how = 'inner')\n",
    "        general_features.rename(columns = {'session_cumul_time_sum': 'Total time online',\n",
    "                                           'session_cumul_time_mean': 'Average session duration',\n",
    "                                           'action_N_clicks': 'Number of clicks',\n",
    "                                           'session_N_sessions': 'Number of sessions',\n",
    "                                           'on_campus_Clicks on Campus': 'Clicks on campus',\n",
    "                                           't_diff_Largest_period_of Inactivity': 'Largest period of inactivity',\n",
    "                                          }, inplace = True)\n",
    "        \n",
    "        #merge features from pivot_table\n",
    "        pivot = pd.merge(pivot_1, pivot, on = 'userid', how = 'inner')\n",
    "        \n",
    "        #joining assignment grades\n",
    "        assignment_pivot = pd.pivot_table(assignment_dict[i][j], index = 'userid', \n",
    "                              columns = 'assign_id',\n",
    "                              values='assignment_mark',\n",
    "                              aggfunc = np.sum).fillna(0)\n",
    "        \n",
    "        #drop assignments that either were not delivered or received grade = 0  \n",
    "        assignment_pivot = assignment_pivot.loc[:, ~assignment_pivot.eq(0).all()]\n",
    "        \n",
    "        #now, we stack these together \n",
    "        assignment_pivot = assignment_pivot.stack().reset_index().rename(columns = {0 : 'Average grade of assignments'})\n",
    "        \n",
    "        #join all together to get the corresponding dataframe\n",
    "        logs_dict[i][j] = pd.merge(general_features, pivot, on = 'userid', how = 'inner')\n",
    "        \n",
    "        #calculating on-campus/off campus ratio\n",
    "        logs_dict[i][j]['On/off campus click ratio'] = np.where((logs_dict[i][j]['Number of clicks'] - logs_dict[i][j]['Clicks on campus']) > 0,\n",
    "                                                                logs_dict[i][j]['Clicks on campus'] / (logs_dict[i][j]['Number of clicks'] - logs_dict[i][j]['Clicks on campus']),\n",
    "                                                                logs_dict[i][j]['Clicks on campus']) # we consider 1 click off campus to avoid dividing by 0 \n",
    "        \n",
    "        #joining final grade for target\n",
    "        logs_dict[i][j] = logs_dict[i][j].merge(targets_table.filter(['course', 'userid', 'final_mark']), on = ['course', 'userid'], how = 'right')\n",
    "    \n",
    "        #joining with assignments\n",
    "        if len(assignment_pivot) > 0:\n",
    "                    \n",
    "            #now, and get the mean of non-zero mean assignments - averaged by all students attending the course\n",
    "            assignment_pivot = assignment_pivot.groupby(['userid'])['Average grade of assignments'].mean().reset_index()\n",
    "            \n",
    "            #and merge with final result\n",
    "            logs_dict[i][j] = logs_dict[i][j].merge(assignment_pivot, on = 'userid', how = 'left')\n",
    "            \n",
    "        #clean unnecessary dfs\n",
    "        del pivot, pivot_1, general_features, session_features, assignment_pivot\n",
    "    \n",
    "    #after the end of the loops:\n",
    "    logs_dict[i] = pd.concat(logs_dict[i], ignore_index=True)\n",
    "    logs_dict[i] = logs_dict[i].sort_values(by = ['course', 'userid', 'final_mark']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0a0aae",
   "metadata": {},
   "source": [
    "In order to account for situations where registered students only access Moodle later in the course, we will make ann additional, but necessary adaptation. \n",
    "\n",
    "We will start by looking at the complete set of valid students/courses in our 100% dataset. From these, we get the indexes of the rows that are valid (i.e. have a valid click count at 100% duration), get the indexes and retain only these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cffea72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#we gather the index number of valid rows in the 100% df\n",
    "rows_to_keep = logs_dict['Date_threshold_100'][~logs_dict['Date_threshold_100']['Number of clicks'].isna()].index\n",
    "columns_copy = ['course', 'userid']\n",
    "\n",
    "# #then slice accordingly\n",
    "for i in tqdm(logs_dict):\n",
    "    logs_dict[i] = logs_dict[i].iloc[rows_to_keep, :].reset_index(drop = True)\n",
    "    #we will need to keep some columns this way - students that made noa ction prior\n",
    "    logs_dict[i][columns_copy] = deepcopy(logs_dict['Date_threshold_100'][columns_copy])\n",
    "    \n",
    "    #Convert timedelta format to numbered format - minutes\n",
    "    logs_dict[i]['Total time online'], logs_dict[i]['Average session duration'] = logs_dict[i]['Total time online'].dt.total_seconds() / 60, logs_dict[i]['Average session duration'].dt.total_seconds() / 60\n",
    "    logs_dict[i]['Largest period of inactivity'] = logs_dict[i]['Largest period of inactivity'].dt.total_seconds() // 60\n",
    "    #fill nans with value 0 - this will only occur in dfs that do not fulfill the entire duration of course\n",
    "    logs_dict[i] = logs_dict[i].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2058285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dict['Date_threshold_10']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c7924",
   "metadata": {},
   "source": [
    "#### Almost Done.\n",
    "\n",
    "We will finish the Feature Extraction Stage momentarily. Before we do, we need to save all dfs in an easily accessible Excel File."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512405bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('../Data/Modeling Stage/R_gonz_Non_temporal_Datasets.xlsx', engine='xlsxwriter')\n",
    "\n",
    "#now loop thru and put each on a specific sheet\n",
    "for sheet, frame in  logs_dict.items(): \n",
    "    frame.to_excel(writer, sheet_name = sheet)\n",
    "\n",
    "#critical last step\n",
    "writer.save()\n",
    "\n",
    "#also saving additional info on class list\n",
    "class_list.to_csv('../Data/Modeling Stage/R_Gonz_updated_classlist.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
