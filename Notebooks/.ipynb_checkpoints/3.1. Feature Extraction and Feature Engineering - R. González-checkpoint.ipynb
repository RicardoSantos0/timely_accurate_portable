{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc58434",
   "metadata": {},
   "source": [
    "In this notebook, we will finally create predictive features using the logs we cleaned on notebook 2.2. Our focus, for now, will be prediction using an aggregate non-temporal representation of each student.\n",
    "\n",
    "Throughout the notebook, we will start with the import of logs and remaining tables that we consider to be relevant for feature engineering and extraction.\n",
    "\n",
    "#### 1. Importing the relevant packages, setting global variables and importing the relevant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.tseries.offsets import *\n",
    "\n",
    "#viz related tools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "#tqdm to monitor progress\n",
    "from tqdm.notebook import tqdm, trange\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "#time related features\n",
    "from datetime import timedelta\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "#starting with other tools\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "#to save\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00622359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables that may come in handy\n",
    "#course threshold sets the % duration that will be considered (1 = 100%)\n",
    "duration_threshold = [0.1, 0.25, 0.33, 0.5, 1]\n",
    "\n",
    "#colors for vizualizations\n",
    "nova_ims_colors = ['#BFD72F', '#5C666C']\n",
    "\n",
    "#standard color for student aggregates\n",
    "student_color = '#474838'\n",
    "\n",
    "#standard color for course aggragates\n",
    "course_color = '#1B3D2F'\n",
    "\n",
    "#standard continuous colormap\n",
    "standard_cmap = 'viridis_r'\n",
    "\n",
    "#Function designed to deal with multiindex and flatten it\n",
    "def flattenHierarchicalCol(col,sep = '_'):\n",
    "    '''converts multiindex columns into single index columns while retaining the hierarchical components'''\n",
    "    if not type(col) is tuple:\n",
    "        return col\n",
    "    else:\n",
    "        new_col = ''\n",
    "        for leveli,level in enumerate(col):\n",
    "            if not level == '':\n",
    "                if not leveli == 0:\n",
    "                    new_col += sep\n",
    "                new_col += level\n",
    "        return new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1209428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading student log data \n",
    "student_logs = pd.read_csv('../Data/Modeling Stage/R_Gonz_cleaned_logs.csv', \n",
    "                           dtype = {\n",
    "                                   'id': object,\n",
    "                                   'itemid': object,\n",
    "                                   'userid': object,\n",
    "                                   'course': object,\n",
    "                                   'cmid': object,\n",
    "                                   },\n",
    "                                   parse_dates = ['time'],).drop(['Unnamed: 0', 'id', 'url', 'info'], axis = 1).dropna(how = 'all', axis = 1) #logs\n",
    "\n",
    "#loading support table\n",
    "support_table = pd.read_csv('../Data/R_Gonz_support_table.csv', \n",
    "                           dtype = {\n",
    "                                   'assign_id': object,\n",
    "                                   'courseid': object,\n",
    "                                   'userid': object,\n",
    "                                   }, \n",
    "                            parse_dates = ['sup_time', 'startdate']).drop('Unnamed: 0', axis = 1).dropna(how = 'all', axis = 1)\n",
    "\n",
    "#save tables \n",
    "class_list = pd.read_csv('../Data/Modeling Stage/R_Gonz_class_duration.csv', \n",
    "                         dtype = {\n",
    "                                   'course': object,                                   \n",
    "                                   },\n",
    "                        parse_dates = ['Start Date','End Date', 'cuttoff_point']).drop('Unnamed: 0', axis = 1).rename(columns = {'cuttoff_point' : 'Week before start'})\n",
    "\n",
    "#targets tables \n",
    "targets_table = pd.read_csv('../Data/Modeling Stage/R_Gonz_targets_table.csv',\n",
    "                           dtype = {\n",
    "                                   'userid': object,\n",
    "                                   'courseid': object,\n",
    "                                   },)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6218f",
   "metadata": {},
   "source": [
    "We'll start with the general verification of the different datasets we've imported. \n",
    "\n",
    "**Starting with the targets table, which includes all valid student-course logs with Final-Grade.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5389476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get info\n",
    "targets_table.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0797724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_table.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4270b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_table.rename(columns = {'courseid' : 'course'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530eb58f",
   "metadata": {},
   "source": [
    "Then, we repeat the same for the list of courses and their respective start and end dates. We know that the number of students attending each course is the number found in the logs. We will need to make further cuts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3910ba5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_list.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27ad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c42fa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e00340",
   "metadata": {},
   "source": [
    "We still note a significant presence of courses with small numbers of students. The first step we will take is the removal of all courses whose number of attending students is below 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = class_list[class_list['Users per course'] >= 25]\n",
    "\n",
    "#updating student logs\n",
    "student_logs = student_logs[student_logs['course'].isin(class_list['course'])]\n",
    "\n",
    "\n",
    "#additionally updating targets_table\n",
    "targets_table = targets_table[targets_table['course'].isin(class_list['course'])]\n",
    "class_list.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac374a",
   "metadata": {},
   "source": [
    "We'll follow up with taking a closer look logs we cleaned in the previous section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d0d86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_logs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a53ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_logs.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d533a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45520e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#then we plot an histogram with all courses, we are not interested in keeping courses with a number of students inferior to 10\n",
    "sns.set_theme(context='paper', style='whitegrid', font='Calibri', rc={\"figure.figsize\":(16, 10)}, font_scale=2)\n",
    "hist4 = sns.histplot(data=class_list, x='Users per course', kde=True, color= student_color, binwidth = 5,)\n",
    "\n",
    "fig = hist4.get_figure()\n",
    "fig.savefig('../Images/hist4_students_per_course_bin_5.png', transparent=True, dpi=300)\n",
    "\n",
    "#delete to remove from memory\n",
    "del fig, hist4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd3d387",
   "metadata": {},
   "source": [
    "\n",
    "Likewise, there is some attention to be found on courses with abnormally high numbers of attending students in a face-to-face context (over 200). We will pay closer attention to those courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb5550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create df only with high affluence courses\n",
    "most_affluent_courses = class_list[class_list['Users per course'] >= 200]\n",
    "\n",
    "#separate logs accordingly\n",
    "high_attendance_logs = student_logs[student_logs['course'].isin(most_affluent_courses['course'])]\n",
    "high_attendance_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2e0b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_attendance_logs.describe(include = 'all', datetime_is_numeric = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881df3a",
   "metadata": {},
   "source": [
    "We can plot the weekly interactions of these courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13e1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then, when it comes to logs, we aggregate by week\n",
    "grouped_data = high_attendance_logs.groupby([pd.Grouper(key='time', freq='W'), 'course']).agg({\n",
    "                                                                             'action': 'count',\n",
    "                                                                             }).reset_index().sort_values('time')\n",
    "#change for better reading\n",
    "grouped_data['Date (week)'] = grouped_data['time'].astype(str)\n",
    "\n",
    "#creating pivot table to create heatmap\n",
    "grouped_data = grouped_data.pivot_table(index =['course'], \n",
    "                       columns = 'Date (week)',\n",
    "                        values = 'action', \n",
    "                       aggfunc =np.sum,\n",
    "                        fill_value=np.nan).reset_index().rename(columns = {'course' : 'Course'})\n",
    "\n",
    "#now, we will sort the courses according to the starting date\n",
    "grouped_data['Course'] = pd.to_numeric(grouped_data['Course']).astype(int)\n",
    "\n",
    "#finally we create the pivot_table that we will use to create our heatmap\n",
    "grouped_data = grouped_data.set_index('Course', drop = True)\n",
    "grouped_data.T.describe(include = 'all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context='paper', style='whitegrid', font='Calibri', rc={\"figure.figsize\":(20, 12)}, font_scale=2)\n",
    "\n",
    "#here, we are plotting the nex\n",
    "heat4 = sns.heatmap(grouped_data, robust=True, norm=LogNorm(), xticklabels = 2, yticklabels= 1,\n",
    "            cmap = standard_cmap, cbar_kws={'label': 'Weekly interactions'})\n",
    "\n",
    "fig = heat4.get_figure()\n",
    "fig.savefig('../Images/highest_attendance_weekly_clicks_heat4.png', transparent=True, dpi=300)\n",
    "\n",
    "#delete to remove from memory\n",
    "del fig, heat4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b59e4c",
   "metadata": {},
   "source": [
    "While most courses are very clearly restricted to their semester, there are courses that have interactions occurring across the entire year. \n",
    "\n",
    "For these courses, we just want to undestand whether all students are interacting continuously or we are speaking of different co-horts of students. As such, we will look more deeply at the following courses:\n",
    "\n",
    "3022, 3069 and 3151"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_long_high_attendance = ['3022.0', '3069.0', '3151.0']\n",
    "\n",
    "#Then, when it comes to logs, we aggregate by week\n",
    "grouped_data = high_attendance_logs[high_attendance_logs['course'].isin(year_long_high_attendance)].groupby([pd.Grouper(key='time', freq='W'), 'course', 'userid']).agg({\n",
    "                                                                             'action': 'count',\n",
    "                                                                             }).reset_index().sort_values('time')\n",
    "#change for better reading\n",
    "grouped_data['Date (week)'] = grouped_data['time'].astype(str)\n",
    "\n",
    "#creating pivot table to create heatmap\n",
    "grouped_data = grouped_data.pivot_table(index =['course', 'userid'], \n",
    "                       columns = 'Date (week)',\n",
    "                        values = 'action', \n",
    "                       aggfunc =np.sum,\n",
    "                        fill_value=np.nan).reset_index().rename(columns = {'course' : 'Course'})\n",
    "\n",
    "#now, we will sort the courses according to the starting date\n",
    "grouped_data['Course'] = pd.to_numeric(grouped_data['Course']).astype(int)\n",
    "\n",
    "#finally we create the pivot_table that we will use to create our heatmap\n",
    "grouped_data = grouped_data.set_index(['Course', 'userid'], drop = True)\n",
    "grouped_data.T.describe(include = 'all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context='paper', style='whitegrid', font='Calibri', rc={\"figure.figsize\":(20, 12)}, font_scale=2)\n",
    "\n",
    "#here, we are plotting the nex\n",
    "heat5 = sns.heatmap(grouped_data, robust=True, norm=LogNorm(), xticklabels = 2, yticklabels= 0,\n",
    "            cmap = standard_cmap, cbar_kws={'label': 'Weekly interactions'})\n",
    "\n",
    "fig = heat5.get_figure()\n",
    "fig.savefig('../Images/high_attend_yearlong_weekly_heat5.png', transparent=True, dpi=300)\n",
    "\n",
    "#delete to remove from memory\n",
    "del fig, heat5, grouped_data, high_attendance_logs, year_long_high_attendance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0d0177",
   "metadata": {},
   "source": [
    "After consideration, we find that the student interactions seem to be consistent with the course duration. \n",
    "\n",
    "We note, however, that the accesses to course 4923 seem to be inconsistent at best. We will monitor this course (and others) in the following steps. For now, we will proceed with the analysis over targets and support table.\n",
    "\n",
    "**1. First, we filter by our current list of valid courses.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384a3ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representation of different targets depending \n",
    "g = sns.PairGrid(targets_table, diag_sharey=False, corner=True)\n",
    "g.map_diag(sns.histplot)\n",
    "g.map_lower(sns.scatterplot)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816df8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a larger overlook at the different courses\n",
    "targets_table.groupby('course').agg({\n",
    "                                    'userid' : 'count', \n",
    "                                    'Grade Mandatory' : ['min', 'mean', 'max'],\n",
    "                                    'Grade Optional' : ['min', 'mean', 'max'],                                    \n",
    "                                    'final_mark' : ['min', 'mean', 'max'],\n",
    "                                    }).describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7ba06",
   "metadata": {},
   "source": [
    "#### Finally, we will take a look at the support table we have and repeat the same steps performed thus far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd19e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate logs accordingly\n",
    "support_table = support_table[support_table['assign_id'].isin(student_logs['cmid'])].rename(columns = {\n",
    "                                                                                            'courseid' : 'course'\n",
    "                                                                                            })\n",
    "#filter student logs_approppriately\n",
    "student_logs = student_logs[student_logs['course'].isin(support_table['course'])].reset_index(drop = True)\n",
    "\n",
    "#get info\n",
    "support_table.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f85ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77969b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a926381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a larger overlook at the different courses\n",
    "support_table.groupby('course').agg({\n",
    "                                    'userid' : 'count',\n",
    "                                    'assign_id' : 'count', \n",
    "                                    'mandatory_status' : 'mean',\n",
    "                                    'delivered' : 'mean',                                    \n",
    "                                    'assignment_mark' : 'mean',\n",
    "                                    }).describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b1761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representation of different targets depending \n",
    "g = sns.PairGrid(support_table, diag_sharey=False, corner=True)\n",
    "g.map_diag(sns.histplot)\n",
    "g.map_lower(sns.scatterplot)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d2168",
   "metadata": {},
   "source": [
    "**Going forward**.\n",
    "\n",
    "After this preliminary look, we will go forward with extracting features from the Moodle logs. \n",
    "\n",
    "In this notebook, we will consider a static non-temporal representation that considers each student-course pair as a row. We will, however, construct different datasets - 1 for each relevant timestep. \n",
    "\n",
    "We will rely on features that are regular presences in the literature. Some of these features may appear in more than one work:\n",
    "\n",
    "**From Macfadyen et al. (2010)**\n",
    "\n",
    "Count related features:\n",
    "- Discussion messages posted, \n",
    "- Online Sessions, \n",
    "- File views,\n",
    "- Assessments finished, \n",
    "- Assessments started, \n",
    "- Replies to discussion messages, \n",
    "- Mail messages sent, \n",
    "- Assignments submitted, \n",
    "- Discussion MEssages read, \n",
    "- Web link views\n",
    "\n",
    "Time related features:\n",
    "- Total time online, \n",
    "- Time spent on assignments,\n",
    "\n",
    "\n",
    "**From Romero et al. (2013)**\n",
    "\n",
    "Number Accesses to:\n",
    "- Assignments done,\n",
    "- Quizzes passed,\n",
    "- Quizzes failed,\n",
    "- Forum messages posted, \n",
    "- Forum messages read,\n",
    "\n",
    "Time related features:\n",
    "- Total time on assignments, \n",
    "- Total time on quizzes, \n",
    "- Total time on forums\n",
    "\n",
    "**From Gasevic et. al (2016)**\n",
    "\n",
    "Number of Accesses of the following variables:\n",
    "- course logins,\n",
    "- forum,\n",
    "- resources,\n",
    "- Turnitin file submission,\n",
    "- assignments,\n",
    "- book,\n",
    "- quizzes, \n",
    "- feedback,\n",
    "- lessons,\n",
    "- virtual classroom\n",
    "- chat,\n",
    "\n",
    "- etc...\n",
    "\n",
    "**From Conijn et. al (2017)**\n",
    "\n",
    "Click count related features:\n",
    "- Clicks,\n",
    "- Online sessions, \n",
    "- Course page views,\n",
    "- Resources viewed,\n",
    "- Links viewed, \n",
    "- Discussion post views,\n",
    "- Content page views,\n",
    "- Quizzes,\n",
    "- Quizzes passed,\n",
    "- Assignments submitted, \n",
    "- Wiki edits,\n",
    "- Wiki views,\n",
    "\n",
    "Time related features:\n",
    "- Total time online,\n",
    "- Largest period of inactivity,\n",
    "- Time until first action, \n",
    "- Averages session time,\n",
    "\n",
    "Performance related features:\n",
    "- Average assignment grade,\n",
    "\n",
    "**Chen and Cui (2020)**\n",
    "\n",
    "Click count related features:\n",
    "- Total clicks, \n",
    "- Clicks on campus, \n",
    "- Online sessions,\n",
    "- Clicks during weekdays,\n",
    "- Clicks on weekend,\n",
    "- Assignments, \n",
    "- File,\n",
    "- Forum,\n",
    "- Overview Report,\n",
    "- Quizz,\n",
    "- System, \n",
    "- User Report\n",
    "\n",
    "Time related features\n",
    "- Total time of online sessions, \n",
    "- Mean duration of online sessions, \n",
    "- SD of time between sessions, \n",
    "- Total time on Quiz, \n",
    "- Total time on File, \n",
    "- SD of time on File, \n",
    "\n",
    "Other statistics\n",
    "- Ratio between on-campups and off-campus clicks\n",
    "\n",
    "**Nuno RosÃ¡rio Thesis**\n",
    "- number of forum messages read, \n",
    "- number of forum messages posted, \n",
    "- number of pages,\n",
    "- number of clicks, \n",
    "- number of submissions,\n",
    "- number of files accessed,\n",
    "\n",
    "As stated, some of the features are calculated across multiple works - and these only address the course level. They are not designed specifically for course-agnostic purposes.\n",
    "\n",
    "First, **we will split the logs by the difference courses and, for each student calculate the different features we intend to calculate.** The are calculated via aggregate operations, from the most common (appear more times in our literature).\n",
    "\n",
    "**But before that**, we will look at different columns of our logs and, when appropriate, keep the different values they may take. The first, and most immediate correction is IP - 127.0.0.1 hints at local connection, while other IPs suggest an out of Campus connection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ba0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts up to on_campus\n",
    "student_logs['on_campus'] = np.where(student_logs['ip'] == '127.0.0.1', 1, 0)\n",
    "student_logs.drop('ip', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae1251e",
   "metadata": {},
   "source": [
    "Secondly, we take a look at the actions and the modules. Here, we can find the most common actions and modules.\n",
    "\n",
    "We are familiar with the most common features: course, resources, assignments, quiz, forums, etc...\n",
    "\n",
    "There are, however, other less common labels whose usage is not very common. We can, start by grouping together the less common modules together in a way that, at least intuitively, makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#other pages with unclear meaning will be grouped together\n",
    "other_modules = ['oublog', 'data', 'data', 'bigbluebuttonbn', 'nanogong', 'role', 'notes', 'calendar', 'recordingsbn', 'bookmark']\n",
    "\n",
    "#converts discussion points to forum or, alternatively,groups other elements to other category\n",
    "student_logs['module'] = np.where(student_logs['module'] == 'discussion', 'forum',\n",
    "                                  np.where(student_logs['module'] == 'imscp', 'resource', #imscp is what allows content packages to be posted\n",
    "                                  np.where(student_logs['module'] == 'glossary', 'course', #usually, glossaries refer to course\n",
    "                                  np.where(student_logs['module'].isin(other_modules), 'others', student_logs['module']))))\n",
    "\n",
    "del other_modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beecaa6f",
   "metadata": {},
   "source": [
    "Likewise, we will need to take a look at the different actions in order to understand how common these may be. \n",
    "\n",
    "Again, we will look at different actions and see how we can group them together in a way that, at least, makes intuitive sense. There is use in keeping the distinction between different types of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#updates and edits related to making editions on presented information:\n",
    "update_related = ['edit post', 'edit override',  'edit report', 'update mod'\n",
    "                  'update submission', 'update entry', 'editsection', 'update switch phase',\n",
    "               'update assessment',   'update post', 'editquestions', 'update submission', 'edit', 'update mod']\n",
    "\n",
    "#additions \n",
    "addition = ['add assessment', 'add category', 'add item', 'add submission', 'add page', 'add mod', 'add entry', 'add post', 'add discussion',\n",
    "            'add', 'save report','add comment']\n",
    "\n",
    "#deletion\n",
    "deletion = ['delete override', 'record delete', 'delete entry', 'delete', 'delete post',\n",
    "            'delete discussion', 'delete attempt','delete mod',]\n",
    "\n",
    "#likewise, we will also join other moderation related tasks together,\n",
    "moderation = ['restore', 'save', 'unlock submission', 'stop tracking', 'assign', 'lock submission', 'usage report',\n",
    "              'start tracking', 'view subscribers', 'grade submission', 'grant extension', 'manualgrade']\n",
    "\n",
    "#other reporting actions\n",
    "report = ['report live', 'report participation', 'report', 'report log', 'report outline', 'view report', 'user report']\n",
    "\n",
    "#messages/files\n",
    "messages = ['message saved', 'message sent', 'files']\n",
    "\n",
    "#other actions\n",
    "other_actions = ['preview', 'unsubscribe', 'download all submissions', 'mark read', 'subscribeall', 'diff', 'comment', \n",
    "                 'unsubscribeall', 'search', 'history', 'map', 'subscribe', 'submissioncopied', 'comments']\n",
    "\n",
    "#smaller view commands to join main view\n",
    "small_view = ['view entry', 'view edit']\n",
    "\n",
    "#converts discussion points to forum or, alternatively,groups other elements to other category\n",
    "student_logs['action'] = np.where(student_logs['action'].isin(update_related), 'update', #edit list\n",
    "                                  np.where(student_logs['action'].isin(addition), 'addition', #addition list\n",
    "                                  np.where(student_logs['action'].isin(deletion), 'delete', #deletion list\n",
    "                                  np.where(student_logs['action'].isin(moderation), 'other admin actions', #other admin actions\n",
    "                                  np.where(student_logs['action'].isin(report), 'report', #reporting related\n",
    "                                  np.where(student_logs['action'].isin(other_actions), 'other actions', #other actions\n",
    "                                  np.where(student_logs['action'].isin(messages), 'messages and files', #messages\n",
    "                                  np.where(student_logs['action'].isin(small_view), 'view others', #main view files\n",
    "                                  np.where(student_logs['action'] == 'choose again','choose', #choose again to choose\n",
    "                                  \n",
    "                                  #finishing with splitting the view command to different subgroups according to the module - will make it easier later\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'assign'),'view assignment',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'choice'),'view choice',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'course'),'view course',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'folder'),'view folder',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'glossary'),'view glossary',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'others'),'view others',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'page'),'view page',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'questionnaire'),'view questionnaire',\n",
    "                                  np.where(((student_logs['action'] == 'view') | (student_logs['action'] == 'view all')) & (student_logs['module'] == 'resource'),'view resource',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'url'),'view url',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'user'),'view user',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'wiki'),'view wiki',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'workshop'),'view workshop',\n",
    "                                  np.where((student_logs['action'] == 'view') & (student_logs['module'] == 'quiz'),'view quiz',                             \n",
    "                                  np.where((student_logs['action'] == 'view forums') & (student_logs['module'] == 'forum'),'view forum',\n",
    "                                  np.where((student_logs['action'] == 'submit for grading') & (student_logs['module'] == 'assign'),'submit',\n",
    "                                  np.where((student_logs['action'] == 'view submit assignment form') & (student_logs['module'] == 'assign'),'view assignment',\n",
    "                                  student_logs['action']))))))))))))))))))))))))))\n",
    "\n",
    "#we finish by ending these lists we've created\n",
    "del update_related, addition, deletion, moderation, messages, other_actions, small_view "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad0e00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#uncomment to verify pairings\n",
    "with pd.option_context('display.max_rows', None,):\n",
    "     display(student_logs.groupby(['module', 'action']).size().to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40729acb",
   "metadata": {},
   "source": [
    "We have addressed the most obvious possible aggregations. Now, we will go forward with our intended feature extraction and selection.\n",
    "\n",
    "For this step, we will create 5 distinct dicts of dataframes. Each dict refers to a certain course duration threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc5abe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of days\n",
    "days = {}\n",
    "\n",
    "#additionally, we will look at our estimated course duration\n",
    "for i in tqdm(duration_threshold):\n",
    "    #create, for each desired threshold, the appropriate cutoff date \n",
    "    class_list[f'Date_threshold_{int(i*100)}'] = pd.to_datetime((class_list['Start Date'] + pd.to_timedelta(class_list['Course duration days'] * i, unit = 'Days')).dt.date)\n",
    "    \n",
    "    #setting up duration threshold to be on friday -> reason being that it will be easier to \n",
    "    class_list[f'Date_threshold_{int(i*100)}'] = class_list[f'Date_threshold_{int(i*100)}'].where( class_list[f'Date_threshold_{int(i*100)}'] == (( class_list[f'Date_threshold_{int(i*100)}'] + Week(weekday=4)) - Week()), class_list[f'Date_threshold_{int(i*100)}'] + Week(weekday=4))\n",
    "    \n",
    "    #storing date threshold and week, week before start to consider calculate features relative to course duration\n",
    "    days[f'Date_threshold_{int(i*100)}'] = deepcopy(class_list.filter(['course', 'Start Date', 'Week before start', 'End Date',\n",
    "                                                                        f'Date_threshold_{int(i*100)}']))\n",
    "    \n",
    "    days[f'Date_threshold_{int(i*100)}']['Number of days'] = (days[f'Date_threshold_{int(i*100)}'][f'Date_threshold_{int(i*100)}'] - days[f'Date_threshold_{int(i*100)}'][f'Week before start']).dt.days\n",
    "    #then, we will create a dictionary of dictionaries, each main dictionary storing and a version of the logs\n",
    "logs_dict = {}\n",
    "assignment_dict = {}\n",
    "\n",
    "for i in tqdm(duration_threshold):\n",
    "    #create, for each desired threshold, a different dictionary of dataframes wherein we will perform the different operations\n",
    "    print(f'Date_threshold_{int(i*100)}\\n' +\n",
    "          f'Logs')\n",
    "    logs_dict[f'Date_threshold_{int(i*100)}'] = {course: student_logs.loc[student_logs['course'] == course].reset_index(drop = True) for course in tqdm(student_logs['course'].unique())}\n",
    "    \n",
    "    #Assignments\n",
    "    print(f'Assignments')\n",
    "    assignment_dict[f'Date_threshold_{int(i*100)}'] = {course: support_table.loc[support_table['course'] == course].reset_index(drop = True) for course in tqdm(support_table['course'].unique())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d90d8b1",
   "metadata": {},
   "source": [
    "Now, we have a nested dictionary with different dataframes inside it. We will use this data structure to perform the most of the operations we are interested in.\n",
    "\n",
    "**First, we will add, to each dataframe, a column with the corresponding threshold date**\n",
    "\n",
    "After this cleaning procedure, we will all different columns referring to our features of interest. These will be:\n",
    "1. Number of assignments submitted, \n",
    "2. Number of online sessions,\n",
    "3. Discussion messages read,\n",
    "4. Resource views, \n",
    "5. Assessments started,\n",
    "6. Total time online,\n",
    "7. Assignment views,\n",
    "8. Average duration of session,\n",
    "9. Messages posted,\n",
    "10. Clicks on Forum, \n",
    "11. Clicks on Folder\n",
    "12. On-campus clicks,\n",
    "13. On-campus/off-campus clicks,\n",
    "14. Total number of clicks\n",
    "15. Number of links viewed\n",
    "16. Largest period of inactivity\n",
    "17. Average clicks per day\n",
    "18. Average clicks per session\n",
    "19. The start date of the first 10 sessions (relative to the entire course duration)\n",
    "20. % of Submissions made in the period,\n",
    "21. % of clicks made in the period,\n",
    "\n",
    "To check difference between inclusion and not inclusion\n",
    "\n",
    "22. Average grade of assignments (optional)\n",
    "\n",
    "A double loop is not very efficient but, to the best of my ability, is the obvious solution to perform these operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d0504b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for each intended course duration threshold\n",
    "for i in tqdm(logs_dict):\n",
    "    #start with creating a dictionary of course and intended cuttoff date\n",
    "    cut = class_list.set_index('course').to_dict()[i] \n",
    "    \n",
    "    #for each dataframe\n",
    "    for j in tqdm(logs_dict[i]):\n",
    "        #where the course is the same as in the class_list, get the corresponding value of the appropriate column,\n",
    "        logs_dict[i][j]['Date Threshold'] = logs_dict[i][j]['course'].map(cut)\n",
    "        logs_dict[i][j] = logs_dict[i][j][logs_dict[i][j]['time'] <= logs_dict[i][j]['Date Threshold']].reset_index(drop = True).drop('Date Threshold', axis = 1)\n",
    "        \n",
    "        #doing the same for the assignments list\n",
    "        try:\n",
    "            assignment_dict[i][j]['Date Threshold'] = assignment_dict[i][j]['course'].map(cut)\n",
    "            assignment_dict[i][j] = assignment_dict[i][j][assignment_dict[i][j]['sup_time'] <= assignment_dict[i][j]['Date Threshold']].reset_index(drop = True).drop('Date Threshold', axis = 1)\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        #calculates the difference between previous within group row and current\n",
    "        logs_dict[i][j]['t_diff'] = logs_dict[i][j].sort_values(['userid', 'time']).groupby('userid')['time'].diff()\n",
    "        \n",
    "        #will need to ignore dictionaries where there is no lenght\n",
    "        if len(logs_dict[i][j]) > 0:\n",
    "            #the nans will be correspond to the first interaction made by each student - also signaling the start of the first session\n",
    "            logs_dict[i][j]['session'] = np.where(logs_dict[i][j]['t_diff'].isna(), 1, #the first session is started by nans\n",
    "                                             np.where(logs_dict[i][j]['t_diff'] > pd.to_timedelta(40, unit = 'minutes'), 1, #also identify the starting point of new sessions\n",
    "                                                      0))\n",
    "            \n",
    "            #then, we cumulative sum all in-group members \n",
    "            logs_dict[i][j]['session'] = logs_dict[i][j].groupby('userid')['session'].transform(pd.Series.cumsum)\n",
    "            \n",
    "            #before finishing this step, we will calculate the accumulated duration of a session\n",
    "            logs_dict[i][j]['mask'] = np.where(logs_dict[i][j]['t_diff'].isna(), 0, #the first session is started by nans\n",
    "                                      np.where(logs_dict[i][j]['t_diff'] > pd.to_timedelta(40, unit = 'minutes'), 0, #also identify the starting point of new sessions\n",
    "                                      logs_dict[i][j]['t_diff'].dt.total_seconds()))\n",
    "            #fillnas in t_diff\n",
    "            logs_dict[i][j]['t_diff'].fillna(pd.to_timedelta(0), inplace = True)\n",
    "\n",
    "            #then, we cumulative sum all in-group members \n",
    "            logs_dict[i][j]['session_cumul_time'] = logs_dict[i][j].groupby(['userid','session'])['mask'].transform(pd.Series.cumsum)\n",
    "            logs_dict[i][j]['session_cumul_time'] = pd.to_timedelta(logs_dict[i][j]['session_cumul_time'], unit = 'seconds')\n",
    "            #drop mask\n",
    "            logs_dict[i][j].drop('mask', axis = 1, inplace = True)\n",
    "        \n",
    "        else:\n",
    "            logs_dict[i][j] = pd.DataFrame(columns=['time', 'userid', 'course', 'module', 'cmid', 'action', 'on_campus', 't_diff',\n",
    "                                                    'session', 'session_cumul_time'])\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1abf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create backup of logs dict, we will need it for later\n",
    "#backup = deepcopy(logs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7815b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logs_dict = deepcopy(backup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a02de1",
   "metadata": {},
   "source": [
    "Now, we'll go forward with the creation of the features using these datasets. We will do it, using groupby commands.\n",
    "\n",
    "After this cleaning procedure, we will all different columns referring to our features of interest.\n",
    "\n",
    "**We cannot perform all steps at once, unfortunately.** (at least not in a capacity I can manage)\n",
    "\n",
    "We will need to create multiple dfs to ensure that all features are accounted for:\n",
    "\n",
    "1. We start with features that relate to raw aggregate counts of clicks and sessions - a general set of features, \n",
    "2. We continue by computing features related with time -  total time online and average duration of session,\n",
    "3. Then, we go into finer grained features using specific pairs of modules and actions.\n",
    "4. Then, we finish by merging these features with the final mark we have previously calculated and the average grade of assignments delivered up to the threshold date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f033e29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#we will need to perform the same double loop we have done before\n",
    "for i in tqdm(logs_dict):\n",
    "     \n",
    "    for j in tqdm(logs_dict[i]):    \n",
    "        #as it is very difficult to we will need to create multiple placeholders\n",
    "        #placeholder 1 - general features\n",
    "        general_features = logs_dict[i][j].groupby(['course', 'userid']).agg(\n",
    "                                                {'action' : [('N_clicks','count')], #number of clicks\n",
    "                                                'session' : [('N_sessions', 'nunique')], #number of sessions\n",
    "                                                 'on_campus' : [('Clicks on Campus', np.sum)], #number of clicks on campus\n",
    "                                                 't_diff' : [('Largest_period_of Inactivity' , 'max')], #largest period of inactivity \n",
    "                                                })\n",
    "        \n",
    "        #the second group will deal with session related time features\n",
    "        session_features = logs_dict[i][j].groupby(['userid', 'session'])['session_cumul_time'].max().to_frame().reset_index() #the accumulated time up to last click of each session identifies the duration \n",
    "        \n",
    "        #now, we get to our intended features\n",
    "        session_features = session_features.groupby(['userid']).agg(\n",
    "                                                {'session_cumul_time' : [np.sum, #The total time online is the sum of the time spent in all sessions \n",
    "                                                                        np.mean], #mean duration across all sessions made by the student\n",
    "                                                 })\n",
    "        \n",
    "        #start og multiple session\n",
    "        start_of_sessions = logs_dict[i][j].groupby(['userid', 'session']).agg({\n",
    "                                                                                'time' : 'min',\n",
    "                                                                                }).reset_index()\n",
    "        \n",
    "        #we will get the start dates of the first 10 sessions\n",
    "        start_of_sessions = pd.pivot_table(start_of_sessions, index = 'userid', columns = 'session', values = 'time',\n",
    "                            aggfunc = 'min').reset_index()\n",
    "        \n",
    "        # We reindex pivot to contain all columns we intend to have, even if they are note present\n",
    "        start_of_sessions = start_of_sessions.reindex(columns = ['userid', 1, 2, 3, 4, 5,\n",
    "                                                                6, 7, 8, 9, 10], fill_value = np.nan).rename(\n",
    "                                                                    columns = {1 : 'Start of Session 1 (%)', \n",
    "                                                                               2 : 'Start of Session 2 (%)',\n",
    "                                                                               3 : 'Start of Session 3 (%)',\n",
    "                                                                               4 : 'Start of Session 4 (%)',\n",
    "                                                                               5 : 'Start of Session 5 (%)',\n",
    "                                                                               6 : 'Start of Session 6 (%)', \n",
    "                                                                               7 : 'Start of Session 7 (%)',\n",
    "                                                                               8 : 'Start of Session 8 (%)',\n",
    "                                                                               9 : 'Start of Session 9 (%)',\n",
    "                                                                               10 : 'Start of Session 10 (%)',\n",
    "                                                                              })\n",
    "        \n",
    "        #the third relies on clicks of multiple types and modules. An elegant way is to deal with these is pivot_tables of the counts\n",
    "        pivot = pd.pivot_table(logs_dict[i][j], index = 'userid', \n",
    "                              columns = ['module', 'action'],\n",
    "                              values='cmid',\n",
    "                              aggfunc = 'count')\n",
    "        \n",
    "        #applies the function that removes multiindex\n",
    "        pivot.columns = pivot.columns.map(flattenHierarchicalCol)\n",
    "        pivot.reset_index(inplace = True)\n",
    "        \n",
    "        #now, we filter the pivot table to only keep the features that we are interested in - specifically, the counts\n",
    "        pivot = pivot.reindex(columns = [\n",
    "                               'userid',\n",
    "                               'assign_submit', #Number of assignments submitted\n",
    "                               'resource_view resource', #resource views,\n",
    "                               'assign_view assignment', #view assignment\n",
    "                               'forum_view discussion', #view discussion,\n",
    "                               'quiz_attempt', #quizzes started\n",
    "                               'forum_addition', #forum messages posted\n",
    "                                'folder_view all', #view all folders\n",
    "                                ])\n",
    "        \n",
    "        #drop columns unnecessary columns and rename others \n",
    "        pivot = pivot.rename(columns = {'forum_addition' : 'Forum posts', \n",
    "                                            'forum_view discussion' : 'Discussions viewed',\n",
    "                                            'assign_submit' : 'Assignments submitted', \n",
    "                                            'resource_view resource' : 'Resources viewed',\n",
    "                                            'quiz_attempt' : 'Quizzes started',  \n",
    "                                            'assign_view assignment' : 'Assignments viewed'\n",
    "                                           })\n",
    "        \n",
    "        #the third relies on clicks of multiple types and modules. An elegant way is to deal with these is pivot_tables of the counts\n",
    "        pivot_1 = pd.pivot_table(logs_dict[i][j], index = 'userid', \n",
    "                              columns = 'module',\n",
    "                              values='cmid',\n",
    "                              aggfunc = 'count').reset_index()\n",
    "        \n",
    "        #now, we filter the pivot table to only keep the features that we are interested in - specifically, the counts\n",
    "        pivot_1 = pivot_1.filter([\n",
    "                               'userid',\n",
    "                               'forum',\n",
    "                                'url',\n",
    "                                'folder',\n",
    "                                'course', \n",
    "                                ], \n",
    "                               ).rename(columns = {'forum' : 'Clicks on forum',\n",
    "                                                    'url': 'Links viewed',\n",
    "                                                   'folder' : 'Clicks on folder',\n",
    "                                                   'course' : 'Clicks on course',\n",
    "                                                  })\n",
    "\n",
    "        #applies the function that removes multiindex\n",
    "        general_features.columns = general_features.columns.map(flattenHierarchicalCol)\n",
    "        general_features.reset_index(inplace = True)\n",
    "        \n",
    "        #same for session features\n",
    "        session_features.columns = session_features.columns.map(flattenHierarchicalCol)\n",
    "        session_features.reset_index(inplace = True)\n",
    "\n",
    "        #merging the timestamp that marks the start of the first 5 sessions\n",
    "        session_features = pd.merge(session_features, start_of_sessions, on = 'userid')\n",
    "        \n",
    "        #we finish this section by wrapping everything together\n",
    "        general_features = pd.merge(general_features, session_features, on = 'userid', how = 'inner')\n",
    "        general_features.rename(columns = {'session_cumul_time_sum': 'Total time online (min)',\n",
    "                                           'session_cumul_time_mean': 'Average session duration (min)',\n",
    "                                           'action_N_clicks': 'Number of clicks',\n",
    "                                           'session_N_sessions': 'Number of sessions',\n",
    "                                           'on_campus_Clicks on Campus': 'Clicks on campus',\n",
    "                                           't_diff_Largest_period_of Inactivity': 'Largest period of inactivity (h)',\n",
    "                                          }, inplace = True)\n",
    "        \n",
    "        #merge features from pivot_table\n",
    "        pivot = pd.merge(pivot_1, pivot, on = 'userid', how = 'inner')\n",
    "        \n",
    "        #joining assignment grades\n",
    "        assignment_pivot = pd.pivot_table(assignment_dict[i][j], index = 'userid', \n",
    "                              columns = 'assign_id',\n",
    "                              values='assignment_mark',\n",
    "                              aggfunc = np.sum)\n",
    "        \n",
    "        #drop assignments that either were not delivered or received grade = 0  \n",
    "        assignment_pivot = assignment_pivot.dropna(axis=1, how='all')\n",
    "        \n",
    "        #now, we stack these together \n",
    "        assignment_pivot = assignment_pivot.stack().reset_index().rename(columns = {0 : 'Average grade of assignments'})\n",
    "        \n",
    "        #join all together to get the corresponding dataframe\n",
    "        logs_dict[i][j] = pd.merge(general_features, pivot, on = 'userid', how = 'inner')\n",
    "        \n",
    "        #calculating on-campus/off campus ratio\n",
    "        logs_dict[i][j]['On/off campus click ratio'] = np.where((logs_dict[i][j]['Number of clicks'] - logs_dict[i][j]['Clicks on campus']) > 0,\n",
    "                                                                logs_dict[i][j]['Clicks on campus'] / (logs_dict[i][j]['Number of clicks'] - logs_dict[i][j]['Clicks on campus']),\n",
    "                                                                logs_dict[i][j]['Clicks on campus']) # we consider 1 click off campus to avoid dividing by 0 \n",
    "        \n",
    "        #Merge here with days - this will allow us to get first action\n",
    "        logs_dict[i][j] = logs_dict[i][j].merge(days[i], on = ['course'])\n",
    "        \n",
    "        #additional features to compute\n",
    "        logs_dict[i][j]['Clicks per day'] = logs_dict[i][j]['Number of clicks'] / logs_dict[i][j]['Number of days'] #clicks per day\n",
    "        logs_dict[i][j]['Clicks per session'] = logs_dict[i][j]['Number of clicks'] / logs_dict[i][j]['Number of sessions'] #clicks per session\n",
    "        \n",
    "        #now computing other features in relative terms:\n",
    "        logs_dict[i][j]['Clicks (% of course total)'] = logs_dict[i][j]['Number of clicks'] / logs_dict[i][j]['Number of clicks'].sum()\n",
    "        logs_dict[i][j]['Submissions (% of course total)'] = logs_dict[i][j]['Assignments submitted'] / logs_dict[i][j]['Assignments submitted'].sum() #avoid dividing by 0 \n",
    "        \n",
    "        #joining final grade for target\n",
    "        logs_dict[i][j] = logs_dict[i][j].merge(targets_table.filter(['course', 'userid', 'final_mark']), on = ['course', 'userid'], how = 'right')\n",
    "        \n",
    "        #changing column dtype to reduce space\n",
    "        logs_dict[i][j][logs_dict[i][j].select_dtypes(np.float64).columns] = logs_dict[i][j].select_dtypes(np.float64).astype(np.float32)\n",
    "        logs_dict[i][j][logs_dict[i][j].select_dtypes(np.int32).columns] = logs_dict[i][j].select_dtypes(np.float64).astype(np.int16)\n",
    "        #joining with assignments\n",
    "        if len(assignment_pivot) > 0:\n",
    "                    \n",
    "            #now, and get the mean of non-zero mean assignments - averaged by all students attending the course\n",
    "            assignment_pivot = assignment_pivot.groupby(['userid'])['Average grade of assignments'].mean().reset_index()\n",
    "            \n",
    "            #and merge with final result\n",
    "            logs_dict[i][j] = logs_dict[i][j].merge(assignment_pivot, on = 'userid', how = 'left')\n",
    "            \n",
    "        #clean unnecessary dfs\n",
    "        del pivot, pivot_1, general_features, session_features, assignment_pivot\n",
    "    \n",
    "    #after the end of the loops:\n",
    "    logs_dict[i] = pd.concat(logs_dict[i], ignore_index=True)\n",
    "    logs_dict[i] = logs_dict[i].sort_values(by = ['course', 'userid', 'final_mark']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0a0aae",
   "metadata": {},
   "source": [
    "In order to account for situations where registered students only access Moodle later in the course, we will make ann additional, but necessary adaptation. \n",
    "\n",
    "We will start by looking at the complete set of valid students/courses in our 100% dataset. From these, we get the indexes of the rows that are valid (i.e. have a valid click count at 100% duration), get the indexes and retain only these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cffea72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#we gather the index number of valid rows in the 100% df\n",
    "rows_to_keep = logs_dict['Date_threshold_100'][~logs_dict['Date_threshold_100']['Number of clicks'].isna()].index\n",
    "columns_copy = ['course', 'userid']\n",
    "\n",
    "#first filter the date treshold for entire course\n",
    "logs_dict['Date_threshold_100'] = logs_dict['Date_threshold_100'].iloc[rows_to_keep, :].reset_index(drop = True)\n",
    "\n",
    "#Convert timedelta format to numbered format - minutes\n",
    "logs_dict['Date_threshold_100']['Total time online (min)'], logs_dict['Date_threshold_100']['Average session duration (min)'] = logs_dict['Date_threshold_100']['Total time online (min)'].dt.total_seconds() / 60, logs_dict['Date_threshold_100']['Average session duration (min)'].dt.total_seconds() / 60\n",
    "logs_dict['Date_threshold_100']['Largest period of inactivity (h)'] = logs_dict['Date_threshold_100']['Largest period of inactivity (h)'].dt.total_seconds() // 60 / 60\n",
    "# #then slice accordingly\n",
    "for i in tqdm(list(logs_dict.keys())[:-1]):\n",
    "    logs_dict[i] = logs_dict[i].iloc[rows_to_keep, :].reset_index(drop = True)\n",
    "    #we will need to keep some columns this way - students that made noa ction prior\n",
    "    logs_dict[i][columns_copy] = deepcopy(logs_dict['Date_threshold_100'][columns_copy])\n",
    "    \n",
    "    #Convert timedelta format to numbered format - minutes\n",
    "    logs_dict[i]['Total time online (min)'], logs_dict[i]['Average session duration (min)'] = logs_dict[i]['Total time online (min)'].dt.total_seconds() / 60, logs_dict[i]['Average session duration (min)'].dt.total_seconds() / 60\n",
    "    logs_dict[i]['Largest period of inactivity (h)'] = logs_dict[i]['Largest period of inactivity (h)'].dt.total_seconds() // 60"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd519fd6",
   "metadata": {},
   "source": [
    "### Now , we calculate the remaining features \n",
    "\n",
    "In specific features related to the time each session starts.\n",
    "\n",
    "The % of course duration time passed at each login between the first 10 logins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe19a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new features related to the different sessions start time relative to the start of the course\n",
    "for i in tqdm(logs_dict):\n",
    "    \n",
    "    #columns mentioning start of session\n",
    "    for k in start_of_sessions.columns[1:]:\n",
    "        logs_dict[i][k] =  ((logs_dict[i][k] - logs_dict[i]['Start Date']).dt.days / (logs_dict[i]['End Date'] - logs_dict[i]['Start Date']).dt.days) * 100\n",
    "        \n",
    "    logs_dict[i].drop(['Start Date', 'End Date', f'{i}', 'Week before start'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64a0b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dict['Date_threshold_10'].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c7924",
   "metadata": {},
   "source": [
    "#### Almost Done.\n",
    "\n",
    "We will finish the Feature Extraction Stage momentarily. Before we do, we need to save all dfs in an easily accessible Excel File."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512405bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('../Data/Modeling Stage/R_gonz_Non_temporal_Datasets.xlsx', engine='xlsxwriter')\n",
    "\n",
    "#now loop thru and put each on a specific sheet\n",
    "for sheet, frame in  logs_dict.items(): \n",
    "    frame.to_excel(writer, sheet_name = sheet)\n",
    "\n",
    "#critical last step\n",
    "writer.save()\n",
    "\n",
    "#also saving additional info on class list\n",
    "class_list.to_csv('../Data/Modeling Stage/R_Gonz_updated_classlist.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
