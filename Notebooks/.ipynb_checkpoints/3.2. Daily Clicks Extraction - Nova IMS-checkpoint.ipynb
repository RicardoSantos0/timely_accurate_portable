{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adc58434",
   "metadata": {},
   "source": [
    "In this notebook, we will finally create predictive features using the logs we cleaned on notebook 2.2. \n",
    "\n",
    "Our focus will be to obtain the Temporal Data. We consider this to be the number of daily clicks \n",
    "\n",
    "#### 1. Importing the relevant packages, setting global variables and importing the relevant files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e2e9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.tseries.offsets import *\n",
    "\n",
    "#viz related tools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LogNorm, Normalize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "#tqdm to monitor progress\n",
    "from tqdm.notebook import tqdm, trange\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "#time related features\n",
    "from datetime import timedelta\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "#starting with other tools\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "#to save\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00622359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables that may come in handy\n",
    "#course threshold sets the % duration that will be considered (1 = 100%)\n",
    "duration_threshold = [0.1, 0.25, 0.33, 0.5, 1]\n",
    "\n",
    "#colors for vizualizations\n",
    "nova_ims_colors = ['#BFD72F', '#5C666C']\n",
    "\n",
    "#standard color for student aggregates\n",
    "student_color = '#474838'\n",
    "\n",
    "#standard color for course aggragates\n",
    "course_color = '#1B3D2F'\n",
    "\n",
    "#standard continuous colormap\n",
    "standard_cmap = 'viridis_r'\n",
    "\n",
    "#Function designed to deal with multiindex and flatten it\n",
    "def flattenHierarchicalCol(col,sep = '_'):\n",
    "    '''converts multiindex columns into single index columns while retaining the hierarchical components'''\n",
    "    if not type(col) is tuple:\n",
    "        return col\n",
    "    else:\n",
    "        new_col = ''\n",
    "        for leveli,level in enumerate(col):\n",
    "            if not level == '':\n",
    "                if not leveli == 0:\n",
    "                    new_col += sep\n",
    "                new_col += level\n",
    "        return new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1209428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading student log data \n",
    "student_logs = pd.read_csv('../Data/Modeling Stage/NovaIMS_cleaned_logs.csv',\n",
    "                           dtype = {\n",
    "                                   'cd_curso': float,\n",
    "                                   'userid': float,\n",
    "                                   'courseid': float,\n",
    "                                   },\n",
    "                                   parse_dates = ['time']).drop('Unnamed: 0', axis = 1) #logs\n",
    "\n",
    "#converting to object\n",
    "student_logs['userid'], student_logs['cd_curso'], student_logs['courseid'] = student_logs['userid'].astype(object), student_logs['cd_curso'].astype(object), student_logs['courseid'].astype(object)\n",
    "\n",
    "#other tables with support information\n",
    "support_table = pd.read_csv('../Data/Nova_IMS_support_table.csv',\n",
    "                             dtype = {\n",
    "                                 'cd_curso' : float,\n",
    "                                 'courseid' : float,\n",
    "                                 'userid' : float,\n",
    "                                 'assign_id': float,\n",
    "                             }, parse_dates = ['startdate', 'end_date']).drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "#converting to object\n",
    "support_table['userid'], support_table['cd_curso'], support_table['courseid'], support_table['assign_id'] = support_table['userid'].astype(object), support_table['cd_curso'].astype(object), support_table['courseid'].astype(object), support_table['assign_id'].astype(object)\n",
    "\n",
    "#save tables \n",
    "class_list = pd.read_csv('../Data/Modeling Stage/NovaIMS_class_duration.csv', \n",
    "                         dtype = {\n",
    "                                   'cd_curso': float,\n",
    "                                   'courseid': float,                                   \n",
    "                                   },\n",
    "                        parse_dates = ['Start Date','End Date', 'cuttoff_point']).rename(columns = {'cuttoff_point' : 'Week before start'})\n",
    "\n",
    "#converting to object\n",
    "class_list['cd_curso'], class_list['courseid'] = class_list['cd_curso'].astype(object), class_list['courseid'].astype(object)\n",
    "\n",
    "#targets tables \n",
    "targets_table = pd.read_csv('../Data/Modeling Stage/Nova_IMS_targets_table.csv',\n",
    "                           dtype = {\n",
    "                                   'cd_curso': float,\n",
    "                                   'userid': float,\n",
    "                                   'courseid': float,\n",
    "                                   },).drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "#converting to float\n",
    "targets_table['userid'], targets_table['courseid'] = targets_table['userid'].astype(object), targets_table['courseid'].astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6218f",
   "metadata": {},
   "source": [
    "We'll start with the general verification of the different datasets we've imported. \n",
    "\n",
    "**Starting with the targets table, which includes all valid student-course logs with Final-Grade.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5389476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get info\n",
    "targets_table.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0797724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_table.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1edc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530eb58f",
   "metadata": {},
   "source": [
    "Then, we repeat the same for the list of courses and their respective start and end dates. We know that the number of students attending each course is the number found in the logs. We will need to make further cuts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3910ba5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_list.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27ad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c42fa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e00340",
   "metadata": {},
   "source": [
    "We still note a significant presence of courses with small numbers of students. The first step we will take is the removal of all courses whose number of attending students is below 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = class_list[class_list['Users per course'] >= 50]\n",
    "\n",
    "#updating student logs\n",
    "student_logs = student_logs[student_logs['courseid'].isin(class_list['courseid']) & \n",
    "                            student_logs['cd_curso'].isin(class_list['cd_curso']) &\n",
    "                            student_logs['semestre'].isin(class_list['semestre'])].reset_index(drop = True)\n",
    "\n",
    "#additionally updating targets_table\n",
    "targets_table = targets_table[targets_table['courseid'].isin(class_list['courseid']) & \n",
    "                              targets_table['cd_curso'].isin(class_list['cd_curso']) &\n",
    "                              targets_table['semestre'].isin(class_list['semestre'])].reset_index(drop = True)\n",
    "\n",
    "#additionally updating support_table\n",
    "support_table = support_table[support_table['courseid'].isin(class_list['courseid']) & \n",
    "                              support_table['cd_curso'].isin(class_list['cd_curso']) &\n",
    "                              support_table['semestre'].isin(class_list['semestre'])].reset_index(drop = True)\n",
    "\n",
    "class_list.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb690c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc2336",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac374a",
   "metadata": {},
   "source": [
    "We'll follow up with taking a closer look logs we cleaned in the previous section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d0d86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_logs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a53ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_logs.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45520e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#then we plot an histogram with all courses, we are not interested in keeping courses with a number of students inferior to 10\n",
    "sns.set_theme(context='paper', style='whitegrid', font='Calibri', rc={\"figure.figsize\":(16, 10)}, font_scale=2)\n",
    "hist4 = sns.histplot(data=class_list, x='Users per course', kde=True, color= student_color, binwidth = 5,)\n",
    "\n",
    "fig = hist4.get_figure()\n",
    "fig.savefig('../Images/Nova_students_per_course_bin_5, filtered.png', transparent=True, dpi=300)\n",
    "\n",
    "#delete to remove from memory\n",
    "del fig, hist4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd3d387",
   "metadata": {},
   "source": [
    "\n",
    "Likewise, there is some attention to be found on courses with abnormally high numbers of attending students in a face-to-face context (over 200). We will pay closer attention to those courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb5550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create df only with high affluence courses\n",
    "most_affluent_courses = class_list[class_list['Users per course'] >= 100]\n",
    "\n",
    "#separate logs accordingly\n",
    "high_attendance_logs = student_logs[student_logs['courseid'].isin(most_affluent_courses['courseid']) & \n",
    "                            student_logs['cd_curso'].isin(most_affluent_courses['cd_curso']) &\n",
    "                            student_logs['semestre'].isin(most_affluent_courses['semestre'])].reset_index(drop = True)\n",
    "high_attendance_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2e0b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_attendance_logs.describe(include = 'all', datetime_is_numeric = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881df3a",
   "metadata": {},
   "source": [
    "We can plot the weekly interactions of these courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13e1a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then, when it comes to logs, we aggregate by week\n",
    "grouped_data = high_attendance_logs.groupby([pd.Grouper(key='time', freq='W'), 'cd_curso', 'semestre', 'courseid']).agg({\n",
    "                                                                             'action': 'count',\n",
    "                                                                             }).reset_index().sort_values('time')\n",
    "\n",
    "#change for better reading\n",
    "grouped_data['Date (week)'] = grouped_data['time'].astype(str)\n",
    "\n",
    "#creating pivot table to create heatmap\n",
    "grouped_data = grouped_data.pivot_table(index =['cd_curso', 'semestre', 'courseid'], \n",
    "                       columns = 'Date (week)',\n",
    "                        values = 'action', \n",
    "                       aggfunc =np.sum,\n",
    "                        fill_value=np.nan)\n",
    "\n",
    "#now, we will sort the courses according to the starting date\n",
    "grouped_data = grouped_data.reset_index().rename(columns = {'courseid': 'Course',\n",
    "                                                            'cd_curso': 'Program',\n",
    "                                                            'semestre': 'Semester',\n",
    "                                                           })\n",
    "\n",
    "grouped_data['Course'] = pd.to_numeric(grouped_data['Course']).astype(int)\n",
    "\n",
    "#finally we create the pivot_table that we will use to create our heatmap\n",
    "grouped_data = grouped_data.set_index(['Program', 'Semester', 'Course'], drop = True)\n",
    "grouped_data.T.describe(include = 'all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context='paper', style='whitegrid', font='Calibri', rc={\"figure.figsize\":(20, 12)}, font_scale=2)\n",
    "\n",
    "#here, we are plotting the nex\n",
    "heat4 = sns.heatmap(grouped_data, robust=True, norm=LogNorm(), xticklabels = 2, yticklabels= 1,\n",
    "            cmap = standard_cmap, cbar_kws={'label': 'Weekly interactions'})\n",
    "\n",
    "fig = heat4.get_figure()\n",
    "fig.savefig('../Images/Nova_IMS_highest_attendance_weekly_clicks_heat.png', transparent=True, dpi=300)\n",
    "\n",
    "#delete to remove from memory\n",
    "del fig, heat4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0d0177",
   "metadata": {},
   "source": [
    "After consideration, we find that the student interactions seem to be consistent with the course duration. \n",
    "\n",
    "**1. First, we filter by our current list of valid courses.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384a3ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representation of different targets depending \n",
    "g = sns.PairGrid(targets_table, diag_sharey=False, corner=True)\n",
    "g.map_diag(sns.histplot)\n",
    "g.map_lower(sns.scatterplot)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816df8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a larger overlook at the different courses\n",
    "targets_table.groupby(['cd_curso', 'semestre', 'courseid']).agg({\n",
    "                                    'userid' : 'count', \n",
    "                                    'exam_mark' : ['min', 'mean', 'max'],                                    \n",
    "                                    'final_mark' : ['min', 'mean', 'max'],\n",
    "                                    }).describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8740d8d",
   "metadata": {},
   "source": [
    "**We will finish by taking a look at our support table**. This table associates all students attending a specific course and the partial grades obtained by each student.\n",
    "\n",
    "In the Nova IMS these grades are not timestamped (i.e. we do not know to which assignment-quizz-event the partial grade refers to nor when the specific assignment refers to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd19e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get info\n",
    "support_table.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f85ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77969b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a926381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a larger overlook at the different courses\n",
    "support_table.groupby(['cd_curso', 'semestre', 'courseid']).agg({\n",
    "                                    'userid' : 'nunique',\n",
    "                                    'assign_id' : 'nunique', \n",
    "#                                     'mandatory_status' : 'mean',\n",
    "#                                     'delivered' : 'mean',                                    \n",
    "                                    'assignment_mark' : 'mean',\n",
    "                                    }).describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b1761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Representation of different targets depending \n",
    "g = sns.PairGrid(support_table, diag_sharey=False, corner=True)\n",
    "g.map_diag(sns.histplot)\n",
    "g.map_lower(sns.scatterplot)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d2168",
   "metadata": {},
   "source": [
    "**Going forward**.\n",
    "\n",
    "After this preliminary look, we will go forward with extracting features from the Moodle logs. \n",
    "\n",
    "In this notebook, we will consider a temporal representation that considers each student-course pair as a row and each column to represent a day in the course.\n",
    "\n",
    "An important distinction is that, when working in this manner, we do not have to perform multiple pre-processing steps. Instead we get, for each row, a sequence of the daily number of clicks.\n",
    "\n",
    "For the purposes of retaining the same sets of rows as the ones we got previously we will follow through with the preprocessing steps we had determined in the non-temporal representation. Again, this is exclusively performed to retain the same rows we had previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52101645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct objecttable\n",
    "other_objects =  ['tag_instance', 'badge', 'feedback_completed', 'feedback', 'course_modules_completion', 'feedback']\n",
    "\n",
    "#badges on target id\n",
    "badges_on_target = ['badge_listing', 'badge', 'recent_activity']\n",
    "\n",
    "#grades \n",
    "grading_objects = ['gradereport_overview', 'gradereport_user']\n",
    "\n",
    "#assignment from elements in the component column\n",
    "assign_objects = ['assignsubmission_onlinetext', 'assignsubmission_comments', 'mod_assign', 'assignsubmission_file']\n",
    "\n",
    "#workshop\n",
    "workshops = ['workshop_submissions', 'workshop']\n",
    "\n",
    "#course\n",
    "courses_on_target = ['course', 'course_resources_list', 'course_user_report']\n",
    "\n",
    "#corrections on the objecttable column\n",
    "student_logs['objecttable'] = np.where(student_logs['objecttable'].isin(other_objects),\n",
    "                                      'other',\n",
    "                                       np.where(student_logs['target'].isin(badges_on_target),\n",
    "                                      'other',\n",
    "                                       np.where(student_logs['target'].isin(courses_on_target),\n",
    "                                      'course',\n",
    "                                      np.where(student_logs['component'] == 'mod_forum',\n",
    "                                      'forum',\n",
    "                                      np.where(student_logs['component'].isin(grading_objects),\n",
    "                                      'grade_grades',\n",
    "                                       np.where(student_logs['component'].isin(assign_objects),\n",
    "                                      'assignments',\n",
    "                                       np.where(student_logs['objecttable'].isin(workshops),\n",
    "                                      'workshop', \n",
    "                                       np.where(student_logs['objecttable'] == 'book_chapters',\n",
    "                                      'book',\n",
    "                                       np.where(student_logs['component'] == 'mod_coice',\n",
    "                                      'choice',\n",
    "                                       np.where(student_logs['component'] == 'mod_choicegroup',\n",
    "                                      'groups',\n",
    "                                      np.where((student_logs['component'] == 'mod_quiz') & (student_logs['objecttable'].isna()),'quiz',\n",
    "                                      student_logs['objecttable'])))))))))))\n",
    "\n",
    "del other_objects, badges_on_target, grading_objects, assign_objects, workshops, courses_on_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc7b36",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #uncomment to verify pairings\n",
    "# with pd.option_context('display.max_rows', None,):\n",
    "#      display(student_logs['objecttable'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dd0bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #uncomment to verify pairings\n",
    "# with pd.option_context('display.max_rows', None,):\n",
    "#      display(student_logs['component'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5dbd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #uncomment to verify pairings\n",
    "# with pd.option_context('display.max_rows', None,):\n",
    "#      display(student_logs['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6339f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #uncomment to verify pairings\n",
    "# with pd.option_context('display.max_rows', None,):\n",
    "#      display(student_logs['action'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beecaa6f",
   "metadata": {},
   "source": [
    "Likewise, we will need to take a look at the different actions in order to understand how common these may be. \n",
    "\n",
    "Again, we will look at different actions and see how we can group them together in a way that, at least, makes intuitive sense. There is use in keeping the distinction between different types of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#updates and edits related to making editions on presented information:\n",
    "\n",
    "#additions \n",
    "addition = ['created', 'added']\n",
    "\n",
    "#deletion\n",
    "deletion = ['deleted', 'removed']\n",
    "\n",
    "#other actions\n",
    "other_actions = ['awarded', 'printed', 'abandoned', 'searched']\n",
    "\n",
    "#submissions\n",
    "submission = ['submitted', 'submission', 'submissions']\n",
    "\n",
    "#converts discussion points to forum or, alternatively,groups other elements to other category\n",
    "student_logs['action'] = np.where(student_logs['action'].isin(addition), 'added', #addition list\n",
    "                                  np.where(student_logs['action'].isin(deletion), 'delete', #deletion list\n",
    "                                  np.where(student_logs['action'].isin(other_actions), 'other actions', #other actions\n",
    "                                  np.where(student_logs['action'].isin(submission), 'submission', #submissions to submission\n",
    "                                  student_logs['action']\n",
    "                                 ))))\n",
    "\n",
    "#we finish by ending these lists we've created\n",
    "del addition, deletion, other_actions, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad0e00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # #uncomment to verify pairings\n",
    "# with pd.option_context('display.max_rows', None,):\n",
    "#      display(student_logs.groupby(['objecttable', 'target', 'action']).size().to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f6f2c5",
   "metadata": {},
   "source": [
    "We do not know, yet, whether we can make an effective association between the partial grades and the logs. We can explore the submissions made in each discipline and calculate the likely dates of submission.\n",
    "\n",
    "For that, for every course, we will look at submissions performed by the different students.\n",
    "\n",
    "We will count the number of assignments assigned to each discipline - the number of graded assignments givenaverage number of grades attributed in each curricular unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3c1cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#then we get all unique entries and assign them an index number\n",
    "courses = class_list[['cd_curso', 'semestre', 'courseid']].drop_duplicates().reset_index(drop = True).reset_index()\n",
    "\n",
    "#then, we create a dict using the combination of index, courseid and status as keys\n",
    "courses = courses.set_index(['cd_curso', 'semestre', 'courseid']).to_dict()['index']\n",
    "\n",
    "#set index of df to match same index\n",
    "class_list.set_index(['cd_curso', 'semestre', 'courseid'], drop = True, inplace = True)\n",
    "\n",
    "#set index of df to match same index\n",
    "student_logs.set_index(['cd_curso', 'semestre', 'courseid'], drop = True, inplace = True)\n",
    "\n",
    "#set index of support_df to match same index\n",
    "support_table.set_index(['cd_curso', 'semestre', 'courseid'], drop = True, inplace = True)\n",
    "\n",
    "#set index of targets_table to match same index\n",
    "targets_table.set_index(['cd_curso', 'semestre', 'courseid'], drop = True, inplace = True)\n",
    "\n",
    "#use index as key for dict\n",
    "student_logs['course_encoding'] = student_logs.index.map(courses).astype(object)\n",
    "class_list['course_encoding'] = class_list.index.map(courses).astype(object)\n",
    "support_table['course_encoding'] = support_table.index.map(courses).astype(object)\n",
    "targets_table['course_encoding'] = targets_table.index.map(courses).astype(object)\n",
    "\n",
    "#resetting index\n",
    "student_logs = student_logs.dropna(subset = ['course_encoding']).reset_index()\n",
    "class_list.reset_index(inplace = True)\n",
    "support_table = support_table.dropna(subset = ['course_encoding']).reset_index()\n",
    "targets_table = targets_table.dropna(subset = ['course_encoding']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972f4afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, get count of assignments as defined in the support_table \n",
    "assignments_per_course = support_table.groupby('course_encoding').agg({'assign_id' : 'nunique'})\n",
    "assignments_per_course = assignments_per_course.to_dict()['assign_id']\n",
    "\n",
    "#next, we filter by only keepting submissions, avoid the first 2 weeks of submissions\n",
    "submission_logs = student_logs[(student_logs['action'] == 'submission') & (student_logs['time'] >= '2020-09-18')].sort_values(by = 'time').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16837a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second filtering condition - keep add, assessable and attempts -> edits and deletes are not relevant to determine number of submissions\n",
    "submission_logs = submission_logs[submission_logs['target'].isin(target_to_keep := ['add',\n",
    "                                                                                    'assessable',\n",
    "                                                                                    'attempt',\n",
    "                                                                                   ])]\n",
    "#add number of assignments of submission_logs\n",
    "submission_logs['nbr_assignments'] = submission_logs['course_encoding'].map(assignments_per_course)\n",
    "\n",
    "#we give each submission made by each student in the context of certain curricular units\n",
    "submission_logs['course_student_submission_number'] = submission_logs.groupby(['course_encoding', 'userid']).cumcount() + 1\n",
    "submission_logs = submission_logs.filter(['course_encoding', 'userid', 'time', 'course_student_submission_number', 'nbr_assignments'])\n",
    "\n",
    "#then, we get the number of submissions made by each student and the average date for each submission\n",
    "submission_logs = submission_logs.groupby(['course_encoding', 'course_student_submission_number']).agg(\n",
    "                                                                                {\n",
    "                                                                                    'userid': 'count',\n",
    "                                                                                    'time': ['mean', 'median'],\n",
    "                                                                                    'nbr_assignments': 'mean' #equal to same \n",
    "                                                                                })\n",
    "\n",
    "#applies the function that removes multiindex\n",
    "submission_logs.columns = submission_logs.columns.map(flattenHierarchicalCol)\n",
    "submission_logs.reset_index(inplace = True)\n",
    "\n",
    "#then, we only keep the number of submissions that is in line with the number on the support_table\n",
    "submission_logs = submission_logs[submission_logs['course_student_submission_number'] <= submission_logs['nbr_assignments_mean']].reset_index(drop = True)\n",
    "submission_logs['course_student_submission_number'] = submission_logs['course_student_submission_number'].astype(object)\n",
    "submission_logs['course_encoding'] = submission_logs['course_encoding'].astype(object)\n",
    "\n",
    "#then, in order to make the proper merge, we'll need to make the proper adjustments - namely label the submission date of each discipline\n",
    "support_table['course_student_submission_number'] = pd.to_numeric(support_table['statusAvaliacao'].str.extract('(\\d+)', expand=False)).astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fffd97",
   "metadata": {},
   "source": [
    "In this next step we, we will timestamp each assignment recorded on the support table. It is likely that, in group assigments, only one student submits for all coleagues. Therefore, it is not possible to make a 1 to 1 between submission and grade.\n",
    "\n",
    "In general -> Partial grade 1 refers to a student's first submission, partial grade 2 to the second submission, etc...\n",
    "We will assume deadline date for each partial grade to be the median delivery date of the classe's ith submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffaa60c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#filtering columns before merge\n",
    "submission_logs = submission_logs.filter(['course_encoding', 'course_student_submission_number',\n",
    "                                         'time_median'])\n",
    "\n",
    "#making a rightward merge with the support_table on course_encoding and course_student_submission_number\n",
    "support_table = pd.merge(support_table, submission_logs, on = ['course_encoding', 'course_student_submission_number'], how = 'left')\n",
    "\n",
    "#at this stage, we can now start dropping columns that are ultimately unnecessary and reclaibrating the assign_id column\n",
    "support_table['assign_id'] = support_table.groupby(['course_encoding', 'statusAvaliacao']).ngroup()\n",
    "support_table['sup_time'] = pd.to_datetime(support_table['time_median'].dt.date)\n",
    "support_table.drop(['statusAvaliacao', 'course_student_submission_number', 'time_median'], axis = 1, inplace = True)\n",
    "\n",
    "del submission_logs, assignments_per_course\n",
    "\n",
    "support_table.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40729acb",
   "metadata": {},
   "source": [
    "We have addressed the most obvious possible aggregations. Now, we will go forward with our intended feature extraction and selection.\n",
    "\n",
    "We can start by removing all of the unnecessary columns that we will not be using going forward and, then, create 5 distinct dicts of dataframes. Each dict refers to a certain course duration threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering for \n",
    "student_logs = student_logs.filter(['cd_curso', 'semestre', 'courseid', 'objecttable', 'action', 'target', 'component',\n",
    "                                   'userid', 'time', 'course_encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc5abe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#additionally, we will look at our estimated course duration\n",
    "for i in tqdm(duration_threshold):\n",
    "    #create, for each desired threshold, the appropriate cutoff date \n",
    "    class_list[f'Date_threshold_{int(i*100)}'] = pd.to_datetime((class_list['Start Date'] + pd.to_timedelta(class_list['Course duration days'] * i, unit = 'Days')).dt.date)\n",
    "    \n",
    "        #setting up duration threshold to be on friday -> reason being that it will be easier to \n",
    "    class_list[f'Date_threshold_{int(i*100)}'] = class_list[f'Date_threshold_{int(i*100)}'].where( class_list[f'Date_threshold_{int(i*100)}'] == (( class_list[f'Date_threshold_{int(i*100)}'] + Week(weekday=4)) - Week()), class_list[f'Date_threshold_{int(i*100)}'] + Week(weekday=4))\n",
    "\n",
    "#then, we will create a dictionary of dictionaries, each main dictionary storing and a version of the logs\n",
    "logs_dict = {}\n",
    "\n",
    "for i in tqdm(duration_threshold):\n",
    "    #create, for each desired threshold, a different dictionary of dataframes wherein we will perform the different operations\n",
    "    print(f'Date_threshold_{int(i*100)}\\n' +\n",
    "          f'Logs')\n",
    "    logs_dict[f'Date_threshold_{int(i*100)}'] = {course: student_logs.loc[student_logs['course_encoding'] == course].reset_index(drop = True) for course in tqdm(student_logs['course_encoding'].unique())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d90d8b1",
   "metadata": {},
   "source": [
    "Now, we have a nested dictionary with different dataframes inside it. We will use this data structure to perform the most of the operations we are interested in.\n",
    "\n",
    "**First, we will add, to each dataframe, a column with the corresponding threshold date**\n",
    "\n",
    "After this cleaning procedure, we will, for each course get the daily clicks and put them on a pivot_table. We will finish the procedure by exporting updated versions of the class list as these will also need to be used going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d0504b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for each intended course duration threshold\n",
    "for i in tqdm(logs_dict):\n",
    "    #start with creating a dictionary of course and intended cuttoff date\n",
    "    cut = class_list.set_index('course_encoding').to_dict()[i] \n",
    "    \n",
    "    #for each dataframe\n",
    "    for j in tqdm(logs_dict[i]):\n",
    "        #where the course is the same as in the class_list, get the corresponding value of the appropriate column,\n",
    "        logs_dict[i][j]['Date Threshold'] = logs_dict[i][j]['course_encoding'].map(cut)\n",
    "        logs_dict[i][j] = logs_dict[i][j][logs_dict[i][j]['time'] <= logs_dict[i][j]['Date Threshold']].reset_index(drop = True).drop('Date Threshold', axis = 1)\n",
    "\n",
    "        #Aggregate by day\n",
    "        logs_dict[i][j] = logs_dict[i][j].groupby(['course_encoding', 'cd_curso', 'semestre', 'courseid', pd.Grouper(key='time', freq='D'), 'userid']).agg({\n",
    "                                                                                                                'action': 'count', \n",
    "                                                                                                                }).reset_index().sort_values('time')\n",
    "        \n",
    "        #then, we create a pivot_table\n",
    "        logs_dict[i][j] = pd.pivot_table(logs_dict[i][j], index=['course_encoding', 'cd_curso', 'semestre', 'courseid', 'userid'], columns = 'time', values = 'action',\n",
    "                    aggfunc='sum').fillna(0)\n",
    "        \n",
    "        #and rename columns to fit with the number of days\n",
    "        logs_dict[i][j] = logs_dict[i][j].rename(columns={x:y for x,y in zip(logs_dict[i][j].columns,range(1,len(logs_dict[i][j].columns) + 1))})\n",
    "        logs_dict[i][j].columns.name = None\n",
    "        \n",
    "        #joining final grade for target\n",
    "        logs_dict[i][j] = logs_dict[i][j].merge(targets_table.filter(['course_encoding', 'userid', 'exam_mark', 'final_mark']), on = ['course_encoding', 'userid'], how = 'right')\n",
    "    \n",
    "    #after the end of the loops:\n",
    "    logs_dict[i] = pd.concat(logs_dict[i], ignore_index=True)\n",
    "    logs_dict[i] = logs_dict[i].sort_values(by = ['course_encoding', 'userid', 'final_mark']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b51df",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_dict['Date_threshold_100'][71]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1abf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create backup of logs dict, we will need it for later\n",
    "backup = deepcopy(logs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create backup of logs dict, we will need it for later\n",
    "logs_dict = deepcopy(backup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0a0aae",
   "metadata": {},
   "source": [
    "In order to account for situations where registered students only access Moodle later in the course, we will make an additional, but necessary adaptation. \n",
    "\n",
    "We will start by looking at the complete set of valid students/courses in our 100% dataset. From these, we get the indexes of the rows that are valid (i.e. have a valid click count at 100% duration), get the indexes and retain only these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ee0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we gather the index number of valid rows in the 100% df\n",
    "rows_to_keep = logs_dict['Date_threshold_100'][~logs_dict['Date_threshold_100'][1].isna()].index\n",
    "\n",
    "# #then slice accordingly\n",
    "for i in tqdm(logs_dict):\n",
    "    logs_dict[i] = logs_dict[i].iloc[rows_to_keep, :].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c7924",
   "metadata": {},
   "source": [
    "#### Almost Done.\n",
    "\n",
    "We will finish this step momentarily. Before we do, we need to save all dfs in an easily accessible Excel File."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512405bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('../Data/Modeling Stage/Nova_IMS_Temporal_Datasets.xlsx', engine='xlsxwriter')\n",
    "\n",
    "#now loop thru and put each on a specific sheet\n",
    "for sheet, frame in  logs_dict.items(): \n",
    "    frame.to_excel(writer, sheet_name = sheet)\n",
    "\n",
    "#critical last step\n",
    "writer.save()\n",
    "\n",
    "#also saving additional info on class list\n",
    "class_list.to_csv('../Data/Modeling Stage/Nova_IMS_updated_classlist.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
