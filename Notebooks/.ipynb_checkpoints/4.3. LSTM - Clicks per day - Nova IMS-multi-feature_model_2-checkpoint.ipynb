{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84eda90e",
   "metadata": {},
   "source": [
    "### Thesis notebook 4.3. - NOVA IMS\n",
    "\n",
    "#### LSTM - Temporal data representation\n",
    "\n",
    "In this notebook, we will finally start our application of temporal representation using LSTMs.\n",
    "The argument for the usage of Deep Learning stems from the fact that sequences themselves encode information that can be extracted using Recurrent Neural Networks and, more specifically, Long Short Term Memory Units.\n",
    "\n",
    "#### First Step: Setup a PyTorch environment that enables the use of GPU for training. \n",
    "\n",
    "The following cell wll confirm that the GPU will be the default device to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f27844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "cuda.init()\n",
    "## Get Id of default device\n",
    "torch.cuda.current_device()\n",
    "# 0\n",
    "cuda.Device(0).name() # '0' is the id of your GPU\n",
    "\n",
    "#set all tensors to gpu\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95429e",
   "metadata": {},
   "source": [
    "#### Second Step: Import the relevant packages and declare global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c2d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules/libraries\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "\n",
    "#tqdm to monitor progress\n",
    "from tqdm.notebook import tqdm, trange\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "#time related features\n",
    "from datetime import timedelta\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "#vizualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#imblearn, scalers, kfold and metrics \n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer,PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, StratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, recall_score, classification_report, average_precision_score, precision_recall_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#import torch related\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "#and optimizer of learning rate\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "#import pytorch modules\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c3f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables that may come in handy\n",
    "\n",
    "\n",
    "#colors for vizualizations\n",
    "nova_ims_colors = ['#BFD72F', '#5C666C']\n",
    "\n",
    "#standard color for student aggregates\n",
    "student_color = '#474838'\n",
    "\n",
    "#standard color for course aggragates\n",
    "course_color = '#1B3D2F'\n",
    "\n",
    "#standard continuous colormap\n",
    "standard_cmap = 'viridis_r'\n",
    "\n",
    "#Function designed to deal with multiindex and flatten it\n",
    "def flattenHierarchicalCol(col,sep = '_'):\n",
    "    '''converts multiindex columns into single index columns while retaining the hierarchical components'''\n",
    "    if not type(col) is tuple:\n",
    "        return col\n",
    "    else:\n",
    "        new_col = ''\n",
    "        for leveli,level in enumerate(col):\n",
    "            if not level == '':\n",
    "                if not leveli == 0:\n",
    "                    new_col += sep\n",
    "                new_col += level\n",
    "        return new_col\n",
    "    \n",
    "#number of replicas - number of repeats of stratified k fold - in this case 10\n",
    "replicas = 30\n",
    "\n",
    "#names to display on result figures\n",
    "date_names = {\n",
    "             'Date_threshold_10': '10% of Course Duration',   \n",
    "             'Date_threshold_25': '25% of Course Duration', \n",
    "             'Date_threshold_33': '33% of Course Duration', \n",
    "             'Date_threshold_50': '50% of Course Duration', \n",
    "             'Date_threshold_100':'100% of Course Duration', \n",
    "            }\n",
    "\n",
    "target_names = {\n",
    "                'exam_fail' : 'At risk - Exam Grade',\n",
    "                'final_fail' : 'At risk - Final Grade', \n",
    "                'exam_gifted' : 'High performer - Exam Grade', \n",
    "                'final_gifted': 'High performer - Final Grade'\n",
    "                }\n",
    "\n",
    "#targets\n",
    "targets = ['exam_fail', 'exam_gifted']\n",
    "\n",
    "#set the indexes to use for later\n",
    "index = [\"course_encoding\", \"cd_curso\", \"semestre\", \"courseid\", \"userid\", 'exam_gifted', 'exam_fail']\n",
    "\n",
    "#categories of objecctables\n",
    "objects = [\"course\", \"resource\", \"forum\", \"url\", \"folder\", \"quiz\", \"grade_grades\", \n",
    "           \"assignments\", \"groups\", \"user\", \"turnitintooltwo\", \"page\", \"choice\", \"other\"]          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c5ddb6",
   "metadata": {},
   "source": [
    "#### Step 3: Import data and take a preliminary look at it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a23ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports dataframes\n",
    "course_programs = pd.read_excel(\"../Data/Modeling Stage/Nova_IMS_Temporal_Datasets_daily_clicks.xlsx\", \n",
    "                                dtype = {\n",
    "                                    'course_encoding' : int,\n",
    "                                    'userid' : int},\n",
    "                               sheet_name = 'Date_threshold_100')\n",
    "\n",
    "#save tables \n",
    "student_list = pd.read_csv('../Data/Modeling Stage/Nova_IMS_Filtered_targets.csv', \n",
    "                         dtype = {\n",
    "                                   'course_encoding': int,\n",
    "                                   'userid' : int,\n",
    "                                   })\n",
    "\n",
    "#drop unnamed 0 column\n",
    "#merge with the targets we calculated on the other \n",
    "course_programs = course_programs.merge(student_list, on = ['course_encoding', 'userid'], how = 'inner')\n",
    "course_programs.drop(['Unnamed: 0', 'exam_mark', 'final_mark'], axis = 1, inplace = True)\n",
    "    \n",
    "#convert results to object and need to convert column names to string\n",
    "course_programs['course_encoding'], course_programs['userid'] = course_programs['course_encoding'].astype(object), course_programs['userid'].astype(object)\n",
    "course_programs.columns = course_programs.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_programs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a751ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_programs.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3291817",
   "metadata": {},
   "source": [
    "In our first attempt, we will use the absolute number of clicks made by each student - scaled using standard scaler. \n",
    "Therefore, we can start by immediately placing our course encoding/userid pairings into the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be722ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train, test, scaler):\n",
    "    \n",
    "    if scaler == 'MinMax':\n",
    "        pt = MinMaxScaler()\n",
    "    elif scaler == 'Standard':\n",
    "        pt = StandardScaler()\n",
    "    elif scaler == 'Robust':\n",
    "        pt = RobustScaler()\n",
    "    elif scaler == 'Quantile':\n",
    "        pt = QuantileTransformer()\n",
    "    else:\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "    \n",
    "    data_train = pt.fit_transform(train)\n",
    "    data_test = pt.transform(test)\n",
    "    # convert the array back to a dataframe\n",
    "    normalized_train = pd.DataFrame(data_train,columns=train.columns)\n",
    "    normalized_test = pd.DataFrame(data_test,columns=test.columns)\n",
    "        \n",
    "    return normalized_train, normalized_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85843427",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_programs.objecttable.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92abd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = course_programs.copy()\n",
    "\n",
    "#The first 6 columns are index - column 141 is fully empty\n",
    "columns = test.drop(targets, axis = 1).columns[6:146]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99ddf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create first pivot\n",
    "placeholder_pivot = pd.pivot_table(test, index = index, values = columns, columns = \"objecttable\",\n",
    "                  aggfunc = 'first')\n",
    "\n",
    "\n",
    "#applies the function that removes multiindex\n",
    "placeholder_pivot.columns = placeholder_pivot.columns.map(flattenHierarchicalCol)\n",
    "\n",
    "#also saving index for reindexing of the remaining stuff\n",
    "save_index = placeholder_pivot.index.copy()\n",
    "\n",
    "#we will need to create the multidimensional tensors\n",
    "placeholder_dict = {}\n",
    "\n",
    "#create dataset for targets\n",
    "df_targets = placeholder_pivot.reset_index().copy()[index]\n",
    "df_targets.set_index([\"course_encoding\", \"cd_curso\", \"semestre\", \"courseid\", \"userid\"], inplace = True)\n",
    "\n",
    "#initialize empty 3d array\n",
    "nd_array_100 = np.zeros((\n",
    "                               len(objects), #nbr of dimensions\n",
    "                               len(placeholder_pivot), #nbr of rows\n",
    "                               len(columns), #nbr of columns \n",
    "                              ))\n",
    "\n",
    "#likely inefficient, but should do the trick\n",
    "counter = 0\n",
    "\n",
    "#create multiple dataframes based on regex - this will create ndarray for the 100 duration\n",
    "for i in objects:\n",
    "    #create the objects\n",
    "    placeholder_dict[f'{i}'] = placeholder_pivot.filter(regex=f'_{i}')\n",
    "    \n",
    "    #remove text, convert column name back to numbers and sort numbers to ensure sequence\n",
    "    placeholder_dict[f'{i}'].columns = placeholder_dict[f'{i}'].columns.str.replace(r\"\\D+\", \"\", regex=True) \n",
    "    placeholder_dict[f'{i}'].columns = placeholder_dict[f'{i}'].columns.astype(int)\n",
    "    placeholder_dict[f'{i}'] = placeholder_dict[f'{i}'][sorted(placeholder_dict[f'{i}'].columns)].fillna(0)\n",
    "    \n",
    "    #converting df to nd array\n",
    "    nd_array_100[counter] = placeholder_dict[f'{i}'].values\n",
    "    counter += 1\n",
    "\n",
    "    #reshape to samples, rows, columns\n",
    "\n",
    "#switching to rows, columns, features\n",
    "nd_array_100 = nd_array_100.transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e3c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nd_array_100.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4d5475",
   "metadata": {},
   "source": [
    "#### Implementing Cross-Validation with Deep Learning Model\n",
    "\n",
    "**1. Create the Deep Learning Model**\n",
    "\n",
    "In this instance, we will follow-up with on the approach used in Chen & Cui - CrossEntropyLoss with applied over a softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a16bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Uni(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length, dropout):\n",
    "        super(LSTM_Uni, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first = True) #lstm\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "    \n",
    "        self.fc = nn.Linear(self.hidden_size, num_classes) #fully connected last layer\n",
    "\n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        \n",
    "        #Xavier_init for both H_0 and C_0\n",
    "        torch.nn.init.xavier_normal_(h_0)\n",
    "        torch.nn.init.xavier_normal_(c_0)\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        lstm_out, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        last_output = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        \n",
    "        #we are interested in only keeping the last output\n",
    "        drop_out = self.dropout(last_output)\n",
    "        pre_bce = self.fc(drop_out) #Final Output - dense\n",
    "        return pre_bce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c356bd",
   "metadata": {},
   "source": [
    "**2. Define the train and validation Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b29a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,dataloader,loss_fn,optimizer):\n",
    "    \n",
    "    train_loss,train_correct=0.0,0 \n",
    "    model.train()\n",
    "    for X, labels in dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = loss_fn(output,labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * X.size(0)\n",
    "        scores = F.sigmoid(output)\n",
    "        predictions = torch.round(scores)\n",
    "        #calculate % correct\n",
    "        train_correct += (predictions == labels.unsqueeze(1)).sum().item()\n",
    "        \n",
    "    return train_loss,train_correct\n",
    "  \n",
    "def valid_epoch(model,dataloader,loss_fn):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    targets = []\n",
    "    y_pred = []\n",
    "    probability_1 = []\n",
    "    \n",
    "    model.eval()\n",
    "    for X, labels in dataloader:\n",
    "\n",
    "        output = model(X)\n",
    "        loss=loss_fn(output,labels.unsqueeze(1))\n",
    "        valid_loss+=loss.item()*X.size(0)\n",
    "        scores = F.sigmoid(output)\n",
    "        predictions = torch.round(scores)\n",
    "        val_correct+=(predictions == labels.unsqueeze(1)).sum().item()\n",
    "        targets.append(labels.unsqueeze(1))\n",
    "        y_pred.append(predictions)\n",
    "        probability_1.append(scores)\n",
    "        \n",
    "    #concat all results\n",
    "    targets = torch.cat(targets).data.cpu().numpy()\n",
    "    y_pred = torch.cat(y_pred).data.cpu().numpy()\n",
    "    probability_1 = torch.cat(probability_1).data.cpu().numpy()\n",
    "    \n",
    "    #calculate precision, recall and AUC score\n",
    "    \n",
    "    precision = precision_score(targets, y_pred)\n",
    "    recall = recall_score(targets, y_pred)\n",
    "    f1 = f1_score(targets, y_pred)\n",
    "    auroc = roc_auc_score(targets, probability_1)\n",
    "    \n",
    "    #return all\n",
    "    return valid_loss,val_correct, precision, recall, auroc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4543fb3",
   "metadata": {},
   "source": [
    "**3. Define main hyperparameters of the model, including splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7f21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly select from params dict\n",
    "params = {\n",
    "    'learning_rate': [0.001, 0.01, 0.005, 0.0005],\n",
    "    'num_epochs' : [30, 40, 50, 60, 70, 80, 90, 100],\n",
    "    'hidden_size': [32, 64, 128, 256],\n",
    "    'batch_size': [32, 64, 128, 256, 512],\n",
    "    'num_layers': [1, 2, 3],\n",
    "    'dropout': [0.2, 0.5, 0.7]\n",
    "    }\n",
    "\n",
    "from random import choice\n",
    "\n",
    "combination = [choice(params['learning_rate']), #choose lr\n",
    "               choice(params['num_epochs']), #max epochs\n",
    "               choice(params['hidden_size']), #hidden layer size\n",
    "               choice(params['batch_size']), #hidden batch size\n",
    "               choice(params['num_layers']), #hidden batch size\n",
    "               choice(params['dropout'])], # select random_combination of dropout\n",
    "\n",
    "#Parameters chosen are\n",
    "combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbbef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial configuation - the default\n",
    "learning_rate = combination[0][0]\n",
    "num_epochs = combination[0][1]\n",
    "input_size = 14 #number of features\n",
    "hidden_size = combination[0][2]\n",
    "num_layers = combination[0][3]\n",
    "batch_size = combination[0][4]\n",
    "dropout = combination[0][5]\n",
    "\n",
    "\n",
    "#Shape of Output as required for Sigmoid Classifier\n",
    "num_classes = 1 #output shape\n",
    "\n",
    "k=10\n",
    "splits= StratifiedKFold(n_splits=k, random_state=15, shuffle = True) #kfold of 10 with 30 replicas\n",
    "criterion = nn.BCEWithLogitsLoss()    # cross-entropy for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89de5180",
   "metadata": {},
   "source": [
    "### Test on data - starting with the first 25 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0588de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create main dict results for 25 days\n",
    "target_df_dict_25 = {}\n",
    "\n",
    "for k in tqdm(targets):\n",
    "    print(k)\n",
    "    \n",
    "    #create main dict results\n",
    "    \n",
    "    target_df_dict_25[f'{(k)}'] = {}\n",
    "    y = df_targets[k].values\n",
    "\n",
    "    #create a list containing one value per row\n",
    "    all_indices = list(range(len(df_targets)))\n",
    "    \n",
    "    #using train test split to later apply the rule accordingly\n",
    "    train_ind, test_ind = train_test_split(all_indices, test_size=0.2, \n",
    "                                           random_state = 5, stratify = y)\n",
    "    \n",
    "    #applied train_test_split rules accordingly\n",
    "    X_train_val = nd_array_100[train_ind,:26,:]\n",
    "    y_train_val = y[train_ind]\n",
    "    \n",
    "    X_test = nd_array_100[test_ind, :26, :]\n",
    "    y_test = y[test_ind]    \n",
    "        \n",
    "    for repeat in range(replicas):\n",
    "        print('Replica {}'.format(repeat+1))\n",
    "        #reset \"best accuracy for treshold i and target k\"\n",
    "        best_f1_score = 0\n",
    "        foldperf={}\n",
    "        \n",
    "        #make train_val split\n",
    "        for fold, (train_idx,val_idx) in tqdm(enumerate(splits.split(X_train_val, y_train_val))):\n",
    "            print('Split {}'.format(fold + 1))\n",
    "            \n",
    "            #make split between train and Val\n",
    "            X_train, y_train = X_train_val[train_idx], y_train_val[train_idx]\n",
    "            X_val, y_val = X_train_val[val_idx], y_train_val[val_idx]\n",
    "            \n",
    "            #scaling requires one scaler per channel (feature)\n",
    "            scalers = {}\n",
    "            for feature in range(X_train.shape[2]):\n",
    "                           \n",
    "                scalers[feature] = RobustScaler()\n",
    "                X_train[:, :, feature] = scalers[feature].fit_transform(X_train[:, :, feature]) \n",
    "\n",
    "            for col in range(X_val.shape[2]):\n",
    "                X_val[:, :, feature] = scalers[feature].transform(X_val[:, :, feature]) \n",
    "            \n",
    "            #need to oversample - will use smote\n",
    "            #will also require oneoversampler per channel (feature)\n",
    "            samplers = {}\n",
    "            \n",
    "            #create new nd_array with the correct size - 2 * majority class \n",
    "            X_train_res = np.zeros(shape = (2* (int(sc.stats.mode(y_train)[1])), X_train.shape[1], X_train.shape[2]))\n",
    "            for feature in range(X_train.shape[2]):\n",
    "                           \n",
    "                samplers[feature] = SMOTE()\n",
    "                X_train_res[:, :, feature], y_train_res = samplers[feature].fit_resample(X_train[:, :, feature], y_train) \n",
    "            \n",
    "            #second, convert everything to pytorch tensor - we will convert to tensor dataset and \n",
    "            X_train_tensors = torch.from_numpy(X_train_res)\n",
    "            X_val_tensors = torch.from_numpy(X_val)\n",
    "            \n",
    "            #convert X tensors to format FloatTensor\n",
    "            X_train_tensors = X_train_tensors.type(torch.cuda.FloatTensor)\n",
    "            X_val_tensors = X_val_tensors.type(torch.cuda.FloatTensor)\n",
    "            \n",
    "            #create y_tensor\n",
    "            y_train_tensors = torch.from_numpy(y_train_res)\n",
    "            y_val_tensors = torch.from_numpy(y_val)\n",
    "            \n",
    "            #convert y tensors to format longtensor\n",
    "            y_train_tensors = y_train_tensors.type(torch.cuda.FloatTensor)\n",
    "            y_val_tensors = y_val_tensors.type(torch.cuda.FloatTensor)\n",
    "            \n",
    "            #create Tensor Datasets and dataloaders for both Train and Val\n",
    "            train_dataset = TensorDataset(X_train_tensors, y_train_tensors)\n",
    "            val_dataset = TensorDataset(X_val_tensors, y_val_tensors)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "            #creates new model for each \n",
    "            model = LSTM_Uni(num_classes, input_size, hidden_size, num_layers, X_train_tensors.shape[1], dropout).to('cuda') #our lstm class\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "            scheduler = ReduceLROnPlateau(optimizer, \n",
    "                                 'min', \n",
    "                                 patience = 15,\n",
    "                                 cooldown = 20,\n",
    "                                threshold=0.00001,\n",
    "                                factor = 0.33,\n",
    "                                verbose = True)\n",
    "    \n",
    "            history = {'train_loss': [], 'val_loss': [],'train_acc':[],'val_acc':[], 'precision': [],\n",
    "                      'recall' : [], 'auroc': [], 'f1_score' : []}\n",
    "\n",
    "            for epoch in tqdm(range(num_epochs)):\n",
    "                train_loss, train_correct=train_epoch(model,train_loader,criterion,optimizer)\n",
    "                val_loss, val_correct, precision, recall, auroc, f1 = valid_epoch(model,val_loader,criterion)\n",
    "\n",
    "                train_loss = train_loss / len(train_loader.sampler)\n",
    "                train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "                val_loss = val_loss / len(val_loader.sampler)\n",
    "                val_acc = val_correct / len(val_loader.sampler) * 100\n",
    "        \n",
    "        \n",
    "                if (epoch+1) % 10 == 0: \n",
    "                 print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Validation Loss:{:.3f} AVG Training Acc {:.2f} % AVG Validation Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                             num_epochs,\n",
    "                                                                                                             train_loss,\n",
    "                                                                                                             val_loss,\n",
    "                                                                                                             train_acc,\n",
    "                                                                                                             val_acc))\n",
    "                history['train_loss'].append(train_loss)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['train_acc'].append(train_acc)\n",
    "                history['val_acc'].append(val_acc)\n",
    "                history['precision'].append(precision)\n",
    "                history['recall'].append(recall)\n",
    "                history['auroc'].append(auroc)\n",
    "                history['f1_score'].append(f1)\n",
    "                scheduler.step(val_loss)\n",
    "    \n",
    "                if f1 > best_f1_score:\n",
    "            \n",
    "                #replace best accuracy and save best model\n",
    "                    print(f'New Best F1_score found: {f1*100:.2f}%\\nEpoch: {epoch + 1}\\n', \n",
    "                         f'Accuracy: {val_acc:.2f}\\nAUC: {auroc*100:.2f}')\n",
    "                    best_f1_score = f1\n",
    "                    best = deepcopy(model)\n",
    "                    curr_epoch = epoch + 1\n",
    "                    \n",
    "            #store fold performance\n",
    "            foldperf['fold{}'.format(fold+1)] = history\n",
    "    \n",
    "        #create dict to store fold performance\n",
    "        target_df_dict_25[f'{(k)}']['repeat{}'.format(repeat + 1)] = foldperf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb9f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each target and number of replicas\n",
    "results_25 = {}\n",
    "\n",
    "total_results_25 = pd.DataFrame(columns =['Target', 'Repeat', 'Fold', 'Accuracy', 'Precision_(Binary)', 'Recall_(Binary)', \n",
    "                                       'F1-score_(Binary)', 'AUC'])\n",
    "\n",
    "for i in targets:\n",
    "    results_25[i] = {}\n",
    "    for k in range(1, replicas + 1):\n",
    "        results_25[i][f'repeat{k}'] = pd.DataFrame.from_dict(target_df_dict_25[i][f'repeat{k}'], orient = 'index') # convert dict to dataframe\n",
    "        \n",
    "        #explode to get eacxh epoch as a row\n",
    "        results_25[i][f'repeat{k}'] = results_25[i][f'repeat{k}'].explode(list(results_25[i][f'repeat{k}'].columns))\n",
    "        results_25[i][f'repeat{k}']['fold'] = results_25[i][f'repeat{k}'].index\n",
    "        results_25[i][f'repeat{k}'].reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        #obtain the result that was obtained at the last possible choice\n",
    "        placeholder_df = results_25[i][f'repeat{k}'].groupby('fold')[['val_acc', 'precision',\n",
    "                                                                      'recall', 'f1_score', 'auroc']].last().reset_index()\n",
    "        \n",
    "        #making adjustments to concat with total_results properly\n",
    "        placeholder_df.columns = ['Fold', 'Accuracy', 'Precision_(Binary)', 'Recall_(Binary)', 'F1-score_(Binary)', 'AUC']\n",
    "        placeholder_df = placeholder_df.reindex(columns = total_results_25.columns)\n",
    "        \n",
    "        #fill new missing values\n",
    "        placeholder_df['Target'], placeholder_df['Repeat'] = i, k\n",
    "        total_results_25 = total_results_25.append(placeholder_df, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f528c",
   "metadata": {},
   "source": [
    "### Test on data - The first 50 days\n",
    "\n",
    "Same hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03274f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create main dict results for 50 days\n",
    "target_df_dict_50 = {}\n",
    "\n",
    "for k in tqdm(targets):\n",
    "    print(k)\n",
    "    \n",
    "    #create main dict results\n",
    "    \n",
    "    target_df_dict_50[f'{(k)}'] = {}\n",
    "    y = df_targets[k].values\n",
    "\n",
    "    #create a list containing one value per row\n",
    "    all_indices = list(range(len(df_targets)))\n",
    "    \n",
    "    #using train test split to later apply the rule accordingly\n",
    "    train_ind, test_ind = train_test_split(all_indices, test_size=0.2, \n",
    "                                           random_state = 5, stratify = y)\n",
    "    \n",
    "    #applied train_test_split rules accordingly\n",
    "    X_train_val = nd_array_100[train_ind,:51,:]\n",
    "    y_train_val = y[train_ind]\n",
    "    \n",
    "    X_test = nd_array_100[test_ind, :51, :]\n",
    "    y_test = y[test_ind]    \n",
    "        \n",
    "    #reset \"best accuracy for treshold i and target k\"     \n",
    "    for repeat in range(replicas):\n",
    "        print('Replica {}'.format(repeat+1))\n",
    "        \n",
    "        foldperf={}\n",
    "        best_f1_score = 0\n",
    "        \n",
    "        #make train_val split\n",
    "        for fold, (train_idx,val_idx) in tqdm(enumerate(splits.split(X_train_val, y_train_val))):\n",
    "            print('Split {}'.format(fold + 1))\n",
    "            \n",
    "            #make split between train and Val\n",
    "            X_train, y_train = X_train_val[train_idx], y_train_val[train_idx]\n",
    "            X_val, y_val = X_train_val[val_idx], y_train_val[val_idx]\n",
    "            \n",
    "            #scaling requires one scaler per channel (feature)\n",
    "            scalers = {}\n",
    "            for feature in range(X_train.shape[2]):\n",
    "                           \n",
    "                scalers[feature] = RobustScaler()\n",
    "                X_train[:, :, feature] = scalers[feature].fit_transform(X_train[:, :, feature]) \n",
    "\n",
    "            for col in range(X_val.shape[2]):\n",
    "                X_val[:, :, feature] = scalers[feature].transform(X_val[:, :, feature]) \n",
    "            \n",
    "            #need to oversample - will use smote\n",
    "            #will also require oneoversampler per channel (feature)\n",
    "            samplers = {}\n",
    "            \n",
    "            #create new nd_array with the correct size - 2 * majority class \n",
    "            X_train_res = np.zeros(shape = (2* (int(sc.stats.mode(y_train)[1])), X_train.shape[1], X_train.shape[2]))\n",
    "            for feature in range(X_train.shape[2]):\n",
    "                           \n",
    "                samplers[feature] = SMOTE()\n",
    "                X_train_res[:, :, feature], y_train_res = samplers[feature].fit_resample(X_train[:, :, feature], y_train) \n",
    "            \n",
    "            #second, convert everything to pytorch tensor - we will convert to tensor dataset and \n",
    "            X_train_tensors = torch.from_numpy(X_train_res)\n",
    "            X_val_tensors = torch.from_numpy(X_val)\n",
    "            \n",
    "            #convert X tensors to format FloatTensor\n",
    "            X_train_tensors = X_train_tensors.type(torch.cuda.FloatTensor)\n",
    "            X_val_tensors = X_val_tensors.type(torch.cuda.FloatTensor)\n",
    "            \n",
    "            #create y_tensor\n",
    "            y_train_tensors = torch.from_numpy(y_train_res)\n",
    "            y_val_tensors = torch.from_numpy(y_val)\n",
    "            \n",
    "            #convert y tensors to format longtensor\n",
    "            y_train_tensors = y_train_tensors.type(torch.cuda.FloatTensor)\n",
    "            y_val_tensors = y_val_tensors.type(torch.cuda.FloatTensor)\n",
    "            \n",
    "            #create Tensor Datasets and dataloaders for both Train and Val\n",
    "            train_dataset = TensorDataset(X_train_tensors, y_train_tensors)\n",
    "            val_dataset = TensorDataset(X_val_tensors, y_val_tensors)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "            #creates new model for each \n",
    "            model = LSTM_Uni(num_classes, input_size, hidden_size, num_layers, X_train_tensors.shape[1], dropout).to('cuda') #our lstm class\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "            scheduler = ReduceLROnPlateau(optimizer, \n",
    "                                 'min', \n",
    "                                 patience = 15,\n",
    "                                 cooldown = 20,\n",
    "                                threshold=0.00001,\n",
    "                                factor = 0.33,\n",
    "                                verbose = True)\n",
    "    \n",
    "            history = {'train_loss': [], 'val_loss': [],'train_acc':[],'val_acc':[], 'precision': [],\n",
    "                      'recall' : [], 'auroc': [], 'f1_score' : []}\n",
    "\n",
    "            for epoch in tqdm(range(num_epochs)):\n",
    "                train_loss, train_correct=train_epoch(model,train_loader,criterion,optimizer)\n",
    "                val_loss, val_correct, precision, recall, auroc, f1 = valid_epoch(model,val_loader,criterion)\n",
    "\n",
    "                train_loss = train_loss / len(train_loader.sampler)\n",
    "                train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "                val_loss = val_loss / len(val_loader.sampler)\n",
    "                val_acc = val_correct / len(val_loader.sampler) * 100\n",
    "        \n",
    "        \n",
    "                if (epoch+1) % 10 == 0: \n",
    "                 print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Validation Loss:{:.3f} AVG Training Acc {:.2f} % AVG Validation Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                             num_epochs,\n",
    "                                                                                                             train_loss,\n",
    "                                                                                                             val_loss,\n",
    "                                                                                                             train_acc,\n",
    "                                                                                                             val_acc))\n",
    "                history['train_loss'].append(train_loss)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['train_acc'].append(train_acc)\n",
    "                history['val_acc'].append(val_acc)\n",
    "                history['precision'].append(precision)\n",
    "                history['recall'].append(recall)\n",
    "                history['auroc'].append(auroc)\n",
    "                history['f1_score'].append(f1)\n",
    "                scheduler.step(val_loss)\n",
    "    \n",
    "                if f1 > best_f1_score:\n",
    "            \n",
    "                #replace best accuracy and save best model\n",
    "                    print(f'New Best F1_score found: {f1*100:.2f}%\\nEpoch: {epoch + 1}\\n', \n",
    "                         f'Accuracy: {val_acc:.2f}\\nAUC: {auroc*100:.2f}')\n",
    "                    best_f1_score = f1\n",
    "                    best = deepcopy(model)\n",
    "                    curr_epoch = epoch + 1\n",
    "                    \n",
    "            #store fold performance\n",
    "            foldperf['fold{}'.format(fold+1)] = history\n",
    "    \n",
    "        #create dict to store fold performance\n",
    "        target_df_dict_50[f'{(k)}']['repeat{}'.format(repeat + 1)] = foldperf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bbd70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each target and number of replicas\n",
    "results_50 = {}\n",
    "\n",
    "total_results_50 = pd.DataFrame(columns =['Target', 'Repeat', 'Fold', 'Accuracy', 'Precision_(Binary)', 'Recall_(Binary)', \n",
    "                                       'F1-score_(Binary)', 'AUC'])\n",
    "\n",
    "for i in targets:\n",
    "    results_50[i] = {}\n",
    "    for k in range(1, replicas + 1):\n",
    "        results_50[i][f'repeat{k}'] = pd.DataFrame.from_dict(target_df_dict_50[i][f'repeat{k}'], orient = 'index') # convert dict to dataframe\n",
    "        \n",
    "        #explode to get eacxh epoch as a row\n",
    "        results_50[i][f'repeat{k}'] = results_50[i][f'repeat{k}'].explode(list(results_50[i][f'repeat{k}'].columns))\n",
    "        results_50[i][f'repeat{k}']['fold'] = results_50[i][f'repeat{k}'].index\n",
    "        results_50[i][f'repeat{k}'].reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        #obtain the result that was obtained at the last possible choice\n",
    "        placeholder_df = results_50[i][f'repeat{k}'].groupby('fold')[['val_acc', 'precision',\n",
    "                                                                      'recall', 'f1_score', 'auroc']].last().reset_index()\n",
    "        \n",
    "        #making adjustments to concat with total_results properly\n",
    "        placeholder_df.columns = ['Fold', 'Accuracy', 'Precision_(Binary)', 'Recall_(Binary)', 'F1-score_(Binary)', 'AUC']\n",
    "        placeholder_df = placeholder_df.reindex(columns = total_results_50.columns)\n",
    "        \n",
    "        #fill new missing values\n",
    "        placeholder_df['Target'], placeholder_df['Repeat'] = i, k\n",
    "        total_results_50 = total_results_50.append(placeholder_df, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed2e66d",
   "metadata": {},
   "source": [
    "### Test on data - The first 75 days\n",
    "\n",
    "Same hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c65679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create main dict results for 75 days\n",
    "target_df_dict_75 = {}\n",
    "\n",
    "for k in tqdm(targets):\n",
    "    print(k)\n",
    "    \n",
    "    #create main dict results\n",
    "    \n",
    "    target_df_dict_75[f'{(k)}'] = {}\n",
    "    y = df_targets[k].values\n",
    "\n",
    "    #create a list containing one value per row\n",
    "    all_indices = list(range(len(df_targets)))\n",
    "    \n",
    "    #using train test split to later apply the rule accordingly\n",
    "    train_ind, test_ind = train_test_split(all_indices, test_size=0.2, \n",
    "                                           random_state = 5, stratify = y)\n",
    "    \n",
    "    #applied train_test_split rules accordingly\n",
    "    X_train_val = nd_array_100[train_ind,:76,:]\n",
    "    y_train_val = y[train_ind]\n",
    "    \n",
    "    X_test = nd_array_100[test_ind, :76, :]\n",
    "    y_test = y[test_ind]    \n",
    "        \n",
    "    #reset \"best accuracy for treshold i and target k\"     \n",
    "    for repeat in range(replicas):\n",
    "        print('Replica {}'.format(repeat+1))\n",
    "        \n",
    "        foldperf={}\n",
    "        best_f1_score = 0\n",
    "        \n",
    "        #make train_val split\n",
    "        for fold, (train_idx,val_idx) in tqdm(enumerate(splits.split(X_train_val, y_train_val))):\n",
    "            print('Split {}'.format(fold + 1))\n",
    "            \n",
    "            #make split between train and Val\n",
    "            X_train, y_train = X_train_val[train_idx], y_train_val[train_idx]\n",
    "            X_val, y_val = X_train_val[val_idx], y_train_val[val_idx]\n",
    "            \n",
    "            #scaling requires one scaler per channel (feature)\n",
    "            scalers = {}\n",
    "            for feature in range(X_train.shape[2]):\n",
    "                           \n",
    "                scalers[feature] = RobustScaler()\n",
    "                X_train[:, :, feature] = scalers[feature].fit_transform(X_train[:, :, feature]) \n",
    "\n",
    "            for col in range(X_val.shape[2]):\n",
    "                X_val[:, :, feature] = scalers[feature].transform(X_val[:, :, feature]) \n",
    "            \n",
    "            #need to oversample - will use smote\n",
    "            #will also require oneoversampler per channel (feature)\n",
    "            samplers = {}\n",
    "            \n",
    "            #create new nd_array with the correct size - 2 * majority class \n",
    "            X_train_res = np.zeros(shape = (2* (int(sc.stats.mode(y_train)[1])), X_train.shape[1], X_train.shape[2]))\n",
    "            for feature in range(X_train.shape[2]):\n",
    "                           \n",
    "                samplers[feature] = SMOTE()\n",
    "                X_train_res[:, :, feature], y_train_res = samplers[feature].fit_resample(X_train[:, :, feature], y_train) \n",
    "            \n",
    "            #second, convert everything to pytorch tensor - we will convert to tensor dataset and \n",
    "            X_train_tensors = torch.from_numpy(X_train_res)\n",
    "            X_val_tensors = torch.from_numpy(X_val)\n",
    "            \n",
    "            #convert X tensors to format FloatTensor\n",
    "            X_train_tensors = X_train_tensors.type(torch.cuda.FloatTensor)\n",
    "            X_val_tensors = X_val_tensors.type(torch.cuda.FloatTensor)\n",
    "            \n",
    "            #create y_tensor\n",
    "            y_train_tensors = torch.from_numpy(y_train_res)\n",
    "            y_val_tensors = torch.from_numpy(y_val)\n",
    "            \n",
    "            #convert y tensors to format longtensor\n",
    "            y_train_tensors = y_train_tensors.type(torch.cuda.FloatTensor)\n",
    "            y_val_tensors = y_val_tensors.type(torch.cuda.FloatTensor)\n",
    "            \n",
    "            #create Tensor Datasets and dataloaders for both Train and Val\n",
    "            train_dataset = TensorDataset(X_train_tensors, y_train_tensors)\n",
    "            val_dataset = TensorDataset(X_val_tensors, y_val_tensors)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "            #creates new model for each \n",
    "            model = LSTM_Uni(num_classes, input_size, hidden_size, num_layers, X_train_tensors.shape[1], dropout).to('cuda') #our lstm class\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "            scheduler = ReduceLROnPlateau(optimizer, \n",
    "                                 'min', \n",
    "                                 patience = 15,\n",
    "                                 cooldown = 20,\n",
    "                                threshold=0.00001,\n",
    "                                factor = 0.33,\n",
    "                                verbose = True)\n",
    "    \n",
    "            history = {'train_loss': [], 'val_loss': [],'train_acc':[],'val_acc':[], 'precision': [],\n",
    "                      'recall' : [], 'auroc': [], 'f1_score' : []}\n",
    "\n",
    "            for epoch in tqdm(range(num_epochs)):\n",
    "                train_loss, train_correct=train_epoch(model,train_loader,criterion,optimizer)\n",
    "                val_loss, val_correct, precision, recall, auroc, f1 = valid_epoch(model,val_loader,criterion)\n",
    "\n",
    "                train_loss = train_loss / len(train_loader.sampler)\n",
    "                train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "                val_loss = val_loss / len(val_loader.sampler)\n",
    "                val_acc = val_correct / len(val_loader.sampler) * 100\n",
    "        \n",
    "        \n",
    "                if (epoch+1) % 10 == 0: \n",
    "                 print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Validation Loss:{:.3f} AVG Training Acc {:.2f} % AVG Validation Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                             num_epochs,\n",
    "                                                                                                             train_loss,\n",
    "                                                                                                             val_loss,\n",
    "                                                                                                             train_acc,\n",
    "                                                                                                             val_acc))\n",
    "                history['train_loss'].append(train_loss)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['train_acc'].append(train_acc)\n",
    "                history['val_acc'].append(val_acc)\n",
    "                history['precision'].append(precision)\n",
    "                history['recall'].append(recall)\n",
    "                history['auroc'].append(auroc)\n",
    "                history['f1_score'].append(f1)\n",
    "                scheduler.step(val_loss)\n",
    "    \n",
    "                if f1 > best_f1_score:\n",
    "            \n",
    "                #replace best accuracy and save best model\n",
    "                    print(f'New Best F1_score found: {f1*100:.2f}%\\nEpoch: {epoch + 1}\\n', \n",
    "                         f'Accuracy: {val_acc:.2f}\\nAUC: {auroc*100:.2f}')\n",
    "                    best_f1_score = f1\n",
    "                    best = deepcopy(model)\n",
    "                    curr_epoch = epoch + 1\n",
    "                    \n",
    "            #store fold performance\n",
    "            foldperf['fold{}'.format(fold+1)] = history\n",
    "    \n",
    "        #create dict to store fold performance\n",
    "        target_df_dict_75[f'{(k)}']['repeat{}'.format(repeat + 1)] = foldperf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d9cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each target and number of replicas\n",
    "results_75 = {}\n",
    "\n",
    "total_results_75 = pd.DataFrame(columns =['Target', 'Repeat', 'Fold', 'Accuracy', 'Precision_(Binary)', 'Recall_(Binary)', \n",
    "                                       'F1-score_(Binary)', 'AUC'])\n",
    "\n",
    "for i in targets:\n",
    "    results_75[i] = {}\n",
    "    for k in range(1, replicas + 1):\n",
    "        results_75[i][f'repeat{k}'] = pd.DataFrame.from_dict(target_df_dict_75[i][f'repeat{k}'], orient = 'index') # convert dict to dataframe\n",
    "        \n",
    "        #explode to get eacxh epoch as a row\n",
    "        results_75[i][f'repeat{k}'] = results_75[i][f'repeat{k}'].explode(list(results_75[i][f'repeat{k}'].columns))\n",
    "        results_75[i][f'repeat{k}']['fold'] = results_75[i][f'repeat{k}'].index\n",
    "        results_75[i][f'repeat{k}'].reset_index(drop = True, inplace = True)\n",
    "        \n",
    "        #obtain the result that was obtained at the last possible choice\n",
    "        placeholder_df = results_75[i][f'repeat{k}'].groupby('fold')[['val_acc', 'precision',\n",
    "                                                                      'recall', 'f1_score', 'auroc']].last().reset_index()\n",
    "        \n",
    "        #making adjustments to concat with total_results properly\n",
    "        placeholder_df.columns = ['Fold', 'Accuracy', 'Precision_(Binary)', 'Recall_(Binary)', 'F1-score_(Binary)', 'AUC']\n",
    "        placeholder_df = placeholder_df.reindex(columns = total_results_75.columns)\n",
    "        \n",
    "        #fill new missing values\n",
    "        placeholder_df['Target'], placeholder_df['Repeat'] = i, k\n",
    "        total_results_75 = total_results_75.append(placeholder_df, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d25a58",
   "metadata": {},
   "source": [
    "### Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98910c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving agregate results in Excel File\n",
    "with pd.ExcelWriter('../Data/Modeling Stage/Results/IMS/Clicks per day/model_2_results.xlsx') as writer:\n",
    "    total_results_25.to_excel(writer, sheet_name = 'total_results_25', index = False)\n",
    "    total_results_50.to_excel(writer, sheet_name = 'total_results_50', index = False)\n",
    "    total_results_75.to_excel(writer, sheet_name = 'total_results_75', index = False)\n",
    "    \n",
    "#saving raw_results in json file\n",
    "import json\n",
    "daily_clicks_raw_results = {'target_df_dict_25', \n",
    "               'target_df_dict_50', \n",
    "               'target_df_dict_75'}\n",
    "\n",
    "# e.g. file = './data.json' \n",
    "with open(\"../Data/Modeling Stage/Results/IMS/Clicks per day/model_2_raw_results.json\", 'w') as f: \n",
    "    json.dump(daily_clicks_raw_results, f,  indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
