{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84eda90e",
   "metadata": {},
   "source": [
    "### Thesis notebook 4.3. - NOVA IMS\n",
    "\n",
    "#### LSTM - Temporal data representation\n",
    "\n",
    "In this notebook, we will finally start our application of temporal representation using LSTMs and bi-directional LSTMs.\n",
    "The argument for the usage of Deep Learning stems from the fact that sequences themselves encode information that can be extracted using Recurrent Neural Networks and, more specifically, Long Short Term Memory Units.\n",
    "\n",
    "#### First Step: Setup a PyTorch environment that enables the use of GPU for training. \n",
    "\n",
    "The following cell wll confirm that the GPU will be the default device to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f27844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "cuda.init()\n",
    "## Get Id of default device\n",
    "torch.cuda.current_device()\n",
    "# 0\n",
    "cuda.Device(0).name() # '0' is the id of your GPU\n",
    "\n",
    "#set all tensors to gpu\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95429e",
   "metadata": {},
   "source": [
    "#### Second Step: Import the relevant packages and declare global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c2d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules/libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "#tqdm to monitor progress\n",
    "from tqdm.notebook import tqdm, trange\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "#time related features\n",
    "from datetime import timedelta\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "#vizualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#imblearn, scalers, kfold and metrics\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer,PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, recall_score, classification_report, average_precision_score, precision_recall_curve\n",
    "\n",
    "#import torch related\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "#and optimizer of learning rate\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "#import pytorch modules\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c3f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables that may come in handy\n",
    "#course threshold sets the % duration that will be considered (1 = 100%)\n",
    "duration_threshold = [0.1, 0.25, 0.33, 0.5, 1]\n",
    "\n",
    "#colors for vizualizations\n",
    "nova_ims_colors = ['#BFD72F', '#5C666C']\n",
    "\n",
    "#standard color for student aggregates\n",
    "student_color = '#474838'\n",
    "\n",
    "#standard color for course aggragates\n",
    "course_color = '#1B3D2F'\n",
    "\n",
    "#standard continuous colormap\n",
    "standard_cmap = 'viridis_r'\n",
    "\n",
    "#Function designed to deal with multiindex and flatten it\n",
    "def flattenHierarchicalCol(col,sep = '_'):\n",
    "    '''converts multiindex columns into single index columns while retaining the hierarchical components'''\n",
    "    if not type(col) is tuple:\n",
    "        return col\n",
    "    else:\n",
    "        new_col = ''\n",
    "        for leveli,level in enumerate(col):\n",
    "            if not level == '':\n",
    "                if not leveli == 0:\n",
    "                    new_col += sep\n",
    "                new_col += level\n",
    "        return new_col\n",
    "    \n",
    "#number of replicas - number of repeats of stratified k fold - in this case 10\n",
    "replicas = 30\n",
    "\n",
    "#names to display on result figures\n",
    "date_names = {\n",
    "             'Date_threshold_10': '10% of Course Duration',   \n",
    "             'Date_threshold_25': '25% of Course Duration', \n",
    "             'Date_threshold_33': '33% of Course Duration', \n",
    "             'Date_threshold_50': '50% of Course Duration', \n",
    "             'Date_threshold_100':'100% of Course Duration', \n",
    "            }\n",
    "\n",
    "target_names = {\n",
    "                'exam_fail' : 'At risk - Exam Grade',\n",
    "                'final_fail' : 'At risk - Final Grade', \n",
    "                'exam_gifted' : 'High performer - Exam Grade', \n",
    "                'final_gifted': 'High performer - Final Grade'\n",
    "                }\n",
    "\n",
    "#targets\n",
    "targets = ['exam_fail' , 'final_fail' , 'exam_gifted' , 'final_gifted']\n",
    "temporal_columns = ['0 to 4%', '4 to 8%', '8 to 12%', '12 to 16%', '16 to 20%', '20 to 24%',\n",
    "       '24 to 28%', '28 to 32%', '32 to 36%', '36 to 40%', '40 to 44%',\n",
    "       '44 to 48%', '48 to 52%', '52 to 56%', '56 to 60%', '60 to 64%',\n",
    "       '64 to 68%', '68 to 72%', '72 to 76%', '76 to 80%', '80 to 84%',\n",
    "       '84 to 88%', '88 to 92%', '92 to 96%', '96 to 100%']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c5ddb6",
   "metadata": {},
   "source": [
    "#### Step 3: Import data and take a preliminary look at it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a23ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports dataframes\n",
    "course_programs = pd.read_excel(\"../Data/Modeling Stage/Nova_IMS_Temporal_Datasets_25_splits.xlsx\", \n",
    "                                dtype = {\n",
    "                                    'course_encoding' : int,\n",
    "                                    'userid' : int},\n",
    "                               sheet_name = None)\n",
    "\n",
    "#save tables \n",
    "student_list = pd.read_csv('../Data/Modeling Stage/Nova_IMS_Filtered_targets.csv', \n",
    "                         dtype = {\n",
    "                                   'course_encoding': int,\n",
    "                                   'userid' : int,\n",
    "                                   })\n",
    "\n",
    "#drop unnamed 0 column\n",
    "for i in course_programs:\n",
    "        \n",
    "    #merge with the targets we calculated on the other \n",
    "    course_programs[i] = course_programs[i].merge(student_list, on = ['course_encoding', 'userid'], how = 'inner')\n",
    "    course_programs[i].drop(['Unnamed: 0', 'exam_mark', 'final_mark'], axis = 1, inplace = True)\n",
    "    \n",
    "    #convert results to object\n",
    "    course_programs[i]['course_encoding'], course_programs[i]['userid'] = course_programs[i]['course_encoding'].astype(object), course_programs[i]['userid'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_programs['Date_threshold_100'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a751ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_programs['Date_threshold_100'].describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3291817",
   "metadata": {},
   "source": [
    "In our first attempt, we will use the absolute number of clicks made by each student - scaled using standard scaler. \n",
    "Therefore, we can start by immediately placing our course encoding/userid pairings into the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be722ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for scalers\n",
    "def normalize(dataset,scaler):\n",
    "    \n",
    "    if scaler == 'MinMax':\n",
    "        pt = MinMaxScaler()\n",
    "    elif scaler == 'Standard':\n",
    "        pt = StandardScaler()\n",
    "    elif scaler == 'Robust':\n",
    "        pt = RobustScaler()\n",
    "    elif scaler == 'Quantile':\n",
    "        pt = QuantileTransformer()\n",
    "    else:\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "    \n",
    "    data = pt.fit_transform(dataset)\n",
    "    \n",
    "    # convert the array back to a dataframe\n",
    "    normalized_df = pd.DataFrame(data,columns=dataset.columns)\n",
    "    return normalized_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea7510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create backup\n",
    "normalized_data = deepcopy(course_programs)\n",
    "\n",
    "#convert index\n",
    "for i in tqdm(normalized_data):\n",
    "    normalized_data[i].set_index(['course_encoding', 'userid'], drop = True, inplace = True)\n",
    "    normalized_data[i].fillna(0, inplace = True)\n",
    "    #Then, apply normalize function to rescale the train columns\n",
    "    normalized_data[i] = normalize(normalized_data[i].filter(temporal_columns),'Standard')\n",
    "    \n",
    "    #and remerge target columns\n",
    "    normalized_data[i][targets] =  deepcopy(course_programs[i][targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4d5475",
   "metadata": {},
   "source": [
    "#### Implementing Cross-Validation with Deep Learning Model\n",
    "\n",
    "**1. Create the Deep Learning Model**\n",
    "\n",
    "In this instance, we will follow-up with on the approach used in Chen & Cui - CrossEntropyLoss with applied over a softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a16bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Uni(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM_Uni, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first = True) #lstm\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "    \n",
    "        self.fc = nn.Linear(self.hidden_size, num_classes) #fully connected last layer\n",
    "\n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        \n",
    "        #Xavier_init for both H_0 and C_0\n",
    "        torch.nn.init.xavier_normal_(h_0)\n",
    "        torch.nn.init.xavier_normal_(c_0)\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        lstm_out, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        last_output = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        \n",
    "        #we are interested in only keeping the last output\n",
    "        drop_out = self.dropout(last_output)\n",
    "        pre_softmax = self.fc(drop_out) #Final Output - dense\n",
    "        return pre_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c356bd",
   "metadata": {},
   "source": [
    "**2. Define the train and validation Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b29a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,dataloader,loss_fn,optimizer):\n",
    "    \n",
    "    train_loss,train_correct=0.0,0 \n",
    "    model.train()\n",
    "    for X, labels in dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = loss_fn(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * X.size(0)\n",
    "        scores, predictions = torch.max(F.log_softmax(output.data), 1)\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "    return train_loss,train_correct\n",
    "  \n",
    "def valid_epoch(model,dataloader,loss_fn):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    targets = []\n",
    "    y_pred = []\n",
    "    probability_1 = []\n",
    "    \n",
    "    model.eval()\n",
    "    for X, labels in dataloader:\n",
    "\n",
    "        output = model(X)\n",
    "        loss=loss_fn(output,labels)\n",
    "        valid_loss+=loss.item()*X.size(0)\n",
    "        probability_1.append(F.softmax(output.data)[:,1])\n",
    "        predictions = torch.argmax(output, dim=1)\n",
    "        val_correct+=(predictions == labels).sum().item()\n",
    "        targets.append(labels)\n",
    "        y_pred.append(predictions)\n",
    "    \n",
    "    #concat all results\n",
    "    targets = torch.cat(targets).data.cpu().numpy()\n",
    "    y_pred = torch.cat(y_pred).data.cpu().numpy()\n",
    "    probability_1 = torch.cat(probability_1).data.cpu().numpy()\n",
    "    \n",
    "    #calculate precision, recall and AUC score\n",
    "    \n",
    "    precision = precision_score(targets, y_pred)\n",
    "    recall = recall_score(targets, y_pred)\n",
    "    auroc = roc_auc_score(targets, probability_1)\n",
    "    \n",
    "    #return all\n",
    "    return valid_loss,val_correct, precision, recall, auroc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4543fb3",
   "metadata": {},
   "source": [
    "**3. Define main hyperparameters of the model, including splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbbef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "num_epochs = 100 #50 epochs\n",
    "learning_rate = 0.01 #0.01 lr\n",
    "input_size = 1 #number of features\n",
    "hidden_size = 40 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "#Shape of Output as required for SoftMax Classifier\n",
    "num_classes = 2 #output shape\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "k=10\n",
    "splits= RepeatedKFold(n_splits=k, n_repeats=replicas, random_state=15) #kfold of 10 with 30 replicas\n",
    "criterion = nn.CrossEntropyLoss()    # cross-entropy for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20713d9",
   "metadata": {},
   "source": [
    "**4. Make the splits and Start Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45544589",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(normalized_data.keys()):\n",
    "    \n",
    "    print(i)\n",
    "    threshold_dict = {} #dict to store information in for each threshold\n",
    "    data = deepcopy(normalized_data[i])\n",
    "    \n",
    "    #set X and Y columns\n",
    "    X = data[data.columns[:25]] #different timesteps\n",
    "    y = data[data.columns[-4:]] #the 4 different putative targets\n",
    "    \n",
    "    for k in tqdm(targets):\n",
    "        print(k)\n",
    "        \n",
    "        #Start with train test split\n",
    "        X_train_val, X_test, y_train_val, y_test, = train_test_split(\n",
    "                                    X,\n",
    "                                   y[k], #replace when going for multi-target \n",
    "                                   test_size = 0.20,\n",
    "                                   random_state = 15,\n",
    "                                   shuffle=True,\n",
    "                                   stratify = y[k] #replace when going for multi-target\n",
    "                                    )\n",
    "\n",
    "        # ##second, convert everything to pytorch tensor - we will convert to tensor dataset and \n",
    "        X_train_tensors = Variable(torch.Tensor(X_train_val.values))\n",
    "        X_test_tensors = Variable(torch.Tensor(X_test.values))\n",
    "\n",
    "        y_train_tensors = Variable(torch.Tensor(y_train_val.values))\n",
    "        y_test_tensors = Variable(torch.Tensor(y_test.values)) \n",
    "\n",
    "        #now, we have a dataset with features\n",
    "        print('Before reshaping:')\n",
    "        print(\"Training Shape\", X_train_tensors.shape, y_train_tensors.shape)\n",
    "        print(\"Testing Shape\", X_test_tensors.shape, y_test_tensors.shape)\n",
    "\n",
    "        #reshaping to rows, timestamps, features \n",
    "        X_train_tensors = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], X_train_tensors.shape[1], 1))\n",
    "        X_test_tensors = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], X_test_tensors.shape[1], 1))\n",
    "\n",
    "        #repeat for y\n",
    "        y_train_tensors = y_train_tensors.type(torch.cuda.LongTensor)\n",
    "        y_test_tensors = y_test_tensors.type(torch.cuda.LongTensor)\n",
    "\n",
    "        print('\\nAfter reshaping:')\n",
    "        print(\"Training Shape\", X_train_tensors.shape, y_train_tensors.shape)\n",
    "        print(\"Testing Shape\", X_test_tensors.shape, y_test_tensors.shape)\n",
    "\n",
    "        #create dataset\n",
    "        dataset = TensorDataset(X_train_tensors, y_train_tensors)\n",
    "        \n",
    "        #create dict to store fold performance\n",
    "        foldperf={}\n",
    "        \n",
    "        #reset \"best accuracy for treshold i and target k\"\n",
    "        \n",
    "        best_accuracy = 0\n",
    "\n",
    "        for fold, (train_idx,val_idx) in tqdm(enumerate(splits.split(np.arange(len(dataset))))):\n",
    "\n",
    "            print('Split {}'.format(fold + 1))\n",
    "\n",
    "            train_sampler = SubsetRandomSampler(train_idx)\n",
    "            val_sampler = SubsetRandomSampler(val_idx)\n",
    "            train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "            val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "    \n",
    "            #creates new model for each \n",
    "            model = LSTM_Uni(num_classes, input_size, hidden_size, num_layers, X_train_tensors.shape[1]).to('cuda') #our lstm class\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "            scheduler = ReduceLROnPlateau(optimizer, \n",
    "                                  'min', \n",
    "                                  patience = 10,\n",
    "                                  cooldown = 20,\n",
    "                                 verbose = True)\n",
    "    \n",
    "            history = {'train_loss': [], 'val_loss': [],'train_acc':[],'val_acc':[], 'precision': [],\n",
    "                      'recall' : [], 'auroc': []}\n",
    "\n",
    "            for epoch in tqdm(range(num_epochs)):\n",
    "                train_loss, train_correct=train_epoch(model,train_loader,criterion,optimizer)\n",
    "                val_loss, val_correct, precision, recall, auroc = valid_epoch(model,val_loader,criterion)\n",
    "\n",
    "                train_loss = train_loss / len(train_loader.sampler)\n",
    "                train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "                val_loss = val_loss / len(val_loader.sampler)\n",
    "                val_acc = val_correct / len(val_loader.sampler) * 100\n",
    "        \n",
    "        \n",
    "                if (epoch+1) % 10 == 0: \n",
    "                    print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Validation Loss:{:.3f} AVG Training Acc {:.2f} % AVG Validation Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                             num_epochs,\n",
    "                                                                                                             train_loss,\n",
    "                                                                                                             val_loss,\n",
    "                                                                                                             train_acc,\n",
    "                                                                                                             val_acc))\n",
    "                history['train_loss'].append(train_loss)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['train_acc'].append(train_acc)\n",
    "                history['val_acc'].append(val_acc)\n",
    "                history['precision'].append(precision)\n",
    "                history['recall'].append(recall)\n",
    "                history['auroc'].append(auroc)\n",
    "                scheduler.step(val_loss)\n",
    "    \n",
    "                if val_acc > best_accuracy:\n",
    "            \n",
    "                #replace best accuracy and save best model\n",
    "                    print(f'New Best Accuracy found: {val_acc:.2f}%\\nEpoch: {epoch + 1}')\n",
    "                    best_accuracy = val_acc\n",
    "                    best = deepcopy(model)\n",
    "                    curr_epoch = epoch + 1\n",
    "                    \n",
    "            #store fold performance\n",
    "            foldperf['fold{}'.format(fold+1)] = history\n",
    "        \n",
    "        #saves fold performance for target \n",
    "        threshold_dict[k] = pd.DataFrame.from_dict(foldperf, orient='index') # convert dict to dataframe\n",
    "        \n",
    "        #explode to get eacxh epoch as a row\n",
    "        threshold_dict[k] = threshold_dict[k].explode(list(threshold_dict[k].columns))\n",
    "        torch.save(best,f\"../Models/{i}/Nova_IMS_best_{k}_{curr_epoch}_epochs.h\")\n",
    "        \n",
    "    # from pandas.io.parsers import ExcelWriter\n",
    "    with pd.ExcelWriter(f\"../Data/Modeling Stage/Results/IMS/Clicks per % duration/25_splits_{i}_{replicas}_replicas.xlsx\") as writer:  \n",
    "        for sheet in targets:\n",
    "                threshold_dict[sheet].to_excel(writer, sheet_name=str(sheet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28def628",
   "metadata": {},
   "source": [
    "**5. Initialize Model and Train with Cross-Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4177bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cccff8a6",
   "metadata": {},
   "source": [
    "#### Defining the LSTM Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b35fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "diz_ep = {'train_loss_ep':[],'val_loss_ep':[],'train_acc_ep':[],'val_acc_ep':[]}\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    diz_ep['train_loss_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['train_loss'][i] for f in range(k)]))\n",
    "    diz_ep['val_loss_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['val_loss'][i] for f in range(k)]))\n",
    "    diz_ep['train_acc_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['train_acc'][i] for f in range(k)]))\n",
    "    diz_ep['val_acc_ep'].append(np.mean([foldperf['fold{}'.format(f+1)]['val_acc'][i] for f in range(k)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5979a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "sns.set_theme(context='paper', style='whitegrid', rc={\"figure.figsize\":(16, 10)}, font='Calibri', font_scale=2)\n",
    "plt.semilogy(diz_ep['train_loss_ep'], label='Train')\n",
    "plt.semilogy(diz_ep['val_loss_ep'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "plt.title(f'Average loss\\n {k} Folds', fontweight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7a05ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracies\n",
    "sns.set_theme(context='paper', style='whitegrid', rc={\"figure.figsize\":(16, 10)}, font='Calibri', font_scale=2)\n",
    "plt.figure()\n",
    "plt.semilogy(diz_ep['train_acc_ep'], label='Train')\n",
    "plt.semilogy(diz_ep['val_acc_ep'], label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "plt.title(f'Accuracy on Validation Data\\n{k} Folds', fontweight = 'bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840de46f",
   "metadata": {},
   "source": [
    "#### Obtaining results on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c558aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.RNN(input_size=i_size, hidden_size=h_size, num_layers = 1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294c031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run desired model\n",
    "def run_model(model_name, X, y):\n",
    "    \n",
    "    ###STANDALONE MODELS\n",
    "        ###Baseline Classifier - most frequent class\n",
    "    if model_name == 'Baseline - Majority Class':\n",
    "        model = DummyClassifier(strategy=\"most_frequent\").fit(X, y)\n",
    "    if model_name == 'LSTM':\n",
    "        '''insert code for LSTM classifier'''\n",
    "    if model_name == 'Bi-directional LSTM':\n",
    "        '''insert code for Bi directional - LSTM classifier'''\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef67328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#averages scores of each run (for the present model) in each iteration of Repeated 10-fold CV that has been called\n",
    "def avg_score(method,X,y, model_name):\n",
    "    \n",
    "    f1micro_train = []\n",
    "    f1micro_val = []\n",
    "    precision_train = []\n",
    "    precision_val = []\n",
    "    recall_train = []\n",
    "    recall_val = []\n",
    "    timer = []\n",
    "    cm_holder = []\n",
    "    test_holder = []\n",
    "    averaged_confusion_matrix=None\n",
    "    \n",
    "    for train_index, val_index in method.split(X,y):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        begin = time.perf_counter()\n",
    "        model = run_model(model_name, X_train, y_train)\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        labels_train = model.predict(X_train)\n",
    "        labels_val = model.predict(X_val)\n",
    "        \n",
    "        f1micro_train.append(f1_score(y_train, labels_train, average='micro'))\n",
    "        f1micro_val.append(f1_score(y_val, labels_val, average='micro'))\n",
    "        \n",
    "        precision_train.append(precision_score(y_train, labels_train))\n",
    "        precision_val.append(precision_score(y_val, labels_val))\n",
    "        \n",
    "        recall_train.append(recall_score(y_train, labels_train))\n",
    "        recall_val.append(recall_score(y_val, labels_val))\n",
    "        \n",
    "        timer.append(end-begin)\n",
    "        \n",
    "        cm_holder.append(confusion_matrix(y_val, labels_val))\n",
    "        \n",
    "    model = run_model(model_name, X,y)\n",
    "    labels_test = model.predict(X_test)\n",
    "    \n",
    "    f1micro_test = f1_score(y_test, labels_test, average='micro')\n",
    "    precision_test = precision_score(y_test, labels_test)\n",
    "    recall_test = recall_score(y_test, labels_test)\n",
    "    print(f'Classification Report for {model_name}:\\nTest Data\\n{classification_report(y_test, labels_test)}\\n\\n')\n",
    "    \n",
    "    # calculate the average and the std for each measure (accuracy, time and number of iterations)\n",
    "    avg_time = round(np.mean(timer),3)\n",
    "    avg_f1_train = round(np.mean(f1micro_train),3)\n",
    "    avg_f1_val = round(np.mean(f1micro_val),3)\n",
    "    avg_f1_test = round(np.mean(f1micro_test),3)\n",
    "    avg_precision_train = round(np.mean(precision_train),3)\n",
    "    avg_precision_val = round(np.mean(precision_val),3)\n",
    "    avg_precision_test = round(precision_test,3)\n",
    "    avg_recall_train = round(np.mean(recall_train),3)\n",
    "    avg_recall_val = round(np.mean(recall_val),3)\n",
    "    avg_recall_test = round(recall_test,3)\n",
    "    \n",
    "    std_time = round(np.std(timer),3)\n",
    "    std_f1_train = round(np.std(f1micro_train),3)\n",
    "    std_f1_val = round(np.std(f1micro_test),3)\n",
    "    std_precision_train = round(np.std(precision_train),3)\n",
    "    std_precision_val = round(np.std(precision_val),3)\n",
    "    std_recall_train = round(np.std(recall_train),3)\n",
    "    std_recall_val = round(np.std(recall_val),3)\n",
    "    \n",
    "    averaged_confusion_matrix = np.mean(cm_holder, axis = 0).round(2)\n",
    "    \n",
    "    #from sklearn.metrics import cohen_kappa_score\n",
    "    \n",
    "    return str(avg_time) + '+/-' + str(std_time), str(avg_f1_train) + '+/-' + str(std_f1_train), str(avg_f1_val) + '+/-' + str(std_f1_val), str(avg_f1_test), str(avg_precision_train) + '+/-' + str(std_precision_train), str(avg_precision_val) + '+/-' + str(std_precision_val), str(avg_precision_test), str(avg_recall_train) + '+/-' + str(std_recall_train), str(avg_recall_val) + '+/-' + str(std_recall_val), str(avg_recall_test), averaged_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a4b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_bar(models, f1micro_train, f1micro_val, f1micro_test, path, date, target):\n",
    "    \n",
    "    sns.set_theme(context='paper', style='whitegrid', font='Calibri', font_scale=2)\n",
    "    \n",
    "    #Creates a figure and a set of subplots\n",
    "    fig, ax = plt.subplots(figsize = (20, 12))\n",
    "\n",
    "    #set width of bar\n",
    "    barwidth = 0.3\n",
    "\n",
    "    #set position of bar on X axis\n",
    "    pos_train = np.arange(len(f1micro_test))\n",
    "    pos_val = np.arange(len(f1micro_test))+0.3\n",
    "    pos_test = np.arange(len(f1micro_test))+0.6\n",
    "    \n",
    "    #convert to number\n",
    "    f1micro_train = [float(i.split('+')[0]) for i in f1micro_train]\n",
    "    f1micro_val = [float(i.split('+')[0]) for i in f1micro_val]\n",
    "    f1micro_test = [float(i.split('+')[0]) for i in f1micro_test]\n",
    "    \n",
    "    #makes the plot\n",
    "    plt.bar(pos_train, f1micro_train, color= nova_ims_colors[0], width=barwidth, edgecolor='white', label='Train')\n",
    "    plt.bar(pos_val, f1micro_val, color=course_color, width=barwidth, edgecolor='white', label='Validation')\n",
    "    plt.bar(pos_test, f1micro_test, color=student_color, width=barwidth, edgecolor='white', label='Test')\n",
    "    \n",
    "    #sets x, y labels\n",
    "    ax.set(xlabel = 'Model', ylabel = 'Accuracy')\n",
    "\n",
    "    #sets x ticks locations and designation\n",
    "    ax.set_xticks((pos_train+pos_val+pos_test)/3)\n",
    "    ax.set_xticklabels(models, rotation='vertical')\n",
    "    \n",
    "    #ads title to the plot\n",
    "    plt.title(f'10-fold Repeated Cross-Validation Results\\nData: {date}, Target:{target}', fontweight=\"bold\")\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    #removes box to make the plot prettier\n",
    "    plt.box(on=None)\n",
    "\n",
    "    #Creates (pretty) legend\n",
    "    plt.legend(frameon=False)\n",
    "    \n",
    "    plt.savefig(path, transparent=True, dpi=300)\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc3ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_pr(models, X, y, path_roc, path_pr, date, target):  \n",
    "    \n",
    "    sns.set_theme(context='paper', style='whitegrid', font='Calibri', font_scale=2)\n",
    "    # Below for loop iterates through your models list\n",
    "    for m in models:\n",
    "        model = m['model']\n",
    "        y_pred=model.predict(X) # predict the test data\n",
    "    #Compute False postive rate, and True positive rate\n",
    "        fpr, tpr, _ = roc_curve(y, model.predict_proba(X)[:,1])\n",
    "    #Calculate AUC\n",
    "        auc = roc_auc_score(y,model.predict_proba(X)[:,1])\n",
    "    #Plot\n",
    "        plt.plot(fpr, tpr, label='%s ROC (area = %0.4f)' % (m['label'], auc))\n",
    "    #Makes it pretty!\n",
    "    #plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Specificity (False Positive Rate)', fontweight = 'bold')\n",
    "    plt.ylabel('Sensitivity (True Positive Rate)', fontweight = 'bold')\n",
    "    plt.title(f'ROC Curve Test\\nData: {date}, Target:{target}', fontweight = 'bold')\n",
    "    plt.legend(loc=\"lower right\", frameon=False)\n",
    "    #save fig\n",
    "    plt.savefig(path_roc, transparent=True, dpi=300)\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    \n",
    "    # Below for loop iterates through your models list\n",
    "    for m in models:\n",
    "        model = m['model']\n",
    "        y_pred=model.predict(X) # predict the test data\n",
    "    #Compute Precision and Recall\n",
    "        precision, recall, _ = precision_recall_curve(y, model.predict_proba(X)[:,1])\n",
    "    #Calculate AP\n",
    "        ap = average_precision_score(y, model.predict_proba(X)[:,1])\n",
    "    #Plot\n",
    "        plt.plot(recall, precision, label='%s AP (area = %0.4f)' % (m['label'], ap))\n",
    "    #Makes it pretty!\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall (Positive Predictive Value)', fontweight=\"bold\")\n",
    "    plt.ylabel('Precision (True Positive Rate)', fontweight=\"bold\")\n",
    "    plt.title(f'Precision-Recall Curve Test\\nData: {date}, Target:{target}', fontweight=\"bold\")\n",
    "    plt.legend(loc=\"lower left\", frameon=False)\n",
    "    \n",
    "    #save fig\n",
    "    plt.savefig(path_pr, transparent=True, dpi=300)\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70423ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
