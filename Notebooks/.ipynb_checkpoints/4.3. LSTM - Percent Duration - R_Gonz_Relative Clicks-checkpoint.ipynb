{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84eda90e",
   "metadata": {},
   "source": [
    "### Thesis notebook 4.3. - R_Gonz\n",
    "\n",
    "#### LSTM - Temporal data representation\n",
    "\n",
    "In this notebook, we will finally start our application of temporal representation using LSTMs and bi-directional LSTMs.\n",
    "The argument for the usage of Deep Learning stems from the fact that sequences themselves encode information that can be extracted using Recurrent Neural Networks and, more specifically, Long Short Term Memory Units.\n",
    "\n",
    "#### First Step: Setup a PyTorch environment that enables the use of GPU for training. \n",
    "\n",
    "The following cell wll confirm that the GPU will be the default device to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f27844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "cuda.init()\n",
    "## Get Id of default device\n",
    "torch.cuda.current_device()\n",
    "# 0\n",
    "cuda.Device(0).name() # '0' is the id of your GPU\n",
    "\n",
    "#set all tensors to gpu\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95429e",
   "metadata": {},
   "source": [
    "#### Second Step: Import the relevant packages and declare global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c2d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules/libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "#tqdm to monitor progress\n",
    "from tqdm.notebook import tqdm, trange\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "#time related features\n",
    "from datetime import timedelta\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "#vizualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#imblearn, scalers, kfold and metrics\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer,PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, RepeatedStratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, recall_score, classification_report, average_precision_score, precision_recall_curve\n",
    "\n",
    "#import torch related\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "#and optimizer of learning rate\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "#import pytorch modules\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c3f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables that may come in handy\n",
    "#course threshold sets the % duration that will be considered (1 = 100%)\n",
    "duration_threshold = [0.1, 0.25, 0.33, 0.5, 1]\n",
    "\n",
    "#colors for vizualizations\n",
    "nova_ims_colors = ['#BFD72F', '#5C666C']\n",
    "\n",
    "#standard color for student aggregates\n",
    "student_color = '#474838'\n",
    "\n",
    "#standard color for course aggragates\n",
    "course_color = '#1B3D2F'\n",
    "\n",
    "#standard continuous colormap\n",
    "standard_cmap = 'viridis_r'\n",
    "\n",
    "#Function designed to deal with multiindex and flatten it\n",
    "def flattenHierarchicalCol(col,sep = '_'):\n",
    "    '''converts multiindex columns into single index columns while retaining the hierarchical components'''\n",
    "    if not type(col) is tuple:\n",
    "        return col\n",
    "    else:\n",
    "        new_col = ''\n",
    "        for leveli,level in enumerate(col):\n",
    "            if not level == '':\n",
    "                if not leveli == 0:\n",
    "                    new_col += sep\n",
    "                new_col += level\n",
    "        return new_col\n",
    "    \n",
    "#number of replicas - number of repeats of stratified k fold - in this case 10\n",
    "replicas = 30\n",
    "\n",
    "#names to display on result figures\n",
    "date_names = {\n",
    "             'Date_threshold_10': '10% of Course Duration',   \n",
    "             'Date_threshold_25': '25% of Course Duration', \n",
    "             'Date_threshold_33': '33% of Course Duration', \n",
    "             'Date_threshold_50': '50% of Course Duration', \n",
    "             'Date_threshold_100':'100% of Course Duration', \n",
    "            }\n",
    "\n",
    "target_names = {\n",
    "                'exam_fail' : 'At risk - Exam Grade',\n",
    "                'final_fail' : 'At risk - Final Grade', \n",
    "                'exam_gifted' : 'High performer - Exam Grade', \n",
    "                'final_gifted': 'High performer - Final Grade'\n",
    "                }\n",
    "\n",
    "#targets\n",
    "targets = ['final_fail' ,'final_gifted']\n",
    "temporal_columns = ['0 to 4%', '4 to 8%', '8 to 12%', '12 to 16%', '16 to 20%', '20 to 24%',\n",
    "       '24 to 28%', '28 to 32%', '32 to 36%', '36 to 40%', '40 to 44%',\n",
    "       '44 to 48%', '48 to 52%', '52 to 56%', '56 to 60%', '60 to 64%',\n",
    "       '64 to 68%', '68 to 72%', '72 to 76%', '76 to 80%', '80 to 84%',\n",
    "       '84 to 88%', '88 to 92%', '92 to 96%', '96 to 100%']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c5ddb6",
   "metadata": {},
   "source": [
    "#### Step 3: Import data and take a preliminary look at it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8a23ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports dataframes\n",
    "course_programs = pd.read_excel(\"../Data/Modeling Stage/R_Gonz_Temporal_Datasets_25_splits.xlsx\", \n",
    "                                dtype = {\n",
    "                                    'course_encoding' : int,\n",
    "                                    'userid' : int},\n",
    "                               sheet_name = None)\n",
    "\n",
    "#save tables \n",
    "student_list = pd.read_csv('../Data/Modeling Stage/R_Gonz_Filtered_targets.csv', \n",
    "                         dtype = {\n",
    "                                   'course_encoding': int,\n",
    "                                   'userid' : int,\n",
    "                                   })\n",
    "\n",
    "#drop unnamed 0 column\n",
    "for i in course_programs:\n",
    "        \n",
    "    #merge with the targets we calculated on the other \n",
    "    course_programs[i] = course_programs[i].merge(student_list, on = ['course', 'userid'], how = 'inner')\n",
    "    course_programs[i].drop(['Unnamed: 0', 'final_mark'], axis = 1, inplace = True)\n",
    "    \n",
    "    #convert results to object\n",
    "    course_programs[i]['course'], course_programs[i]['userid'] = course_programs[i]['course'].astype(object), course_programs[i]['userid'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc3c84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13857 entries, 0 to 13856\n",
      "Data columns (total 29 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   course        13857 non-null  object\n",
      " 1   userid        13857 non-null  object\n",
      " 2   1             13857 non-null  int64 \n",
      " 3   2             13857 non-null  int64 \n",
      " 4   3             13857 non-null  int64 \n",
      " 5   4             13857 non-null  int64 \n",
      " 6   5             13857 non-null  int64 \n",
      " 7   6             13857 non-null  int64 \n",
      " 8   7             13857 non-null  int64 \n",
      " 9   8             13857 non-null  int64 \n",
      " 10  9             13857 non-null  int64 \n",
      " 11  10            13857 non-null  int64 \n",
      " 12  11            13857 non-null  int64 \n",
      " 13  12            13857 non-null  int64 \n",
      " 14  13            13857 non-null  int64 \n",
      " 15  14            13857 non-null  int64 \n",
      " 16  15            13857 non-null  int64 \n",
      " 17  16            13857 non-null  int64 \n",
      " 18  17            13857 non-null  int64 \n",
      " 19  18            13857 non-null  int64 \n",
      " 20  19            13857 non-null  int64 \n",
      " 21  20            13857 non-null  int64 \n",
      " 22  21            13857 non-null  int64 \n",
      " 23  22            13857 non-null  int64 \n",
      " 24  23            13857 non-null  int64 \n",
      " 25  24            13857 non-null  int64 \n",
      " 26  25            13857 non-null  int64 \n",
      " 27  final_fail    13857 non-null  int64 \n",
      " 28  final_gifted  13857 non-null  int64 \n",
      "dtypes: int64(27), object(2)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "course_programs['Date_threshold_100'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a751ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course</th>\n",
       "      <th>userid</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>final_fail</th>\n",
       "      <th>final_gifted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13857.0</td>\n",
       "      <td>13857.0</td>\n",
       "      <td>13857.000000</td>\n",
       "      <td>13857.000000</td>\n",
       "      <td>13857.000000</td>\n",
       "      <td>13857.000000</td>\n",
       "      <td>13857.000000</td>\n",
       "      <td>13857.000000</td>\n",
       "      <td>13857.000000</td>\n",
       "      <td>13857.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>13857.000000</td>\n",
       "      <td>13857.000000</td>\n",
       "      <td>13857.000000</td>\n",
       "      <td>13857.000000</td>\n",
       "      <td>13857.000000</td>\n",
       "      <td>13857.000000</td>\n",
       "      <td>13857.000000</td>\n",
       "      <td>13857.000000</td>\n",
       "      <td>13857.000000</td>\n",
       "      <td>13857.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>174.0</td>\n",
       "      <td>8544.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2059.0</td>\n",
       "      <td>68888.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>507.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.601140</td>\n",
       "      <td>4.616584</td>\n",
       "      <td>7.876092</td>\n",
       "      <td>8.510067</td>\n",
       "      <td>9.804792</td>\n",
       "      <td>10.839431</td>\n",
       "      <td>11.184167</td>\n",
       "      <td>12.273147</td>\n",
       "      <td>...</td>\n",
       "      <td>11.521036</td>\n",
       "      <td>11.677997</td>\n",
       "      <td>8.524067</td>\n",
       "      <td>10.015155</td>\n",
       "      <td>8.560583</td>\n",
       "      <td>7.720935</td>\n",
       "      <td>3.454355</td>\n",
       "      <td>0.082413</td>\n",
       "      <td>0.381035</td>\n",
       "      <td>0.198528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.581259</td>\n",
       "      <td>12.238187</td>\n",
       "      <td>15.785656</td>\n",
       "      <td>14.600375</td>\n",
       "      <td>16.021089</td>\n",
       "      <td>16.473371</td>\n",
       "      <td>20.043011</td>\n",
       "      <td>20.126765</td>\n",
       "      <td>...</td>\n",
       "      <td>22.043869</td>\n",
       "      <td>27.925613</td>\n",
       "      <td>18.816024</td>\n",
       "      <td>29.534244</td>\n",
       "      <td>20.248598</td>\n",
       "      <td>20.105366</td>\n",
       "      <td>14.589819</td>\n",
       "      <td>1.264520</td>\n",
       "      <td>0.485659</td>\n",
       "      <td>0.398906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>236.000000</td>\n",
       "      <td>292.000000</td>\n",
       "      <td>239.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>219.000000</td>\n",
       "      <td>880.000000</td>\n",
       "      <td>602.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>557.000000</td>\n",
       "      <td>729.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>1316.000000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>422.000000</td>\n",
       "      <td>523.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         course   userid             1             2             3  \\\n",
       "count   13857.0  13857.0  13857.000000  13857.000000  13857.000000   \n",
       "unique    174.0   8544.0           NaN           NaN           NaN   \n",
       "top      2059.0  68888.0           NaN           NaN           NaN   \n",
       "freq      507.0      7.0           NaN           NaN           NaN   \n",
       "mean        NaN      NaN      1.601140      4.616584      7.876092   \n",
       "std         NaN      NaN      6.581259     12.238187     15.785656   \n",
       "min         NaN      NaN      0.000000      0.000000      0.000000   \n",
       "25%         NaN      NaN      0.000000      0.000000      0.000000   \n",
       "50%         NaN      NaN      0.000000      0.000000      1.000000   \n",
       "75%         NaN      NaN      0.000000      4.000000     10.000000   \n",
       "max         NaN      NaN    178.000000    236.000000    292.000000   \n",
       "\n",
       "                   4             5             6             7             8  \\\n",
       "count   13857.000000  13857.000000  13857.000000  13857.000000  13857.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean        8.510067      9.804792     10.839431     11.184167     12.273147   \n",
       "std        14.600375     16.021089     16.473371     20.043011     20.126765   \n",
       "min         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%         3.000000      4.000000      4.000000      5.000000      5.000000   \n",
       "75%        11.000000     13.000000     15.000000     15.000000     17.000000   \n",
       "max       239.000000    255.000000    219.000000    880.000000    602.000000   \n",
       "\n",
       "        ...            18            19            20            21  \\\n",
       "count   ...  13857.000000  13857.000000  13857.000000  13857.000000   \n",
       "unique  ...           NaN           NaN           NaN           NaN   \n",
       "top     ...           NaN           NaN           NaN           NaN   \n",
       "freq    ...           NaN           NaN           NaN           NaN   \n",
       "mean    ...     11.521036     11.677997      8.524067     10.015155   \n",
       "std     ...     22.043869     27.925613     18.816024     29.534244   \n",
       "min     ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%     ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%     ...      4.000000      3.000000      1.000000      1.000000   \n",
       "75%     ...     15.000000     13.000000     10.000000     11.000000   \n",
       "max     ...    557.000000    729.000000    678.000000   1316.000000   \n",
       "\n",
       "                  22            23            24            25    final_fail  \\\n",
       "count   13857.000000  13857.000000  13857.000000  13857.000000  13857.000000   \n",
       "unique           NaN           NaN           NaN           NaN           NaN   \n",
       "top              NaN           NaN           NaN           NaN           NaN   \n",
       "freq             NaN           NaN           NaN           NaN           NaN   \n",
       "mean        8.560583      7.720935      3.454355      0.082413      0.381035   \n",
       "std        20.248598     20.105366     14.589819      1.264520      0.485659   \n",
       "min         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%         1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%         9.000000      7.000000      1.000000      0.000000      1.000000   \n",
       "max       407.000000    422.000000    523.000000     74.000000      1.000000   \n",
       "\n",
       "        final_gifted  \n",
       "count   13857.000000  \n",
       "unique           NaN  \n",
       "top              NaN  \n",
       "freq             NaN  \n",
       "mean        0.198528  \n",
       "std         0.398906  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[11 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_programs['Date_threshold_100'].describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a150610b",
   "metadata": {},
   "source": [
    "In our second attempt, we are looking to obtain a different result. Instead of using the absolute number of clicks used in each instance, we are instead looking to use the percent number of clicks made by each student relative to the the total number of clicks performed in the curricular unit.\n",
    "\n",
    "For that we will use transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa5e45f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893ad6953c48471fb54bf996ad295d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38fb243402446d79ec67c92346182cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63ebe2d06cd481cb1fc2d8d619658c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955da2cdc5294efebc95681b0e06e3f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d3941661724913ac040cb476a82a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f768d1069b69401eb1b051026653f922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(course_programs.keys()):\n",
    "    \n",
    "    for j in tqdm(range(1,25)):\n",
    "            course_programs[i][j] = np.where(course_programs[i].fillna(0).groupby('course')[j].transform('sum') != 0, #where valid operations occur\n",
    "                                             course_programs[i][j].fillna(0) / course_programs[i].fillna(0).groupby('course')[j].transform('sum') * 100, #calculate percentage\n",
    "                                             0) #otherwise, its 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3291817",
   "metadata": {},
   "source": [
    "In our first attempt, we will use the absolute number of clicks made by each student - scaled using standard scaler. \n",
    "Therefore, we can start by immediately placing our course encoding/userid pairings into the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be722ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train, test, scaler):\n",
    "    \n",
    "    if scaler == 'MinMax':\n",
    "        pt = MinMaxScaler()\n",
    "    elif scaler == 'Standard':\n",
    "        pt = StandardScaler()\n",
    "    elif scaler == 'Robust':\n",
    "        pt = RobustScaler()\n",
    "    elif scaler == 'Quantile':\n",
    "        pt = QuantileTransformer()\n",
    "    else:\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "    \n",
    "    data_train = pt.fit_transform(train)\n",
    "    data_test = pt.transform(test)\n",
    "    # convert the array back to a dataframe\n",
    "    normalized_train = pd.DataFrame(data_train,columns=train.columns)\n",
    "    normalized_test = pd.DataFrame(data_test,columns=test.columns)\n",
    "        \n",
    "    return normalized_train, normalized_test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4d5475",
   "metadata": {},
   "source": [
    "#### Implementing Cross-Validation with Deep Learning Model\n",
    "\n",
    "**1. Create the Deep Learning Model**\n",
    "\n",
    "In this instance, we will follow-up with on the approach used in Chen & Cui - CrossEntropyLoss with applied over a softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a16bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Uni(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM_Uni, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first = True) #lstm\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "    \n",
    "        self.fc = nn.Linear(self.hidden_size, num_classes) #fully connected last layer\n",
    "\n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        \n",
    "        #Xavier_init for both H_0 and C_0\n",
    "        torch.nn.init.xavier_normal_(h_0)\n",
    "        torch.nn.init.xavier_normal_(c_0)\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        lstm_out, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        last_output = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        \n",
    "        #we are interested in only keeping the last output\n",
    "        drop_out = self.dropout(last_output)\n",
    "        pre_softmax = self.fc(drop_out) #Final Output - dense\n",
    "        return pre_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c356bd",
   "metadata": {},
   "source": [
    "**2. Define the train and validation Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b29a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,dataloader,loss_fn,optimizer):\n",
    "    \n",
    "    train_loss,train_correct=0.0,0 \n",
    "    model.train()\n",
    "    for X, labels in dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = loss_fn(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * X.size(0)\n",
    "        scores, predictions = torch.max(F.log_softmax(output.data), 1)\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "    return train_loss,train_correct\n",
    "  \n",
    "def valid_epoch(model,dataloader,loss_fn):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    targets = []\n",
    "    y_pred = []\n",
    "    probability_1 = []\n",
    "    \n",
    "    model.eval()\n",
    "    for X, labels in dataloader:\n",
    "\n",
    "        output = model(X)\n",
    "        loss=loss_fn(output,labels)\n",
    "        valid_loss+=loss.item()*X.size(0)\n",
    "        probability_1.append(F.softmax(output.data)[:,1])\n",
    "        predictions = torch.argmax(output, dim=1)\n",
    "        val_correct+=(predictions == labels).sum().item()\n",
    "        targets.append(labels)\n",
    "        y_pred.append(predictions)\n",
    "    \n",
    "    #concat all results\n",
    "    targets = torch.cat(targets).data.cpu().numpy()\n",
    "    y_pred = torch.cat(y_pred).data.cpu().numpy()\n",
    "    probability_1 = torch.cat(probability_1).data.cpu().numpy()\n",
    "    \n",
    "    #calculate precision, recall and AUC score\n",
    "    \n",
    "    precision = precision_score(targets, y_pred)\n",
    "    recall = recall_score(targets, y_pred)\n",
    "    auroc = roc_auc_score(targets, probability_1)\n",
    "    \n",
    "    #return all\n",
    "    return valid_loss,val_correct, precision, recall, auroc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4543fb3",
   "metadata": {},
   "source": [
    "**3. Define main hyperparameters of the model, including splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcbbef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "num_epochs = 200 #50 epochs\n",
    "learning_rate = 0.01 #0.001 lr\n",
    "input_size = 1 #number of features\n",
    "hidden_size = 40 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "#Shape of Output as required for SoftMax Classifier\n",
    "num_classes = 2 #output shape\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "k=10\n",
    "splits= RepeatedStratifiedKFold(n_splits=k, n_repeats=replicas, random_state=15) #kfold of 10 with 30 replicas\n",
    "criterion = nn.CrossEntropyLoss()    # cross-entropy for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20713d9",
   "metadata": {},
   "source": [
    "**4. Make the splits and Start Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9af744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84082dd565034ddfb1d70ee2183dc271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date_threshold_33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621e47bea9d049ac8af4934c572d2d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_fail\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c614fda45b0a49c38739e9587f068a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8193e6c21c97452a8fd23f1706cebe92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best Accuracy found: 61.95%\n",
      "Epoch: 1\n",
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.95 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.93 % AVG Validation Acc 61.95 %\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.655 AVG Validation Loss:0.668 AVG Training Acc 61.94 % AVG Validation Acc 61.95 %\n",
      "Epoch:70/200 AVG Training Loss:0.654 AVG Validation Loss:0.667 AVG Training Acc 61.98 % AVG Validation Acc 61.95 %\n",
      "Epoch:80/200 AVG Training Loss:0.653 AVG Validation Loss:0.668 AVG Training Acc 61.96 % AVG Validation Acc 61.95 %\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.653 AVG Validation Loss:0.668 AVG Training Acc 62.01 % AVG Validation Acc 61.95 %\n",
      "Epoch:100/200 AVG Training Loss:0.653 AVG Validation Loss:0.668 AVG Training Acc 61.97 % AVG Validation Acc 61.95 %\n",
      "Epoch:110/200 AVG Training Loss:0.653 AVG Validation Loss:0.668 AVG Training Acc 62.00 % AVG Validation Acc 61.95 %\n",
      "Epoch   118: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.653 AVG Validation Loss:0.668 AVG Training Acc 62.00 % AVG Validation Acc 61.95 %\n",
      "Epoch:130/200 AVG Training Loss:0.654 AVG Validation Loss:0.668 AVG Training Acc 61.98 % AVG Validation Acc 61.95 %\n",
      "Epoch:140/200 AVG Training Loss:0.654 AVG Validation Loss:0.668 AVG Training Acc 61.98 % AVG Validation Acc 61.95 %\n",
      "Epoch   149: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.653 AVG Validation Loss:0.668 AVG Training Acc 61.98 % AVG Validation Acc 61.95 %\n",
      "Epoch:160/200 AVG Training Loss:0.653 AVG Validation Loss:0.668 AVG Training Acc 61.96 % AVG Validation Acc 61.95 %\n",
      "Epoch:170/200 AVG Training Loss:0.654 AVG Validation Loss:0.668 AVG Training Acc 61.95 % AVG Validation Acc 61.95 %\n",
      "Epoch:180/200 AVG Training Loss:0.654 AVG Validation Loss:0.668 AVG Training Acc 62.00 % AVG Validation Acc 61.95 %\n",
      "Epoch   180: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.654 AVG Validation Loss:0.668 AVG Training Acc 61.99 % AVG Validation Acc 61.95 %\n",
      "Epoch:200/200 AVG Training Loss:0.654 AVG Validation Loss:0.668 AVG Training Acc 61.96 % AVG Validation Acc 61.95 %\n",
      "Split 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5683297279654ed2831f495d08962008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.661 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.661 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.659 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.660 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:50/200 AVG Training Loss:0.659 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.658 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:70/200 AVG Training Loss:0.658 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:80/200 AVG Training Loss:0.658 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    88: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.658 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:100/200 AVG Training Loss:0.658 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:110/200 AVG Training Loss:0.658 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch   119: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.658 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:130/200 AVG Training Loss:0.658 AVG Validation Loss:0.658 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:140/200 AVG Training Loss:0.658 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:150/200 AVG Training Loss:0.658 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch   150: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.658 AVG Validation Loss:0.658 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:170/200 AVG Training Loss:0.658 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:180/200 AVG Training Loss:0.658 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.658 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:200/200 AVG Training Loss:0.658 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Split 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a171971910a4403aa7e2262cc1f814c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.660 AVG Validation Loss:0.659 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:30/200 AVG Training Loss:0.658 AVG Validation Loss:0.659 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.96 % AVG Validation Acc 61.59 %\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.662 AVG Training Acc 61.97 % AVG Validation Acc 61.68 %\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.655 AVG Validation Loss:0.664 AVG Training Acc 61.94 % AVG Validation Acc 61.77 %\n",
      "Epoch:70/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 61.97 % AVG Validation Acc 61.77 %\n",
      "Epoch:80/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 62.05 % AVG Validation Acc 61.77 %\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.653 AVG Validation Loss:0.666 AVG Training Acc 61.99 % AVG Validation Acc 61.77 %\n",
      "Epoch:100/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 62.05 % AVG Validation Acc 61.86 %\n",
      "Epoch:110/200 AVG Training Loss:0.654 AVG Validation Loss:0.667 AVG Training Acc 62.05 % AVG Validation Acc 61.86 %\n",
      "Epoch   112: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.654 AVG Validation Loss:0.666 AVG Training Acc 62.00 % AVG Validation Acc 61.86 %\n",
      "Epoch:130/200 AVG Training Loss:0.654 AVG Validation Loss:0.667 AVG Training Acc 62.03 % AVG Validation Acc 61.77 %\n",
      "Epoch:140/200 AVG Training Loss:0.654 AVG Validation Loss:0.666 AVG Training Acc 62.09 % AVG Validation Acc 61.68 %\n",
      "Epoch   143: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 61.94 % AVG Validation Acc 61.86 %\n",
      "Epoch:160/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 61.96 % AVG Validation Acc 61.86 %\n",
      "Epoch:170/200 AVG Training Loss:0.654 AVG Validation Loss:0.666 AVG Training Acc 62.00 % AVG Validation Acc 61.77 %\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.654 AVG Validation Loss:0.666 AVG Training Acc 62.02 % AVG Validation Acc 61.77 %\n",
      "Epoch:190/200 AVG Training Loss:0.654 AVG Validation Loss:0.667 AVG Training Acc 61.97 % AVG Validation Acc 61.77 %\n",
      "Epoch:200/200 AVG Training Loss:0.654 AVG Validation Loss:0.666 AVG Training Acc 62.00 % AVG Validation Acc 61.86 %\n",
      "Split 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6220173aa644310986946ad8055c151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.666 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.666 AVG Training Acc 61.78 % AVG Validation Acc 61.86 %\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.664 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.664 AVG Training Acc 61.88 % AVG Validation Acc 61.77 %\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.664 AVG Training Acc 61.90 % AVG Validation Acc 61.77 %\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.664 AVG Training Acc 61.94 % AVG Validation Acc 61.77 %\n",
      "Epoch:70/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.92 % AVG Validation Acc 61.77 %\n",
      "Epoch:80/200 AVG Training Loss:0.656 AVG Validation Loss:0.664 AVG Training Acc 61.99 % AVG Validation Acc 61.86 %\n",
      "Epoch:90/200 AVG Training Loss:0.656 AVG Validation Loss:0.664 AVG Training Acc 61.91 % AVG Validation Acc 61.77 %\n",
      "Epoch    90: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.656 AVG Validation Loss:0.664 AVG Training Acc 61.96 % AVG Validation Acc 61.86 %\n",
      "Epoch:110/200 AVG Training Loss:0.655 AVG Validation Loss:0.664 AVG Training Acc 61.96 % AVG Validation Acc 61.86 %\n",
      "Epoch:120/200 AVG Training Loss:0.656 AVG Validation Loss:0.664 AVG Training Acc 61.90 % AVG Validation Acc 61.77 %\n",
      "Epoch   121: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.655 AVG Validation Loss:0.664 AVG Training Acc 61.97 % AVG Validation Acc 61.86 %\n",
      "Epoch:140/200 AVG Training Loss:0.656 AVG Validation Loss:0.664 AVG Training Acc 61.95 % AVG Validation Acc 61.77 %\n",
      "Epoch:150/200 AVG Training Loss:0.656 AVG Validation Loss:0.664 AVG Training Acc 61.99 % AVG Validation Acc 61.77 %\n",
      "Epoch   152: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.655 AVG Validation Loss:0.664 AVG Training Acc 62.01 % AVG Validation Acc 61.77 %\n",
      "Epoch:170/200 AVG Training Loss:0.655 AVG Validation Loss:0.664 AVG Training Acc 62.00 % AVG Validation Acc 61.77 %\n",
      "Epoch:180/200 AVG Training Loss:0.656 AVG Validation Loss:0.664 AVG Training Acc 61.96 % AVG Validation Acc 61.77 %\n",
      "Epoch   183: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.655 AVG Validation Loss:0.664 AVG Training Acc 62.01 % AVG Validation Acc 61.77 %\n",
      "Epoch:200/200 AVG Training Loss:0.655 AVG Validation Loss:0.664 AVG Training Acc 61.92 % AVG Validation Acc 61.86 %\n",
      "Split 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08556df255a3437ba8b68707ee8a1890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.661 AVG Validation Loss:0.665 AVG Training Acc 61.86 % AVG Validation Acc 61.86 %\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.659 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:30/200 AVG Training Loss:0.658 AVG Validation Loss:0.666 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.667 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/200 AVG Training Loss:0.657 AVG Validation Loss:0.668 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.668 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch:70/200 AVG Training Loss:0.656 AVG Validation Loss:0.668 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:80/200 AVG Training Loss:0.656 AVG Validation Loss:0.668 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:90/200 AVG Training Loss:0.656 AVG Validation Loss:0.668 AVG Training Acc 61.94 % AVG Validation Acc 61.86 %\n",
      "Epoch:100/200 AVG Training Loss:0.656 AVG Validation Loss:0.668 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch   108: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:110/200 AVG Training Loss:0.656 AVG Validation Loss:0.668 AVG Training Acc 61.88 % AVG Validation Acc 61.86 %\n",
      "Epoch:120/200 AVG Training Loss:0.656 AVG Validation Loss:0.668 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch:130/200 AVG Training Loss:0.656 AVG Validation Loss:0.668 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch   139: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:140/200 AVG Training Loss:0.657 AVG Validation Loss:0.668 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:150/200 AVG Training Loss:0.656 AVG Validation Loss:0.668 AVG Training Acc 61.92 % AVG Validation Acc 61.86 %\n",
      "Epoch:160/200 AVG Training Loss:0.656 AVG Validation Loss:0.668 AVG Training Acc 61.92 % AVG Validation Acc 61.86 %\n",
      "Epoch:170/200 AVG Training Loss:0.657 AVG Validation Loss:0.668 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch   170: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.656 AVG Validation Loss:0.668 AVG Training Acc 61.86 % AVG Validation Acc 61.86 %\n",
      "Epoch:190/200 AVG Training Loss:0.656 AVG Validation Loss:0.668 AVG Training Acc 61.87 % AVG Validation Acc 61.86 %\n",
      "Epoch:200/200 AVG Training Loss:0.656 AVG Validation Loss:0.668 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Split 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e524b9982aae4a608a66f9b1328d43bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.663 AVG Validation Loss:0.661 AVG Training Acc 61.82 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.658 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.659 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch:60/200 AVG Training Loss:0.657 AVG Validation Loss:0.658 AVG Training Acc 61.93 % AVG Validation Acc 61.91 %\n",
      "Epoch:70/200 AVG Training Loss:0.655 AVG Validation Loss:0.656 AVG Training Acc 61.96 % AVG Validation Acc 61.82 %\n",
      "Epoch:80/200 AVG Training Loss:0.654 AVG Validation Loss:0.658 AVG Training Acc 61.98 % AVG Validation Acc 61.55 %\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:90/200 AVG Training Loss:0.652 AVG Validation Loss:0.658 AVG Training Acc 62.08 % AVG Validation Acc 61.55 %\n",
      "Epoch:100/200 AVG Training Loss:0.651 AVG Validation Loss:0.658 AVG Training Acc 62.11 % AVG Validation Acc 61.55 %\n",
      "Epoch:110/200 AVG Training Loss:0.652 AVG Validation Loss:0.657 AVG Training Acc 62.06 % AVG Validation Acc 61.55 %\n",
      "Epoch   112: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:120/200 AVG Training Loss:0.652 AVG Validation Loss:0.658 AVG Training Acc 62.06 % AVG Validation Acc 61.64 %\n",
      "Epoch:130/200 AVG Training Loss:0.652 AVG Validation Loss:0.657 AVG Training Acc 62.05 % AVG Validation Acc 61.64 %\n",
      "Epoch:140/200 AVG Training Loss:0.651 AVG Validation Loss:0.658 AVG Training Acc 61.99 % AVG Validation Acc 61.64 %\n",
      "Epoch   143: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:150/200 AVG Training Loss:0.651 AVG Validation Loss:0.658 AVG Training Acc 62.08 % AVG Validation Acc 61.73 %\n",
      "Epoch:160/200 AVG Training Loss:0.652 AVG Validation Loss:0.657 AVG Training Acc 62.04 % AVG Validation Acc 61.73 %\n",
      "Epoch:170/200 AVG Training Loss:0.651 AVG Validation Loss:0.657 AVG Training Acc 62.26 % AVG Validation Acc 61.64 %\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:180/200 AVG Training Loss:0.651 AVG Validation Loss:0.658 AVG Training Acc 62.08 % AVG Validation Acc 61.73 %\n",
      "Epoch:190/200 AVG Training Loss:0.652 AVG Validation Loss:0.657 AVG Training Acc 62.04 % AVG Validation Acc 61.73 %\n",
      "Epoch:200/200 AVG Training Loss:0.651 AVG Validation Loss:0.658 AVG Training Acc 62.06 % AVG Validation Acc 61.64 %\n",
      "Split 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaeb5848b4b54e7f97a460882d384107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.658 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.658 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.659 AVG Validation Loss:0.658 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:60/200 AVG Training Loss:0.658 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:70/200 AVG Training Loss:0.658 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.657 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:90/200 AVG Training Loss:0.657 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:100/200 AVG Training Loss:0.658 AVG Validation Loss:0.657 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch   104: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:110/200 AVG Training Loss:0.657 AVG Validation Loss:0.656 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:120/200 AVG Training Loss:0.657 AVG Validation Loss:0.657 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch:130/200 AVG Training Loss:0.657 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch   135: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:140/200 AVG Training Loss:0.657 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:150/200 AVG Training Loss:0.656 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:160/200 AVG Training Loss:0.657 AVG Validation Loss:0.657 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch   166: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:170/200 AVG Training Loss:0.656 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:180/200 AVG Training Loss:0.657 AVG Validation Loss:0.657 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch:190/200 AVG Training Loss:0.656 AVG Validation Loss:0.657 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch   197: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:200/200 AVG Training Loss:0.657 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Split 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6288775dbbee4a83952e5fdec9292662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.658 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.656 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.659 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.657 AVG Validation Loss:0.659 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.661 AVG Training Acc 61.96 % AVG Validation Acc 61.91 %\n",
      "Epoch:60/200 AVG Training Loss:0.655 AVG Validation Loss:0.663 AVG Training Acc 62.00 % AVG Validation Acc 61.64 %\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 61.98 % AVG Validation Acc 61.73 %\n",
      "Epoch:80/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.09 % AVG Validation Acc 61.73 %\n",
      "Epoch:90/200 AVG Training Loss:0.655 AVG Validation Loss:0.663 AVG Training Acc 62.02 % AVG Validation Acc 61.73 %\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.03 % AVG Validation Acc 61.64 %\n",
      "Epoch:110/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 61.92 % AVG Validation Acc 61.64 %\n",
      "Epoch:120/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 61.99 % AVG Validation Acc 61.64 %\n",
      "Epoch   122: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.03 % AVG Validation Acc 61.64 %\n",
      "Epoch:140/200 AVG Training Loss:0.653 AVG Validation Loss:0.664 AVG Training Acc 62.03 % AVG Validation Acc 61.64 %\n",
      "Epoch:150/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.06 % AVG Validation Acc 61.73 %\n",
      "Epoch   153: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.08 % AVG Validation Acc 61.64 %\n",
      "Epoch:170/200 AVG Training Loss:0.653 AVG Validation Loss:0.664 AVG Training Acc 62.01 % AVG Validation Acc 61.64 %\n",
      "Epoch:180/200 AVG Training Loss:0.653 AVG Validation Loss:0.664 AVG Training Acc 62.01 % AVG Validation Acc 61.73 %\n",
      "Epoch   184: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.653 AVG Validation Loss:0.664 AVG Training Acc 62.12 % AVG Validation Acc 61.73 %\n",
      "Epoch:200/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.01 % AVG Validation Acc 61.64 %\n",
      "Split 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e70832f82b46f7ae4dd4ad66b74ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.661 AVG Validation Loss:0.666 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.658 AVG Validation Loss:0.669 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.657 AVG Validation Loss:0.669 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.654 AVG Validation Loss:0.673 AVG Training Acc 61.96 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.653 AVG Validation Loss:0.673 AVG Training Acc 61.91 % AVG Validation Acc 62.00 %\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "New Best Accuracy found: 62.00%\n",
      "Epoch: 50\n",
      "Epoch:60/200 AVG Training Loss:0.651 AVG Validation Loss:0.673 AVG Training Acc 62.09 % AVG Validation Acc 61.82 %\n",
      "Epoch:70/200 AVG Training Loss:0.651 AVG Validation Loss:0.673 AVG Training Acc 62.04 % AVG Validation Acc 61.91 %\n",
      "Epoch:80/200 AVG Training Loss:0.651 AVG Validation Loss:0.675 AVG Training Acc 62.21 % AVG Validation Acc 61.64 %\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.651 AVG Validation Loss:0.676 AVG Training Acc 61.98 % AVG Validation Acc 61.73 %\n",
      "Epoch:100/200 AVG Training Loss:0.650 AVG Validation Loss:0.676 AVG Training Acc 62.26 % AVG Validation Acc 61.46 %\n",
      "Epoch:110/200 AVG Training Loss:0.650 AVG Validation Loss:0.674 AVG Training Acc 62.30 % AVG Validation Acc 61.64 %\n",
      "Epoch   112: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.651 AVG Validation Loss:0.676 AVG Training Acc 62.03 % AVG Validation Acc 61.46 %\n",
      "Epoch:130/200 AVG Training Loss:0.650 AVG Validation Loss:0.676 AVG Training Acc 61.98 % AVG Validation Acc 61.46 %\n",
      "Epoch:140/200 AVG Training Loss:0.650 AVG Validation Loss:0.675 AVG Training Acc 62.18 % AVG Validation Acc 61.55 %\n",
      "Epoch   143: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.649 AVG Validation Loss:0.675 AVG Training Acc 62.33 % AVG Validation Acc 61.55 %\n",
      "Epoch:160/200 AVG Training Loss:0.650 AVG Validation Loss:0.675 AVG Training Acc 62.14 % AVG Validation Acc 61.46 %\n",
      "Epoch:170/200 AVG Training Loss:0.650 AVG Validation Loss:0.676 AVG Training Acc 62.13 % AVG Validation Acc 61.55 %\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.649 AVG Validation Loss:0.675 AVG Training Acc 62.26 % AVG Validation Acc 61.46 %\n",
      "Epoch:190/200 AVG Training Loss:0.650 AVG Validation Loss:0.675 AVG Training Acc 62.05 % AVG Validation Acc 61.46 %\n",
      "Epoch:200/200 AVG Training Loss:0.650 AVG Validation Loss:0.674 AVG Training Acc 62.10 % AVG Validation Acc 61.46 %\n",
      "Split 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69b9ff87fde4d39a09b3beade99bc4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.658 AVG Validation Loss:0.659 AVG Training Acc 61.92 % AVG Validation Acc 61.82 %\n",
      "Epoch:40/200 AVG Training Loss:0.656 AVG Validation Loss:0.661 AVG Training Acc 61.93 % AVG Validation Acc 61.82 %\n",
      "Epoch:50/200 AVG Training Loss:0.654 AVG Validation Loss:0.665 AVG Training Acc 62.06 % AVG Validation Acc 61.46 %\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.650 AVG Validation Loss:0.668 AVG Training Acc 62.23 % AVG Validation Acc 61.01 %\n",
      "Epoch:70/200 AVG Training Loss:0.650 AVG Validation Loss:0.668 AVG Training Acc 62.28 % AVG Validation Acc 61.19 %\n",
      "Epoch:80/200 AVG Training Loss:0.650 AVG Validation Loss:0.668 AVG Training Acc 62.30 % AVG Validation Acc 61.01 %\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.649 AVG Validation Loss:0.668 AVG Training Acc 62.39 % AVG Validation Acc 61.01 %\n",
      "Epoch:100/200 AVG Training Loss:0.649 AVG Validation Loss:0.669 AVG Training Acc 62.39 % AVG Validation Acc 60.92 %\n",
      "Epoch:110/200 AVG Training Loss:0.649 AVG Validation Loss:0.669 AVG Training Acc 62.35 % AVG Validation Acc 61.01 %\n",
      "Epoch:120/200 AVG Training Loss:0.649 AVG Validation Loss:0.669 AVG Training Acc 62.33 % AVG Validation Acc 60.92 %\n",
      "Epoch   120: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.649 AVG Validation Loss:0.669 AVG Training Acc 62.38 % AVG Validation Acc 60.74 %\n",
      "Epoch:140/200 AVG Training Loss:0.649 AVG Validation Loss:0.670 AVG Training Acc 62.39 % AVG Validation Acc 60.83 %\n",
      "Epoch:150/200 AVG Training Loss:0.650 AVG Validation Loss:0.669 AVG Training Acc 62.26 % AVG Validation Acc 61.01 %\n",
      "Epoch   151: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.650 AVG Validation Loss:0.669 AVG Training Acc 62.27 % AVG Validation Acc 60.92 %\n",
      "Epoch:170/200 AVG Training Loss:0.648 AVG Validation Loss:0.669 AVG Training Acc 62.39 % AVG Validation Acc 60.92 %\n",
      "Epoch:180/200 AVG Training Loss:0.649 AVG Validation Loss:0.669 AVG Training Acc 62.35 % AVG Validation Acc 61.01 %\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.649 AVG Validation Loss:0.669 AVG Training Acc 62.29 % AVG Validation Acc 60.83 %\n",
      "Epoch:200/200 AVG Training Loss:0.649 AVG Validation Loss:0.669 AVG Training Acc 62.25 % AVG Validation Acc 60.83 %\n",
      "Split 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575827cfee7542c89f44607477bc38d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.663 AVG Training Acc 61.87 % AVG Validation Acc 61.95 %\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.664 AVG Training Acc 61.98 % AVG Validation Acc 61.95 %\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.655 AVG Validation Loss:0.664 AVG Training Acc 61.94 % AVG Validation Acc 61.86 %\n",
      "Epoch:70/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 61.98 % AVG Validation Acc 61.77 %\n",
      "Epoch:80/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 61.99 % AVG Validation Acc 61.77 %\n",
      "Epoch    88: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 61.87 % AVG Validation Acc 61.77 %\n",
      "Epoch:100/200 AVG Training Loss:0.654 AVG Validation Loss:0.665 AVG Training Acc 62.05 % AVG Validation Acc 61.77 %\n",
      "Epoch:110/200 AVG Training Loss:0.654 AVG Validation Loss:0.666 AVG Training Acc 62.02 % AVG Validation Acc 61.77 %\n",
      "Epoch   119: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 62.03 % AVG Validation Acc 61.77 %\n",
      "Epoch:130/200 AVG Training Loss:0.654 AVG Validation Loss:0.666 AVG Training Acc 61.99 % AVG Validation Acc 61.77 %\n",
      "Epoch:140/200 AVG Training Loss:0.654 AVG Validation Loss:0.665 AVG Training Acc 62.02 % AVG Validation Acc 61.77 %\n",
      "Epoch:150/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 62.01 % AVG Validation Acc 61.77 %\n",
      "Epoch   150: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.654 AVG Validation Loss:0.666 AVG Training Acc 62.12 % AVG Validation Acc 61.77 %\n",
      "Epoch:170/200 AVG Training Loss:0.654 AVG Validation Loss:0.666 AVG Training Acc 61.99 % AVG Validation Acc 61.77 %\n",
      "Epoch:180/200 AVG Training Loss:0.654 AVG Validation Loss:0.666 AVG Training Acc 61.99 % AVG Validation Acc 61.77 %\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.654 AVG Validation Loss:0.666 AVG Training Acc 62.01 % AVG Validation Acc 61.77 %\n",
      "Epoch:200/200 AVG Training Loss:0.654 AVG Validation Loss:0.665 AVG Training Acc 62.02 % AVG Validation Acc 61.77 %\n",
      "Split 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89ea7452f284c01a2eac22afaf938c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:30/200 AVG Training Loss:0.661 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.661 AVG Training Acc 61.83 % AVG Validation Acc 61.86 %\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:50/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:60/200 AVG Training Loss:0.653 AVG Validation Loss:0.664 AVG Training Acc 61.96 % AVG Validation Acc 61.68 %\n",
      "Epoch:70/200 AVG Training Loss:0.650 AVG Validation Loss:0.665 AVG Training Acc 62.04 % AVG Validation Acc 61.77 %\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.647 AVG Validation Loss:0.674 AVG Training Acc 62.06 % AVG Validation Acc 61.41 %\n",
      "Epoch:90/200 AVG Training Loss:0.647 AVG Validation Loss:0.673 AVG Training Acc 62.26 % AVG Validation Acc 61.41 %\n",
      "Epoch:100/200 AVG Training Loss:0.646 AVG Validation Loss:0.669 AVG Training Acc 62.32 % AVG Validation Acc 61.32 %\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:110/200 AVG Training Loss:0.646 AVG Validation Loss:0.672 AVG Training Acc 62.10 % AVG Validation Acc 61.32 %\n",
      "Epoch:120/200 AVG Training Loss:0.647 AVG Validation Loss:0.668 AVG Training Acc 62.27 % AVG Validation Acc 61.32 %\n",
      "Epoch:130/200 AVG Training Loss:0.647 AVG Validation Loss:0.671 AVG Training Acc 62.25 % AVG Validation Acc 61.41 %\n",
      "Epoch   137: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:140/200 AVG Training Loss:0.647 AVG Validation Loss:0.673 AVG Training Acc 62.30 % AVG Validation Acc 61.41 %\n",
      "Epoch:150/200 AVG Training Loss:0.645 AVG Validation Loss:0.673 AVG Training Acc 62.10 % AVG Validation Acc 61.32 %\n",
      "Epoch:160/200 AVG Training Loss:0.646 AVG Validation Loss:0.675 AVG Training Acc 62.18 % AVG Validation Acc 61.32 %\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:170/200 AVG Training Loss:0.646 AVG Validation Loss:0.667 AVG Training Acc 62.37 % AVG Validation Acc 61.59 %\n",
      "Epoch:180/200 AVG Training Loss:0.646 AVG Validation Loss:0.673 AVG Training Acc 62.17 % AVG Validation Acc 61.41 %\n",
      "Epoch:190/200 AVG Training Loss:0.645 AVG Validation Loss:0.674 AVG Training Acc 62.18 % AVG Validation Acc 61.50 %\n",
      "Epoch   199: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:200/200 AVG Training Loss:0.646 AVG Validation Loss:0.671 AVG Training Acc 62.33 % AVG Validation Acc 61.41 %\n",
      "Split 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8937ed6228ed42b289f8d7ac1bb3d049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.658 AVG Validation Loss:0.663 AVG Training Acc 61.92 % AVG Validation Acc 61.68 %\n",
      "Epoch:40/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.94 % AVG Validation Acc 61.50 %\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.664 AVG Training Acc 61.84 % AVG Validation Acc 61.95 %\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.664 AVG Training Acc 61.93 % AVG Validation Acc 61.68 %\n",
      "Epoch:70/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.68 %\n",
      "Epoch:80/200 AVG Training Loss:0.656 AVG Validation Loss:0.666 AVG Training Acc 61.84 % AVG Validation Acc 61.68 %\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.654 AVG Validation Loss:0.666 AVG Training Acc 62.01 % AVG Validation Acc 61.59 %\n",
      "Epoch:100/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 61.88 % AVG Validation Acc 61.59 %\n",
      "Epoch:110/200 AVG Training Loss:0.654 AVG Validation Loss:0.666 AVG Training Acc 62.09 % AVG Validation Acc 61.59 %\n",
      "Epoch   114: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 61.94 % AVG Validation Acc 61.59 %\n",
      "Epoch:130/200 AVG Training Loss:0.654 AVG Validation Loss:0.667 AVG Training Acc 61.97 % AVG Validation Acc 61.68 %\n",
      "Epoch:140/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 62.05 % AVG Validation Acc 61.59 %\n",
      "Epoch   145: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 62.08 % AVG Validation Acc 61.68 %\n",
      "Epoch:160/200 AVG Training Loss:0.654 AVG Validation Loss:0.666 AVG Training Acc 61.94 % AVG Validation Acc 61.59 %\n",
      "Epoch:170/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 61.94 % AVG Validation Acc 61.59 %\n",
      "Epoch   176: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 61.84 % AVG Validation Acc 61.59 %\n",
      "Epoch:190/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 62.07 % AVG Validation Acc 61.59 %\n",
      "Epoch:200/200 AVG Training Loss:0.654 AVG Validation Loss:0.666 AVG Training Acc 61.97 % AVG Validation Acc 61.68 %\n",
      "Split 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afb0f9fe3bb4dcfbeea55c8db66aa4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.661 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:30/200 AVG Training Loss:0.662 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.660 AVG Validation Loss:0.664 AVG Training Acc 61.86 % AVG Validation Acc 61.86 %\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.663 AVG Training Acc 61.96 % AVG Validation Acc 61.77 %\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.666 AVG Training Acc 61.85 % AVG Validation Acc 61.86 %\n",
      "Epoch:70/200 AVG Training Loss:0.654 AVG Validation Loss:0.670 AVG Training Acc 62.05 % AVG Validation Acc 61.68 %\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.653 AVG Validation Loss:0.672 AVG Training Acc 62.12 % AVG Validation Acc 61.59 %\n",
      "Epoch:90/200 AVG Training Loss:0.653 AVG Validation Loss:0.673 AVG Training Acc 62.03 % AVG Validation Acc 61.59 %\n",
      "Epoch:100/200 AVG Training Loss:0.653 AVG Validation Loss:0.674 AVG Training Acc 62.17 % AVG Validation Acc 61.59 %\n",
      "Epoch   104: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:110/200 AVG Training Loss:0.652 AVG Validation Loss:0.674 AVG Training Acc 62.14 % AVG Validation Acc 61.59 %\n",
      "Epoch:120/200 AVG Training Loss:0.654 AVG Validation Loss:0.674 AVG Training Acc 62.14 % AVG Validation Acc 61.59 %\n",
      "Epoch:130/200 AVG Training Loss:0.653 AVG Validation Loss:0.674 AVG Training Acc 62.16 % AVG Validation Acc 61.68 %\n",
      "Epoch   135: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:140/200 AVG Training Loss:0.652 AVG Validation Loss:0.674 AVG Training Acc 62.19 % AVG Validation Acc 61.68 %\n",
      "Epoch:150/200 AVG Training Loss:0.652 AVG Validation Loss:0.674 AVG Training Acc 62.21 % AVG Validation Acc 61.59 %\n",
      "Epoch:160/200 AVG Training Loss:0.652 AVG Validation Loss:0.674 AVG Training Acc 62.16 % AVG Validation Acc 61.59 %\n",
      "Epoch   166: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:170/200 AVG Training Loss:0.653 AVG Validation Loss:0.674 AVG Training Acc 62.08 % AVG Validation Acc 61.59 %\n",
      "Epoch:180/200 AVG Training Loss:0.652 AVG Validation Loss:0.674 AVG Training Acc 62.22 % AVG Validation Acc 61.59 %\n",
      "Epoch:190/200 AVG Training Loss:0.653 AVG Validation Loss:0.674 AVG Training Acc 62.01 % AVG Validation Acc 61.59 %\n",
      "Epoch   197: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:200/200 AVG Training Loss:0.653 AVG Validation Loss:0.674 AVG Training Acc 62.23 % AVG Validation Acc 61.68 %\n",
      "Split 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671bc0c3455b407ea34ddb3ee1369260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.666 AVG Training Acc 61.88 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.664 AVG Training Acc 61.85 % AVG Validation Acc 61.86 %\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.666 AVG Training Acc 61.80 % AVG Validation Acc 61.86 %\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.77 %\n",
      "Epoch:50/200 AVG Training Loss:0.657 AVG Validation Loss:0.666 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.669 AVG Training Acc 61.97 % AVG Validation Acc 61.95 %\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.655 AVG Validation Loss:0.669 AVG Training Acc 61.97 % AVG Validation Acc 61.95 %\n",
      "Epoch:80/200 AVG Training Loss:0.655 AVG Validation Loss:0.669 AVG Training Acc 61.92 % AVG Validation Acc 61.95 %\n",
      "Epoch:90/200 AVG Training Loss:0.655 AVG Validation Loss:0.670 AVG Training Acc 61.96 % AVG Validation Acc 61.95 %\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.655 AVG Validation Loss:0.670 AVG Training Acc 61.98 % AVG Validation Acc 61.95 %\n",
      "Epoch:110/200 AVG Training Loss:0.655 AVG Validation Loss:0.670 AVG Training Acc 61.95 % AVG Validation Acc 61.95 %\n",
      "Epoch:120/200 AVG Training Loss:0.655 AVG Validation Loss:0.670 AVG Training Acc 61.95 % AVG Validation Acc 61.95 %\n",
      "Epoch   124: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.655 AVG Validation Loss:0.670 AVG Training Acc 62.02 % AVG Validation Acc 61.95 %\n",
      "Epoch:140/200 AVG Training Loss:0.654 AVG Validation Loss:0.670 AVG Training Acc 61.99 % AVG Validation Acc 61.95 %\n",
      "Epoch:150/200 AVG Training Loss:0.654 AVG Validation Loss:0.670 AVG Training Acc 61.96 % AVG Validation Acc 61.95 %\n",
      "Epoch   155: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.655 AVG Validation Loss:0.670 AVG Training Acc 61.88 % AVG Validation Acc 61.95 %\n",
      "Epoch:170/200 AVG Training Loss:0.654 AVG Validation Loss:0.670 AVG Training Acc 61.90 % AVG Validation Acc 61.95 %\n",
      "Epoch:180/200 AVG Training Loss:0.655 AVG Validation Loss:0.670 AVG Training Acc 62.00 % AVG Validation Acc 61.95 %\n",
      "Epoch   186: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.655 AVG Validation Loss:0.670 AVG Training Acc 61.96 % AVG Validation Acc 61.95 %\n",
      "Epoch:200/200 AVG Training Loss:0.656 AVG Validation Loss:0.670 AVG Training Acc 61.91 % AVG Validation Acc 61.95 %\n",
      "Split 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd99be570d14deea130f0b84fe1fc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.661 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.657 AVG Validation Loss:0.659 AVG Training Acc 61.93 % AVG Validation Acc 61.91 %\n",
      "New Best Accuracy found: 62.09%\n",
      "Epoch: 45\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 61.84 % AVG Validation Acc 62.09 %\n",
      "New Best Accuracy found: 62.18%\n",
      "Epoch: 58\n",
      "Epoch:60/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 62.00 % AVG Validation Acc 62.00 %\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.654 AVG Validation Loss:0.661 AVG Training Acc 61.98 % AVG Validation Acc 62.18 %\n",
      "Epoch:80/200 AVG Training Loss:0.653 AVG Validation Loss:0.661 AVG Training Acc 61.88 % AVG Validation Acc 62.09 %\n",
      "New Best Accuracy found: 62.36%\n",
      "Epoch: 85\n",
      "Epoch:90/200 AVG Training Loss:0.653 AVG Validation Loss:0.662 AVG Training Acc 62.01 % AVG Validation Acc 62.00 %\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.653 AVG Validation Loss:0.662 AVG Training Acc 62.21 % AVG Validation Acc 61.91 %\n",
      "Epoch:110/200 AVG Training Loss:0.653 AVG Validation Loss:0.662 AVG Training Acc 62.13 % AVG Validation Acc 62.09 %\n",
      "Epoch:120/200 AVG Training Loss:0.653 AVG Validation Loss:0.661 AVG Training Acc 62.02 % AVG Validation Acc 62.00 %\n",
      "Epoch   124: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.653 AVG Validation Loss:0.661 AVG Training Acc 62.02 % AVG Validation Acc 62.09 %\n",
      "Epoch:140/200 AVG Training Loss:0.652 AVG Validation Loss:0.661 AVG Training Acc 62.05 % AVG Validation Acc 62.00 %\n",
      "Epoch:150/200 AVG Training Loss:0.652 AVG Validation Loss:0.662 AVG Training Acc 62.09 % AVG Validation Acc 62.00 %\n",
      "Epoch   155: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.652 AVG Validation Loss:0.662 AVG Training Acc 61.99 % AVG Validation Acc 62.00 %\n",
      "Epoch:170/200 AVG Training Loss:0.653 AVG Validation Loss:0.662 AVG Training Acc 62.21 % AVG Validation Acc 62.00 %\n",
      "Epoch:180/200 AVG Training Loss:0.653 AVG Validation Loss:0.662 AVG Training Acc 62.14 % AVG Validation Acc 62.18 %\n",
      "Epoch   186: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.653 AVG Validation Loss:0.662 AVG Training Acc 61.93 % AVG Validation Acc 62.27 %\n",
      "Epoch:200/200 AVG Training Loss:0.653 AVG Validation Loss:0.662 AVG Training Acc 62.08 % AVG Validation Acc 62.00 %\n",
      "Split 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cefbdc49d45342e395b8e0ba39e047a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.661 AVG Validation Loss:0.662 AVG Training Acc 61.86 % AVG Validation Acc 61.82 %\n",
      "Epoch:40/200 AVG Training Loss:0.660 AVG Validation Loss:0.662 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.82 %\n",
      "Epoch:60/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 61.81 % AVG Validation Acc 61.64 %\n",
      "Epoch:70/200 AVG Training Loss:0.653 AVG Validation Loss:0.662 AVG Training Acc 62.00 % AVG Validation Acc 61.73 %\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.651 AVG Validation Loss:0.664 AVG Training Acc 62.01 % AVG Validation Acc 61.73 %\n",
      "Epoch:90/200 AVG Training Loss:0.650 AVG Validation Loss:0.664 AVG Training Acc 62.02 % AVG Validation Acc 61.64 %\n",
      "Epoch:100/200 AVG Training Loss:0.651 AVG Validation Loss:0.664 AVG Training Acc 62.07 % AVG Validation Acc 61.64 %\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:110/200 AVG Training Loss:0.650 AVG Validation Loss:0.665 AVG Training Acc 62.06 % AVG Validation Acc 61.73 %\n",
      "Epoch:120/200 AVG Training Loss:0.651 AVG Validation Loss:0.664 AVG Training Acc 62.03 % AVG Validation Acc 61.73 %\n",
      "Epoch:130/200 AVG Training Loss:0.651 AVG Validation Loss:0.664 AVG Training Acc 62.04 % AVG Validation Acc 61.73 %\n",
      "Epoch   137: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:140/200 AVG Training Loss:0.650 AVG Validation Loss:0.665 AVG Training Acc 62.02 % AVG Validation Acc 61.73 %\n",
      "Epoch:150/200 AVG Training Loss:0.650 AVG Validation Loss:0.664 AVG Training Acc 62.03 % AVG Validation Acc 61.73 %\n",
      "Epoch:160/200 AVG Training Loss:0.650 AVG Validation Loss:0.664 AVG Training Acc 62.05 % AVG Validation Acc 61.73 %\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:170/200 AVG Training Loss:0.649 AVG Validation Loss:0.664 AVG Training Acc 62.08 % AVG Validation Acc 61.73 %\n",
      "Epoch:180/200 AVG Training Loss:0.650 AVG Validation Loss:0.664 AVG Training Acc 61.96 % AVG Validation Acc 61.73 %\n",
      "Epoch:190/200 AVG Training Loss:0.650 AVG Validation Loss:0.664 AVG Training Acc 61.88 % AVG Validation Acc 61.73 %\n",
      "Epoch   199: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:200/200 AVG Training Loss:0.649 AVG Validation Loss:0.664 AVG Training Acc 62.03 % AVG Validation Acc 61.73 %\n",
      "Split 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63449976ccf14bbfbece5fc07c630b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.664 AVG Training Acc 61.86 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.664 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.663 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.656 AVG Validation Loss:0.664 AVG Training Acc 61.99 % AVG Validation Acc 61.73 %\n",
      "Epoch:50/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.06 % AVG Validation Acc 61.55 %\n",
      "Epoch:60/200 AVG Training Loss:0.651 AVG Validation Loss:0.667 AVG Training Acc 62.22 % AVG Validation Acc 61.55 %\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.648 AVG Validation Loss:0.671 AVG Training Acc 62.39 % AVG Validation Acc 61.37 %\n",
      "Epoch:80/200 AVG Training Loss:0.647 AVG Validation Loss:0.667 AVG Training Acc 62.41 % AVG Validation Acc 61.37 %\n",
      "Epoch:90/200 AVG Training Loss:0.647 AVG Validation Loss:0.668 AVG Training Acc 62.54 % AVG Validation Acc 61.37 %\n",
      "Epoch    98: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.647 AVG Validation Loss:0.673 AVG Training Acc 62.51 % AVG Validation Acc 61.28 %\n",
      "Epoch:110/200 AVG Training Loss:0.646 AVG Validation Loss:0.669 AVG Training Acc 62.57 % AVG Validation Acc 61.37 %\n",
      "Epoch:120/200 AVG Training Loss:0.646 AVG Validation Loss:0.670 AVG Training Acc 62.61 % AVG Validation Acc 61.37 %\n",
      "Epoch   129: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.647 AVG Validation Loss:0.674 AVG Training Acc 62.43 % AVG Validation Acc 61.37 %\n",
      "Epoch:140/200 AVG Training Loss:0.647 AVG Validation Loss:0.674 AVG Training Acc 62.50 % AVG Validation Acc 61.37 %\n",
      "Epoch:150/200 AVG Training Loss:0.646 AVG Validation Loss:0.673 AVG Training Acc 62.53 % AVG Validation Acc 61.28 %\n",
      "Epoch:160/200 AVG Training Loss:0.647 AVG Validation Loss:0.670 AVG Training Acc 62.22 % AVG Validation Acc 61.28 %\n",
      "Epoch   160: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:170/200 AVG Training Loss:0.647 AVG Validation Loss:0.669 AVG Training Acc 62.44 % AVG Validation Acc 61.28 %\n",
      "Epoch:180/200 AVG Training Loss:0.646 AVG Validation Loss:0.669 AVG Training Acc 62.37 % AVG Validation Acc 61.28 %\n",
      "Epoch:190/200 AVG Training Loss:0.646 AVG Validation Loss:0.673 AVG Training Acc 62.47 % AVG Validation Acc 61.28 %\n",
      "Epoch   191: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:200/200 AVG Training Loss:0.646 AVG Validation Loss:0.668 AVG Training Acc 62.46 % AVG Validation Acc 61.37 %\n",
      "Split 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4cd56c1b15548329b95e94062536987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.663 AVG Validation Loss:0.658 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.658 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.658 AVG Training Acc 61.86 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.657 AVG Validation Loss:0.658 AVG Training Acc 61.94 % AVG Validation Acc 61.82 %\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.659 AVG Training Acc 62.04 % AVG Validation Acc 61.82 %\n",
      "Epoch:70/200 AVG Training Loss:0.656 AVG Validation Loss:0.659 AVG Training Acc 62.01 % AVG Validation Acc 61.64 %\n",
      "Epoch:80/200 AVG Training Loss:0.656 AVG Validation Loss:0.659 AVG Training Acc 61.99 % AVG Validation Acc 61.64 %\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.656 AVG Validation Loss:0.659 AVG Training Acc 62.12 % AVG Validation Acc 61.46 %\n",
      "Epoch:100/200 AVG Training Loss:0.656 AVG Validation Loss:0.659 AVG Training Acc 61.98 % AVG Validation Acc 61.73 %\n",
      "Epoch:110/200 AVG Training Loss:0.656 AVG Validation Loss:0.659 AVG Training Acc 61.97 % AVG Validation Acc 61.82 %\n",
      "Epoch   115: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.655 AVG Validation Loss:0.659 AVG Training Acc 62.05 % AVG Validation Acc 61.28 %\n",
      "Epoch:130/200 AVG Training Loss:0.656 AVG Validation Loss:0.659 AVG Training Acc 62.08 % AVG Validation Acc 61.46 %\n",
      "Epoch:140/200 AVG Training Loss:0.656 AVG Validation Loss:0.659 AVG Training Acc 62.08 % AVG Validation Acc 61.46 %\n",
      "Epoch   146: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.655 AVG Validation Loss:0.659 AVG Training Acc 62.09 % AVG Validation Acc 61.46 %\n",
      "Epoch:160/200 AVG Training Loss:0.655 AVG Validation Loss:0.659 AVG Training Acc 62.16 % AVG Validation Acc 61.55 %\n",
      "Epoch:170/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 62.04 % AVG Validation Acc 61.46 %\n",
      "Epoch   177: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.656 AVG Validation Loss:0.659 AVG Training Acc 61.97 % AVG Validation Acc 61.64 %\n",
      "Epoch:190/200 AVG Training Loss:0.656 AVG Validation Loss:0.659 AVG Training Acc 62.09 % AVG Validation Acc 61.55 %\n",
      "Epoch:200/200 AVG Training Loss:0.655 AVG Validation Loss:0.659 AVG Training Acc 62.11 % AVG Validation Acc 61.64 %\n",
      "Split 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291865940d2245af894c700abaa2b614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.663 AVG Validation Loss:0.659 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.661 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.661 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.661 AVG Training Acc 61.95 % AVG Validation Acc 61.91 %\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.87 % AVG Validation Acc 61.82 %\n",
      "Epoch:70/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.73 %\n",
      "Epoch:80/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 62.00 % AVG Validation Acc 61.73 %\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 61.92 % AVG Validation Acc 61.73 %\n",
      "Epoch:100/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 61.96 % AVG Validation Acc 61.73 %\n",
      "Epoch:110/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.73 %\n",
      "Epoch   114: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 61.99 % AVG Validation Acc 61.73 %\n",
      "Epoch:130/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 62.01 % AVG Validation Acc 61.73 %\n",
      "Epoch:140/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 61.91 % AVG Validation Acc 61.73 %\n",
      "Epoch   145: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 61.94 % AVG Validation Acc 61.73 %\n",
      "Epoch:160/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 61.86 % AVG Validation Acc 61.73 %\n",
      "Epoch:170/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 61.97 % AVG Validation Acc 61.73 %\n",
      "Epoch   176: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 61.93 % AVG Validation Acc 61.73 %\n",
      "Epoch:190/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 61.91 % AVG Validation Acc 61.73 %\n",
      "Epoch:200/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 61.94 % AVG Validation Acc 61.73 %\n",
      "Split 21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe7db1c04b24ea2aecd23b654e01d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.659 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.91 % AVG Validation Acc 61.95 %\n",
      "Epoch:60/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.95 %\n",
      "Epoch:70/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.88 % AVG Validation Acc 61.95 %\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:80/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.86 % AVG Validation Acc 61.95 %\n",
      "Epoch:90/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.95 %\n",
      "Epoch:100/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.95 %\n",
      "Epoch:110/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch   110: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:130/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.95 %\n",
      "Epoch:140/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.91 % AVG Validation Acc 61.95 %\n",
      "Epoch   141: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.88 % AVG Validation Acc 61.95 %\n",
      "Epoch:160/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.91 % AVG Validation Acc 61.95 %\n",
      "Epoch:170/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch   172: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.95 %\n",
      "Epoch:190/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.91 % AVG Validation Acc 61.95 %\n",
      "Epoch:200/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Split 22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d791df797da448ab36e699236515a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.660 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.659 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.660 AVG Validation Loss:0.659 AVG Training Acc 61.88 % AVG Validation Acc 61.86 %\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.659 AVG Training Acc 61.94 % AVG Validation Acc 61.86 %\n",
      "Epoch:60/200 AVG Training Loss:0.654 AVG Validation Loss:0.657 AVG Training Acc 61.96 % AVG Validation Acc 61.86 %\n",
      "Epoch:70/200 AVG Training Loss:0.653 AVG Validation Loss:0.658 AVG Training Acc 61.85 % AVG Validation Acc 61.68 %\n",
      "Epoch:80/200 AVG Training Loss:0.651 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.77 %\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:90/200 AVG Training Loss:0.649 AVG Validation Loss:0.657 AVG Training Acc 62.24 % AVG Validation Acc 61.86 %\n",
      "Epoch:100/200 AVG Training Loss:0.649 AVG Validation Loss:0.657 AVG Training Acc 62.09 % AVG Validation Acc 61.68 %\n",
      "Epoch:110/200 AVG Training Loss:0.650 AVG Validation Loss:0.659 AVG Training Acc 61.96 % AVG Validation Acc 61.77 %\n",
      "Epoch   113: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:120/200 AVG Training Loss:0.647 AVG Validation Loss:0.658 AVG Training Acc 62.03 % AVG Validation Acc 61.86 %\n",
      "Epoch:130/200 AVG Training Loss:0.649 AVG Validation Loss:0.658 AVG Training Acc 61.99 % AVG Validation Acc 61.68 %\n",
      "Epoch:140/200 AVG Training Loss:0.649 AVG Validation Loss:0.658 AVG Training Acc 61.95 % AVG Validation Acc 61.68 %\n",
      "Epoch   144: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:150/200 AVG Training Loss:0.649 AVG Validation Loss:0.658 AVG Training Acc 62.18 % AVG Validation Acc 61.86 %\n",
      "Epoch:160/200 AVG Training Loss:0.649 AVG Validation Loss:0.658 AVG Training Acc 62.10 % AVG Validation Acc 61.77 %\n",
      "Epoch:170/200 AVG Training Loss:0.648 AVG Validation Loss:0.657 AVG Training Acc 62.23 % AVG Validation Acc 61.59 %\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:180/200 AVG Training Loss:0.649 AVG Validation Loss:0.657 AVG Training Acc 62.00 % AVG Validation Acc 61.68 %\n",
      "Epoch:190/200 AVG Training Loss:0.649 AVG Validation Loss:0.657 AVG Training Acc 62.18 % AVG Validation Acc 61.86 %\n",
      "Epoch:200/200 AVG Training Loss:0.650 AVG Validation Loss:0.658 AVG Training Acc 61.99 % AVG Validation Acc 61.77 %\n",
      "Split 23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5388b20b09e64039bb786c706a22b8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.659 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 62.00 % AVG Validation Acc 61.86 %\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.655 AVG Validation Loss:0.662 AVG Training Acc 62.10 % AVG Validation Acc 61.77 %\n",
      "Epoch:70/200 AVG Training Loss:0.655 AVG Validation Loss:0.663 AVG Training Acc 61.96 % AVG Validation Acc 61.77 %\n",
      "Epoch:80/200 AVG Training Loss:0.655 AVG Validation Loss:0.663 AVG Training Acc 62.04 % AVG Validation Acc 61.77 %\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.655 AVG Validation Loss:0.663 AVG Training Acc 62.11 % AVG Validation Acc 61.77 %\n",
      "Epoch:100/200 AVG Training Loss:0.655 AVG Validation Loss:0.663 AVG Training Acc 62.03 % AVG Validation Acc 61.77 %\n",
      "Epoch:110/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 62.12 % AVG Validation Acc 61.68 %\n",
      "Epoch   116: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.655 AVG Validation Loss:0.663 AVG Training Acc 62.13 % AVG Validation Acc 61.68 %\n",
      "Epoch:130/200 AVG Training Loss:0.655 AVG Validation Loss:0.663 AVG Training Acc 62.12 % AVG Validation Acc 61.68 %\n",
      "Epoch:140/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 62.04 % AVG Validation Acc 61.77 %\n",
      "Epoch   147: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 62.04 % AVG Validation Acc 61.68 %\n",
      "Epoch:160/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 62.03 % AVG Validation Acc 61.59 %\n",
      "Epoch:170/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 62.03 % AVG Validation Acc 61.68 %\n",
      "Epoch   178: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.655 AVG Validation Loss:0.663 AVG Training Acc 62.11 % AVG Validation Acc 61.68 %\n",
      "Epoch:190/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 62.05 % AVG Validation Acc 61.68 %\n",
      "Epoch:200/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 62.16 % AVG Validation Acc 61.68 %\n",
      "Split 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37aa284c9c9496581ef53146a7b6a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.658 AVG Validation Loss:0.661 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.661 AVG Training Acc 61.94 % AVG Validation Acc 61.68 %\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.661 AVG Training Acc 61.97 % AVG Validation Acc 61.68 %\n",
      "Epoch:60/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 62.05 % AVG Validation Acc 61.68 %\n",
      "Epoch:70/200 AVG Training Loss:0.655 AVG Validation Loss:0.662 AVG Training Acc 62.20 % AVG Validation Acc 61.68 %\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 62.02 % AVG Validation Acc 61.68 %\n",
      "Epoch:90/200 AVG Training Loss:0.655 AVG Validation Loss:0.662 AVG Training Acc 61.99 % AVG Validation Acc 61.68 %\n",
      "Epoch:100/200 AVG Training Loss:0.653 AVG Validation Loss:0.663 AVG Training Acc 62.09 % AVG Validation Acc 61.68 %\n",
      "Epoch   101: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:110/200 AVG Training Loss:0.653 AVG Validation Loss:0.663 AVG Training Acc 62.07 % AVG Validation Acc 61.68 %\n",
      "Epoch:120/200 AVG Training Loss:0.653 AVG Validation Loss:0.663 AVG Training Acc 62.16 % AVG Validation Acc 61.68 %\n",
      "Epoch:130/200 AVG Training Loss:0.653 AVG Validation Loss:0.662 AVG Training Acc 62.08 % AVG Validation Acc 61.77 %\n",
      "Epoch   132: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:140/200 AVG Training Loss:0.653 AVG Validation Loss:0.663 AVG Training Acc 62.24 % AVG Validation Acc 61.68 %\n",
      "Epoch:150/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 62.22 % AVG Validation Acc 61.68 %\n",
      "Epoch:160/200 AVG Training Loss:0.653 AVG Validation Loss:0.663 AVG Training Acc 62.03 % AVG Validation Acc 61.77 %\n",
      "Epoch   163: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:170/200 AVG Training Loss:0.653 AVG Validation Loss:0.663 AVG Training Acc 62.09 % AVG Validation Acc 61.68 %\n",
      "Epoch:180/200 AVG Training Loss:0.653 AVG Validation Loss:0.663 AVG Training Acc 62.20 % AVG Validation Acc 61.68 %\n",
      "Epoch:190/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 62.17 % AVG Validation Acc 61.77 %\n",
      "Epoch   194: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:200/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 62.17 % AVG Validation Acc 61.77 %\n",
      "Split 25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ad70f107044cdb8ec5431536541cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.656 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.656 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.660 AVG Validation Loss:0.656 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:50/200 AVG Training Loss:0.659 AVG Validation Loss:0.655 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:60/200 AVG Training Loss:0.658 AVG Validation Loss:0.654 AVG Training Acc 61.84 % AVG Validation Acc 61.86 %\n",
      "Epoch:70/200 AVG Training Loss:0.658 AVG Validation Loss:0.655 AVG Training Acc 61.92 % AVG Validation Acc 61.86 %\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.657 AVG Validation Loss:0.655 AVG Training Acc 61.96 % AVG Validation Acc 61.59 %\n",
      "Epoch:90/200 AVG Training Loss:0.656 AVG Validation Loss:0.655 AVG Training Acc 61.97 % AVG Validation Acc 61.59 %\n",
      "Epoch:100/200 AVG Training Loss:0.656 AVG Validation Loss:0.656 AVG Training Acc 62.02 % AVG Validation Acc 61.68 %\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:110/200 AVG Training Loss:0.655 AVG Validation Loss:0.655 AVG Training Acc 61.98 % AVG Validation Acc 61.59 %\n",
      "Epoch:120/200 AVG Training Loss:0.656 AVG Validation Loss:0.656 AVG Training Acc 61.96 % AVG Validation Acc 61.68 %\n",
      "Epoch:130/200 AVG Training Loss:0.655 AVG Validation Loss:0.656 AVG Training Acc 62.00 % AVG Validation Acc 61.68 %\n",
      "Epoch   137: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:140/200 AVG Training Loss:0.655 AVG Validation Loss:0.656 AVG Training Acc 62.01 % AVG Validation Acc 61.68 %\n",
      "Epoch:150/200 AVG Training Loss:0.656 AVG Validation Loss:0.656 AVG Training Acc 62.00 % AVG Validation Acc 61.68 %\n",
      "Epoch:160/200 AVG Training Loss:0.655 AVG Validation Loss:0.656 AVG Training Acc 62.00 % AVG Validation Acc 61.77 %\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:170/200 AVG Training Loss:0.657 AVG Validation Loss:0.656 AVG Training Acc 61.87 % AVG Validation Acc 61.77 %\n",
      "Epoch:180/200 AVG Training Loss:0.655 AVG Validation Loss:0.656 AVG Training Acc 62.05 % AVG Validation Acc 61.68 %\n",
      "Epoch:190/200 AVG Training Loss:0.656 AVG Validation Loss:0.656 AVG Training Acc 61.93 % AVG Validation Acc 61.68 %\n",
      "Epoch   199: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:200/200 AVG Training Loss:0.656 AVG Validation Loss:0.656 AVG Training Acc 61.94 % AVG Validation Acc 61.77 %\n",
      "Split 26\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0fb1b75abc4e228bc74f362fff75da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.658 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.661 AVG Validation Loss:0.657 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.657 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 61.96 % AVG Validation Acc 61.91 %\n",
      "Epoch:60/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 62.01 % AVG Validation Acc 62.00 %\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.652 AVG Validation Loss:0.662 AVG Training Acc 62.05 % AVG Validation Acc 62.00 %\n",
      "Epoch:80/200 AVG Training Loss:0.651 AVG Validation Loss:0.663 AVG Training Acc 62.06 % AVG Validation Acc 62.00 %\n",
      "Epoch:90/200 AVG Training Loss:0.652 AVG Validation Loss:0.664 AVG Training Acc 62.05 % AVG Validation Acc 62.00 %\n",
      "Epoch    99: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.652 AVG Validation Loss:0.665 AVG Training Acc 61.91 % AVG Validation Acc 62.00 %\n",
      "Epoch:110/200 AVG Training Loss:0.652 AVG Validation Loss:0.664 AVG Training Acc 62.00 % AVG Validation Acc 62.00 %\n",
      "Epoch:120/200 AVG Training Loss:0.652 AVG Validation Loss:0.664 AVG Training Acc 62.07 % AVG Validation Acc 62.00 %\n",
      "Epoch:130/200 AVG Training Loss:0.652 AVG Validation Loss:0.664 AVG Training Acc 62.04 % AVG Validation Acc 62.09 %\n",
      "Epoch   130: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:140/200 AVG Training Loss:0.652 AVG Validation Loss:0.664 AVG Training Acc 62.02 % AVG Validation Acc 62.09 %\n",
      "Epoch:150/200 AVG Training Loss:0.652 AVG Validation Loss:0.665 AVG Training Acc 62.04 % AVG Validation Acc 61.91 %\n",
      "Epoch:160/200 AVG Training Loss:0.652 AVG Validation Loss:0.665 AVG Training Acc 62.04 % AVG Validation Acc 62.09 %\n",
      "Epoch   161: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:170/200 AVG Training Loss:0.652 AVG Validation Loss:0.665 AVG Training Acc 62.08 % AVG Validation Acc 62.09 %\n",
      "Epoch:180/200 AVG Training Loss:0.652 AVG Validation Loss:0.664 AVG Training Acc 62.03 % AVG Validation Acc 61.91 %\n",
      "Epoch:190/200 AVG Training Loss:0.652 AVG Validation Loss:0.665 AVG Training Acc 61.88 % AVG Validation Acc 62.09 %\n",
      "Epoch   192: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:200/200 AVG Training Loss:0.651 AVG Validation Loss:0.664 AVG Training Acc 62.10 % AVG Validation Acc 62.00 %\n",
      "Split 27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfae15b7b522403cad760e48dec75e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.659 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.94 % AVG Validation Acc 61.82 %\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.95 % AVG Validation Acc 61.82 %\n",
      "Epoch:70/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.98 % AVG Validation Acc 61.91 %\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:80/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 61.94 % AVG Validation Acc 61.91 %\n",
      "Epoch:90/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.94 % AVG Validation Acc 61.91 %\n",
      "Epoch:100/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.98 % AVG Validation Acc 61.91 %\n",
      "Epoch   108: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:110/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 61.98 % AVG Validation Acc 61.91 %\n",
      "Epoch:120/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.97 % AVG Validation Acc 61.91 %\n",
      "Epoch:130/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 62.01 % AVG Validation Acc 61.91 %\n",
      "Epoch   139: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:140/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 61.97 % AVG Validation Acc 61.91 %\n",
      "Epoch:150/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 61.99 % AVG Validation Acc 61.91 %\n",
      "Epoch:160/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 62.00 % AVG Validation Acc 61.91 %\n",
      "Epoch:170/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 61.95 % AVG Validation Acc 61.91 %\n",
      "Epoch   170: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.93 % AVG Validation Acc 61.91 %\n",
      "Epoch:190/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 61.96 % AVG Validation Acc 61.91 %\n",
      "Epoch:200/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 62.01 % AVG Validation Acc 61.91 %\n",
      "Split 28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b6f10578a740b883e97efe8ab9dada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.663 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.658 AVG Training Acc 61.83 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.661 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.660 AVG Validation Loss:0.658 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:50/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:60/200 AVG Training Loss:0.657 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.82 %\n",
      "Epoch:70/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.94 % AVG Validation Acc 61.82 %\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.655 AVG Validation Loss:0.667 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Epoch:90/200 AVG Training Loss:0.655 AVG Validation Loss:0.668 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:100/200 AVG Training Loss:0.655 AVG Validation Loss:0.668 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch   104: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:110/200 AVG Training Loss:0.654 AVG Validation Loss:0.668 AVG Training Acc 61.93 % AVG Validation Acc 61.91 %\n",
      "Epoch:120/200 AVG Training Loss:0.655 AVG Validation Loss:0.668 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Epoch:130/200 AVG Training Loss:0.654 AVG Validation Loss:0.668 AVG Training Acc 61.96 % AVG Validation Acc 61.91 %\n",
      "Epoch   135: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:140/200 AVG Training Loss:0.655 AVG Validation Loss:0.668 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:150/200 AVG Training Loss:0.655 AVG Validation Loss:0.668 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Epoch:160/200 AVG Training Loss:0.654 AVG Validation Loss:0.668 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch   166: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:170/200 AVG Training Loss:0.654 AVG Validation Loss:0.668 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:180/200 AVG Training Loss:0.655 AVG Validation Loss:0.668 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:190/200 AVG Training Loss:0.654 AVG Validation Loss:0.668 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch   197: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:200/200 AVG Training Loss:0.655 AVG Validation Loss:0.668 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Split 29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44f253561f14e719b5b976f0a921f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.661 AVG Validation Loss:0.665 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.659 AVG Validation Loss:0.666 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.666 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.669 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.670 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.670 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:70/200 AVG Training Loss:0.656 AVG Validation Loss:0.671 AVG Training Acc 61.92 % AVG Validation Acc 61.82 %\n",
      "Epoch:80/200 AVG Training Loss:0.656 AVG Validation Loss:0.671 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.655 AVG Validation Loss:0.671 AVG Training Acc 61.88 % AVG Validation Acc 61.82 %\n",
      "Epoch:100/200 AVG Training Loss:0.656 AVG Validation Loss:0.671 AVG Training Acc 61.93 % AVG Validation Acc 61.82 %\n",
      "Epoch:110/200 AVG Training Loss:0.655 AVG Validation Loss:0.671 AVG Training Acc 61.90 % AVG Validation Acc 61.55 %\n",
      "Epoch   111: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.655 AVG Validation Loss:0.671 AVG Training Acc 61.93 % AVG Validation Acc 61.82 %\n",
      "Epoch:130/200 AVG Training Loss:0.655 AVG Validation Loss:0.671 AVG Training Acc 61.93 % AVG Validation Acc 61.82 %\n",
      "Epoch:140/200 AVG Training Loss:0.656 AVG Validation Loss:0.671 AVG Training Acc 61.92 % AVG Validation Acc 61.64 %\n",
      "Epoch   142: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.656 AVG Validation Loss:0.671 AVG Training Acc 61.86 % AVG Validation Acc 61.82 %\n",
      "Epoch:160/200 AVG Training Loss:0.655 AVG Validation Loss:0.671 AVG Training Acc 61.92 % AVG Validation Acc 61.73 %\n",
      "Epoch:170/200 AVG Training Loss:0.656 AVG Validation Loss:0.671 AVG Training Acc 61.89 % AVG Validation Acc 61.64 %\n",
      "Epoch   173: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.655 AVG Validation Loss:0.671 AVG Training Acc 61.92 % AVG Validation Acc 61.82 %\n",
      "Epoch:190/200 AVG Training Loss:0.656 AVG Validation Loss:0.671 AVG Training Acc 61.91 % AVG Validation Acc 61.73 %\n",
      "Epoch:200/200 AVG Training Loss:0.655 AVG Validation Loss:0.671 AVG Training Acc 61.96 % AVG Validation Acc 61.82 %\n",
      "Split 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e48a0cf82c64773904ebe943d99816d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.660 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/200 AVG Training Loss:0.657 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:60/200 AVG Training Loss:0.657 AVG Validation Loss:0.663 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch:70/200 AVG Training Loss:0.657 AVG Validation Loss:0.663 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:80/200 AVG Training Loss:0.658 AVG Validation Loss:0.663 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch:90/200 AVG Training Loss:0.657 AVG Validation Loss:0.663 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch:100/200 AVG Training Loss:0.657 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch   108: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:110/200 AVG Training Loss:0.657 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:120/200 AVG Training Loss:0.657 AVG Validation Loss:0.663 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch:130/200 AVG Training Loss:0.657 AVG Validation Loss:0.663 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch   139: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:140/200 AVG Training Loss:0.658 AVG Validation Loss:0.663 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch:150/200 AVG Training Loss:0.657 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:160/200 AVG Training Loss:0.658 AVG Validation Loss:0.663 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:170/200 AVG Training Loss:0.657 AVG Validation Loss:0.663 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch   170: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.657 AVG Validation Loss:0.663 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch:190/200 AVG Training Loss:0.658 AVG Validation Loss:0.663 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch:200/200 AVG Training Loss:0.658 AVG Validation Loss:0.663 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Split 31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53b519c149540bfb97da6a8187ab7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.663 AVG Validation Loss:0.664 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.660 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.664 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.663 AVG Training Acc 61.92 % AVG Validation Acc 61.95 %\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.664 AVG Training Acc 61.97 % AVG Validation Acc 61.95 %\n",
      "Epoch:60/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 62.12 % AVG Validation Acc 61.86 %\n",
      "Epoch:70/200 AVG Training Loss:0.654 AVG Validation Loss:0.666 AVG Training Acc 62.05 % AVG Validation Acc 61.86 %\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:80/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 62.06 % AVG Validation Acc 61.95 %\n",
      "Epoch:90/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.06 % AVG Validation Acc 61.86 %\n",
      "Epoch:100/200 AVG Training Loss:0.655 AVG Validation Loss:0.664 AVG Training Acc 62.01 % AVG Validation Acc 61.86 %\n",
      "Epoch   105: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:110/200 AVG Training Loss:0.655 AVG Validation Loss:0.664 AVG Training Acc 62.00 % AVG Validation Acc 61.86 %\n",
      "Epoch:120/200 AVG Training Loss:0.655 AVG Validation Loss:0.664 AVG Training Acc 61.99 % AVG Validation Acc 61.86 %\n",
      "Epoch:130/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.03 % AVG Validation Acc 61.86 %\n",
      "Epoch   136: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:140/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 62.04 % AVG Validation Acc 61.86 %\n",
      "Epoch:150/200 AVG Training Loss:0.653 AVG Validation Loss:0.665 AVG Training Acc 62.09 % AVG Validation Acc 61.86 %\n",
      "Epoch:160/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:170/200 AVG Training Loss:0.653 AVG Validation Loss:0.664 AVG Training Acc 62.16 % AVG Validation Acc 61.86 %\n",
      "Epoch:180/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.05 % AVG Validation Acc 61.86 %\n",
      "Epoch:190/200 AVG Training Loss:0.654 AVG Validation Loss:0.665 AVG Training Acc 61.98 % AVG Validation Acc 61.86 %\n",
      "Epoch:200/200 AVG Training Loss:0.653 AVG Validation Loss:0.664 AVG Training Acc 62.05 % AVG Validation Acc 61.86 %\n",
      "Split 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50331674e1e4b869aebd619edd59bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.663 AVG Validation Loss:0.656 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.655 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:30/200 AVG Training Loss:0.662 AVG Validation Loss:0.658 AVG Training Acc 61.85 % AVG Validation Acc 61.86 %\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.655 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.657 AVG Training Acc 61.96 % AVG Validation Acc 61.77 %\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 62.07 % AVG Validation Acc 61.50 %\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.655 AVG Validation Loss:0.662 AVG Training Acc 62.11 % AVG Validation Acc 61.05 %\n",
      "Epoch:80/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 61.97 % AVG Validation Acc 61.05 %\n",
      "Epoch:90/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 62.05 % AVG Validation Acc 61.05 %\n",
      "Epoch    95: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 62.07 % AVG Validation Acc 61.14 %\n",
      "Epoch:110/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 62.11 % AVG Validation Acc 61.14 %\n",
      "Epoch:120/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 62.06 % AVG Validation Acc 61.05 %\n",
      "Epoch   126: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 62.04 % AVG Validation Acc 61.05 %\n",
      "Epoch:140/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 62.00 % AVG Validation Acc 61.05 %\n",
      "Epoch:150/200 AVG Training Loss:0.655 AVG Validation Loss:0.663 AVG Training Acc 62.13 % AVG Validation Acc 61.05 %\n",
      "Epoch   157: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.655 AVG Validation Loss:0.662 AVG Training Acc 62.03 % AVG Validation Acc 61.14 %\n",
      "Epoch:170/200 AVG Training Loss:0.655 AVG Validation Loss:0.662 AVG Training Acc 62.07 % AVG Validation Acc 61.05 %\n",
      "Epoch:180/200 AVG Training Loss:0.655 AVG Validation Loss:0.662 AVG Training Acc 62.05 % AVG Validation Acc 61.14 %\n",
      "Epoch   188: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 62.11 % AVG Validation Acc 61.05 %\n",
      "Epoch:200/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 62.05 % AVG Validation Acc 61.14 %\n",
      "Split 33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312bcf15eb6e4bce8f5b79469f074324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.663 AVG Training Acc 61.86 % AVG Validation Acc 61.86 %\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.660 AVG Validation Loss:0.663 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.664 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:60/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:70/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:80/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:90/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:100/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch   109: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:110/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:120/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:130/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch:140/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch   140: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:160/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch:170/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:190/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:200/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Split 34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6cecfb08cce494a9a0de2944261d7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.664 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.664 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.663 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.88 % AVG Validation Acc 61.86 %\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.661 AVG Training Acc 61.95 % AVG Validation Acc 61.86 %\n",
      "Epoch:70/200 AVG Training Loss:0.655 AVG Validation Loss:0.662 AVG Training Acc 62.04 % AVG Validation Acc 61.95 %\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.652 AVG Validation Loss:0.662 AVG Training Acc 62.14 % AVG Validation Acc 61.77 %\n",
      "Epoch:90/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 62.18 % AVG Validation Acc 62.04 %\n",
      "Epoch:100/200 AVG Training Loss:0.652 AVG Validation Loss:0.663 AVG Training Acc 62.17 % AVG Validation Acc 61.77 %\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:110/200 AVG Training Loss:0.652 AVG Validation Loss:0.663 AVG Training Acc 62.11 % AVG Validation Acc 61.77 %\n",
      "Epoch:120/200 AVG Training Loss:0.652 AVG Validation Loss:0.663 AVG Training Acc 62.13 % AVG Validation Acc 61.59 %\n",
      "Epoch:130/200 AVG Training Loss:0.653 AVG Validation Loss:0.663 AVG Training Acc 62.04 % AVG Validation Acc 61.59 %\n",
      "Epoch   137: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:140/200 AVG Training Loss:0.652 AVG Validation Loss:0.663 AVG Training Acc 62.10 % AVG Validation Acc 61.59 %\n",
      "Epoch:150/200 AVG Training Loss:0.652 AVG Validation Loss:0.663 AVG Training Acc 62.25 % AVG Validation Acc 61.59 %\n",
      "Epoch:160/200 AVG Training Loss:0.653 AVG Validation Loss:0.663 AVG Training Acc 62.01 % AVG Validation Acc 61.59 %\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:170/200 AVG Training Loss:0.652 AVG Validation Loss:0.663 AVG Training Acc 62.14 % AVG Validation Acc 61.59 %\n",
      "Epoch:180/200 AVG Training Loss:0.653 AVG Validation Loss:0.663 AVG Training Acc 62.03 % AVG Validation Acc 61.59 %\n",
      "Epoch:190/200 AVG Training Loss:0.652 AVG Validation Loss:0.663 AVG Training Acc 62.10 % AVG Validation Acc 61.50 %\n",
      "Epoch   199: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:200/200 AVG Training Loss:0.652 AVG Validation Loss:0.663 AVG Training Acc 62.17 % AVG Validation Acc 61.68 %\n",
      "Split 35\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf76ee12143941a98d2fcce3d90ce139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.661 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.661 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:50/200 AVG Training Loss:0.657 AVG Validation Loss:0.661 AVG Training Acc 61.92 % AVG Validation Acc 61.86 %\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.662 AVG Training Acc 61.91 % AVG Validation Acc 61.77 %\n",
      "Epoch:70/200 AVG Training Loss:0.656 AVG Validation Loss:0.662 AVG Training Acc 61.91 % AVG Validation Acc 61.77 %\n",
      "Epoch:80/200 AVG Training Loss:0.655 AVG Validation Loss:0.662 AVG Training Acc 62.00 % AVG Validation Acc 61.77 %\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 62.00 % AVG Validation Acc 61.86 %\n",
      "Epoch:100/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.96 % AVG Validation Acc 61.86 %\n",
      "Epoch:110/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.93 % AVG Validation Acc 61.86 %\n",
      "Epoch   118: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.96 % AVG Validation Acc 61.86 %\n",
      "Epoch:130/200 AVG Training Loss:0.654 AVG Validation Loss:0.661 AVG Training Acc 61.98 % AVG Validation Acc 61.86 %\n",
      "Epoch:140/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.93 % AVG Validation Acc 61.86 %\n",
      "Epoch   149: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.97 % AVG Validation Acc 61.86 %\n",
      "Epoch:160/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 62.05 % AVG Validation Acc 61.86 %\n",
      "Epoch:170/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.97 % AVG Validation Acc 61.86 %\n",
      "Epoch:180/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.98 % AVG Validation Acc 61.86 %\n",
      "Epoch   180: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.97 % AVG Validation Acc 61.86 %\n",
      "Epoch:200/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.95 % AVG Validation Acc 61.86 %\n",
      "Split 36\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acac9c45c9554144b1ab127f6bc169bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.658 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.656 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.657 AVG Validation Loss:0.656 AVG Training Acc 61.96 % AVG Validation Acc 61.91 %\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.657 AVG Training Acc 61.95 % AVG Validation Acc 61.91 %\n",
      "Epoch:70/200 AVG Training Loss:0.657 AVG Validation Loss:0.657 AVG Training Acc 61.94 % AVG Validation Acc 61.82 %\n",
      "Epoch:80/200 AVG Training Loss:0.656 AVG Validation Loss:0.658 AVG Training Acc 62.03 % AVG Validation Acc 61.82 %\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.656 AVG Validation Loss:0.658 AVG Training Acc 62.05 % AVG Validation Acc 61.82 %\n",
      "Epoch:100/200 AVG Training Loss:0.656 AVG Validation Loss:0.658 AVG Training Acc 61.96 % AVG Validation Acc 61.82 %\n",
      "Epoch:110/200 AVG Training Loss:0.656 AVG Validation Loss:0.658 AVG Training Acc 61.99 % AVG Validation Acc 61.82 %\n",
      "Epoch   116: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.656 AVG Validation Loss:0.658 AVG Training Acc 62.07 % AVG Validation Acc 61.82 %\n",
      "Epoch:130/200 AVG Training Loss:0.656 AVG Validation Loss:0.658 AVG Training Acc 62.02 % AVG Validation Acc 61.82 %\n",
      "Epoch:140/200 AVG Training Loss:0.656 AVG Validation Loss:0.658 AVG Training Acc 61.96 % AVG Validation Acc 61.82 %\n",
      "Epoch   147: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 62.01 % AVG Validation Acc 61.82 %\n",
      "Epoch:160/200 AVG Training Loss:0.656 AVG Validation Loss:0.658 AVG Training Acc 62.04 % AVG Validation Acc 61.82 %\n",
      "Epoch:170/200 AVG Training Loss:0.656 AVG Validation Loss:0.658 AVG Training Acc 62.02 % AVG Validation Acc 61.82 %\n",
      "Epoch   178: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.656 AVG Validation Loss:0.658 AVG Training Acc 61.98 % AVG Validation Acc 61.82 %\n",
      "Epoch:190/200 AVG Training Loss:0.656 AVG Validation Loss:0.658 AVG Training Acc 62.01 % AVG Validation Acc 61.82 %\n",
      "Epoch:200/200 AVG Training Loss:0.656 AVG Validation Loss:0.658 AVG Training Acc 61.96 % AVG Validation Acc 61.82 %\n",
      "Split 37\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446cf022a6404642854f63a47e3fca79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.661 AVG Training Acc 61.71 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.660 AVG Training Acc 61.86 % AVG Validation Acc 61.91 %\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 61.86 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 61.95 % AVG Validation Acc 61.64 %\n",
      "Epoch:60/200 AVG Training Loss:0.652 AVG Validation Loss:0.662 AVG Training Acc 62.05 % AVG Validation Acc 61.64 %\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.652 AVG Validation Loss:0.662 AVG Training Acc 62.15 % AVG Validation Acc 61.55 %\n",
      "Epoch:80/200 AVG Training Loss:0.650 AVG Validation Loss:0.663 AVG Training Acc 62.12 % AVG Validation Acc 61.55 %\n",
      "Epoch:90/200 AVG Training Loss:0.651 AVG Validation Loss:0.663 AVG Training Acc 62.22 % AVG Validation Acc 61.55 %\n",
      "Epoch    96: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.650 AVG Validation Loss:0.663 AVG Training Acc 62.09 % AVG Validation Acc 61.55 %\n",
      "Epoch:110/200 AVG Training Loss:0.650 AVG Validation Loss:0.663 AVG Training Acc 61.98 % AVG Validation Acc 61.73 %\n",
      "Epoch:120/200 AVG Training Loss:0.650 AVG Validation Loss:0.663 AVG Training Acc 62.17 % AVG Validation Acc 61.64 %\n",
      "Epoch   127: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.650 AVG Validation Loss:0.663 AVG Training Acc 62.26 % AVG Validation Acc 61.64 %\n",
      "Epoch:140/200 AVG Training Loss:0.649 AVG Validation Loss:0.663 AVG Training Acc 62.17 % AVG Validation Acc 61.73 %\n",
      "Epoch:150/200 AVG Training Loss:0.651 AVG Validation Loss:0.663 AVG Training Acc 62.14 % AVG Validation Acc 61.64 %\n",
      "Epoch   158: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.649 AVG Validation Loss:0.663 AVG Training Acc 62.16 % AVG Validation Acc 61.73 %\n",
      "Epoch:170/200 AVG Training Loss:0.649 AVG Validation Loss:0.663 AVG Training Acc 62.20 % AVG Validation Acc 61.64 %\n",
      "Epoch:180/200 AVG Training Loss:0.650 AVG Validation Loss:0.663 AVG Training Acc 62.23 % AVG Validation Acc 61.64 %\n",
      "Epoch   189: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.649 AVG Validation Loss:0.663 AVG Training Acc 62.16 % AVG Validation Acc 61.55 %\n",
      "Epoch:200/200 AVG Training Loss:0.650 AVG Validation Loss:0.663 AVG Training Acc 62.14 % AVG Validation Acc 61.64 %\n",
      "Split 38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c8c6068a484edea8e1b6498fd0da24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.661 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.661 AVG Training Acc 61.82 % AVG Validation Acc 61.91 %\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.657 AVG Validation Loss:0.661 AVG Training Acc 61.98 % AVG Validation Acc 61.55 %\n",
      "Epoch:50/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.91 % AVG Validation Acc 61.55 %\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.661 AVG Training Acc 61.92 % AVG Validation Acc 61.64 %\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.655 AVG Validation Loss:0.662 AVG Training Acc 62.05 % AVG Validation Acc 61.46 %\n",
      "Epoch:80/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.92 % AVG Validation Acc 61.55 %\n",
      "Epoch:90/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 62.06 % AVG Validation Acc 61.73 %\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 62.03 % AVG Validation Acc 61.64 %\n",
      "Epoch:110/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 62.00 % AVG Validation Acc 61.64 %\n",
      "Epoch:120/200 AVG Training Loss:0.653 AVG Validation Loss:0.663 AVG Training Acc 62.14 % AVG Validation Acc 61.73 %\n",
      "Epoch   123: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.653 AVG Validation Loss:0.664 AVG Training Acc 62.04 % AVG Validation Acc 61.55 %\n",
      "Epoch:140/200 AVG Training Loss:0.653 AVG Validation Loss:0.664 AVG Training Acc 61.98 % AVG Validation Acc 61.46 %\n",
      "Epoch:150/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 61.96 % AVG Validation Acc 61.73 %\n",
      "Epoch   154: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.653 AVG Validation Loss:0.663 AVG Training Acc 62.07 % AVG Validation Acc 61.55 %\n",
      "Epoch:170/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.03 % AVG Validation Acc 61.64 %\n",
      "Epoch:180/200 AVG Training Loss:0.655 AVG Validation Loss:0.662 AVG Training Acc 61.91 % AVG Validation Acc 61.73 %\n",
      "Epoch   185: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.653 AVG Validation Loss:0.663 AVG Training Acc 62.09 % AVG Validation Acc 61.73 %\n",
      "Epoch:200/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 61.87 % AVG Validation Acc 61.64 %\n",
      "Split 39\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0fc7df51e3b4e75ae49a08a4009eb9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.663 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.662 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:50/200 AVG Training Loss:0.655 AVG Validation Loss:0.668 AVG Training Acc 61.96 % AVG Validation Acc 61.82 %\n",
      "Epoch:60/200 AVG Training Loss:0.653 AVG Validation Loss:0.670 AVG Training Acc 62.05 % AVG Validation Acc 61.55 %\n",
      "Epoch:70/200 AVG Training Loss:0.651 AVG Validation Loss:0.670 AVG Training Acc 62.10 % AVG Validation Acc 61.55 %\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.650 AVG Validation Loss:0.670 AVG Training Acc 62.21 % AVG Validation Acc 61.55 %\n",
      "Epoch:90/200 AVG Training Loss:0.650 AVG Validation Loss:0.670 AVG Training Acc 62.17 % AVG Validation Acc 61.55 %\n",
      "Epoch:100/200 AVG Training Loss:0.649 AVG Validation Loss:0.671 AVG Training Acc 62.30 % AVG Validation Acc 61.55 %\n",
      "Epoch   104: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:110/200 AVG Training Loss:0.649 AVG Validation Loss:0.670 AVG Training Acc 62.19 % AVG Validation Acc 61.55 %\n",
      "Epoch:120/200 AVG Training Loss:0.649 AVG Validation Loss:0.671 AVG Training Acc 62.15 % AVG Validation Acc 61.55 %\n",
      "Epoch:130/200 AVG Training Loss:0.649 AVG Validation Loss:0.670 AVG Training Acc 62.33 % AVG Validation Acc 61.55 %\n",
      "Epoch   135: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:140/200 AVG Training Loss:0.650 AVG Validation Loss:0.670 AVG Training Acc 62.04 % AVG Validation Acc 61.55 %\n",
      "Epoch:150/200 AVG Training Loss:0.650 AVG Validation Loss:0.670 AVG Training Acc 62.23 % AVG Validation Acc 61.55 %\n",
      "Epoch:160/200 AVG Training Loss:0.650 AVG Validation Loss:0.670 AVG Training Acc 62.19 % AVG Validation Acc 61.55 %\n",
      "Epoch   166: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:170/200 AVG Training Loss:0.649 AVG Validation Loss:0.670 AVG Training Acc 62.21 % AVG Validation Acc 61.55 %\n",
      "Epoch:180/200 AVG Training Loss:0.649 AVG Validation Loss:0.670 AVG Training Acc 62.32 % AVG Validation Acc 61.55 %\n",
      "Epoch:190/200 AVG Training Loss:0.649 AVG Validation Loss:0.670 AVG Training Acc 62.16 % AVG Validation Acc 61.55 %\n",
      "Epoch   197: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:200/200 AVG Training Loss:0.649 AVG Validation Loss:0.670 AVG Training Acc 62.14 % AVG Validation Acc 61.55 %\n",
      "Split 40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aaf345124e0458fa0b5bd5398a6ca26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:70/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:80/200 AVG Training Loss:0.657 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:100/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch:110/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch   113: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.657 AVG Validation Loss:0.661 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch:130/200 AVG Training Loss:0.657 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:140/200 AVG Training Loss:0.656 AVG Validation Loss:0.661 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch   144: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.656 AVG Validation Loss:0.661 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch:160/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:170/200 AVG Training Loss:0.657 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.657 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:190/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:200/200 AVG Training Loss:0.657 AVG Validation Loss:0.661 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Split 41\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926bd13db88c4ac4846abc207dd995d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:60/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.88 % AVG Validation Acc 61.95 %\n",
      "Epoch:70/200 AVG Training Loss:0.657 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:80/200 AVG Training Loss:0.657 AVG Validation Loss:0.661 AVG Training Acc 61.86 % AVG Validation Acc 61.95 %\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:90/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.98 % AVG Validation Acc 61.95 %\n",
      "Epoch:100/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.99 % AVG Validation Acc 62.04 %\n",
      "Epoch:110/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.96 % AVG Validation Acc 62.13 %\n",
      "Epoch   118: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:120/200 AVG Training Loss:0.654 AVG Validation Loss:0.661 AVG Training Acc 62.01 % AVG Validation Acc 62.13 %\n",
      "Epoch:130/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 62.05 % AVG Validation Acc 62.04 %\n",
      "Epoch:140/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.92 % AVG Validation Acc 62.13 %\n",
      "Epoch   149: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:150/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 62.04 %\n",
      "Epoch:160/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.97 % AVG Validation Acc 62.04 %\n",
      "Epoch:170/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.95 % AVG Validation Acc 62.04 %\n",
      "Epoch:180/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.93 % AVG Validation Acc 62.04 %\n",
      "Epoch   180: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:190/200 AVG Training Loss:0.654 AVG Validation Loss:0.661 AVG Training Acc 62.06 % AVG Validation Acc 62.04 %\n",
      "Epoch:200/200 AVG Training Loss:0.655 AVG Validation Loss:0.661 AVG Training Acc 61.87 % AVG Validation Acc 62.13 %\n",
      "Split 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07087a39d2994d08a5ac83a1b15f569b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.661 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:30/200 AVG Training Loss:0.661 AVG Validation Loss:0.661 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.99 % AVG Validation Acc 61.86 %\n",
      "Epoch:50/200 AVG Training Loss:0.655 AVG Validation Loss:0.660 AVG Training Acc 61.95 % AVG Validation Acc 61.95 %\n",
      "Epoch:60/200 AVG Training Loss:0.653 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.653 AVG Validation Loss:0.663 AVG Training Acc 61.97 % AVG Validation Acc 61.95 %\n",
      "Epoch:80/200 AVG Training Loss:0.652 AVG Validation Loss:0.665 AVG Training Acc 61.93 % AVG Validation Acc 62.13 %\n",
      "Epoch:90/200 AVG Training Loss:0.651 AVG Validation Loss:0.665 AVG Training Acc 62.09 % AVG Validation Acc 61.95 %\n",
      "Epoch    96: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.651 AVG Validation Loss:0.667 AVG Training Acc 62.10 % AVG Validation Acc 62.04 %\n",
      "Epoch:110/200 AVG Training Loss:0.651 AVG Validation Loss:0.669 AVG Training Acc 61.94 % AVG Validation Acc 61.95 %\n",
      "Epoch:120/200 AVG Training Loss:0.651 AVG Validation Loss:0.669 AVG Training Acc 62.13 % AVG Validation Acc 61.95 %\n",
      "Epoch   127: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.650 AVG Validation Loss:0.669 AVG Training Acc 62.10 % AVG Validation Acc 61.95 %\n",
      "Epoch:140/200 AVG Training Loss:0.651 AVG Validation Loss:0.668 AVG Training Acc 62.05 % AVG Validation Acc 62.04 %\n",
      "Epoch:150/200 AVG Training Loss:0.650 AVG Validation Loss:0.669 AVG Training Acc 62.08 % AVG Validation Acc 62.13 %\n",
      "Epoch   158: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.652 AVG Validation Loss:0.668 AVG Training Acc 62.12 % AVG Validation Acc 62.22 %\n",
      "Epoch:170/200 AVG Training Loss:0.650 AVG Validation Loss:0.671 AVG Training Acc 62.12 % AVG Validation Acc 62.04 %\n",
      "Epoch:180/200 AVG Training Loss:0.651 AVG Validation Loss:0.669 AVG Training Acc 61.85 % AVG Validation Acc 61.95 %\n",
      "Epoch   189: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.651 AVG Validation Loss:0.670 AVG Training Acc 61.88 % AVG Validation Acc 62.13 %\n",
      "Epoch:200/200 AVG Training Loss:0.652 AVG Validation Loss:0.669 AVG Training Acc 61.84 % AVG Validation Acc 62.22 %\n",
      "Split 43\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6355d1d9c4c94736acab78bed7c2d249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.659 AVG Training Acc 61.78 % AVG Validation Acc 61.86 %\n",
      "Epoch:30/200 AVG Training Loss:0.661 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.660 AVG Validation Loss:0.661 AVG Training Acc 61.92 % AVG Validation Acc 61.86 %\n",
      "Epoch:50/200 AVG Training Loss:0.659 AVG Validation Loss:0.662 AVG Training Acc 61.93 % AVG Validation Acc 61.86 %\n",
      "Epoch:60/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.96 % AVG Validation Acc 61.86 %\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.95 % AVG Validation Acc 61.86 %\n",
      "Epoch:80/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.96 % AVG Validation Acc 61.86 %\n",
      "Epoch:90/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.94 % AVG Validation Acc 61.86 %\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.97 % AVG Validation Acc 61.86 %\n",
      "Epoch:110/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.95 % AVG Validation Acc 61.86 %\n",
      "Epoch:120/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.94 % AVG Validation Acc 61.86 %\n",
      "Epoch   123: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.94 % AVG Validation Acc 61.86 %\n",
      "Epoch:140/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.96 % AVG Validation Acc 61.86 %\n",
      "Epoch:150/200 AVG Training Loss:0.656 AVG Validation Loss:0.662 AVG Training Acc 61.97 % AVG Validation Acc 61.86 %\n",
      "Epoch   154: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.95 % AVG Validation Acc 61.86 %\n",
      "Epoch:170/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.96 % AVG Validation Acc 61.86 %\n",
      "Epoch:180/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.93 % AVG Validation Acc 61.86 %\n",
      "Epoch   185: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:200/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.98 % AVG Validation Acc 61.86 %\n",
      "Split 44\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0f7e9a3b3543b082a57222306d4933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.660 AVG Training Acc 61.87 % AVG Validation Acc 61.86 %\n",
      "Epoch:30/200 AVG Training Loss:0.661 AVG Validation Loss:0.659 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.661 AVG Training Acc 62.07 % AVG Validation Acc 61.32 %\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.661 AVG Training Acc 61.95 % AVG Validation Acc 61.32 %\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 62.15 % AVG Validation Acc 61.23 %\n",
      "Epoch:80/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 62.04 % AVG Validation Acc 61.41 %\n",
      "Epoch:90/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 62.13 % AVG Validation Acc 61.41 %\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.08 % AVG Validation Acc 61.23 %\n",
      "Epoch:110/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.06 % AVG Validation Acc 61.14 %\n",
      "Epoch:120/200 AVG Training Loss:0.653 AVG Validation Loss:0.663 AVG Training Acc 62.13 % AVG Validation Acc 61.32 %\n",
      "Epoch   123: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 62.00 % AVG Validation Acc 61.23 %\n",
      "Epoch:140/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.13 % AVG Validation Acc 61.41 %\n",
      "Epoch:150/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.09 % AVG Validation Acc 61.41 %\n",
      "Epoch   154: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 62.13 % AVG Validation Acc 61.41 %\n",
      "Epoch:170/200 AVG Training Loss:0.655 AVG Validation Loss:0.663 AVG Training Acc 61.96 % AVG Validation Acc 61.41 %\n",
      "Epoch:180/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 62.04 % AVG Validation Acc 61.68 %\n",
      "Epoch   185: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.00 % AVG Validation Acc 61.14 %\n",
      "Epoch:200/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.19 % AVG Validation Acc 61.23 %\n",
      "Split 45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef76b2f535e4f769431b57f4e03497d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.663 AVG Validation Loss:0.659 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.659 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.657 AVG Validation Loss:0.658 AVG Training Acc 61.89 % AVG Validation Acc 61.77 %\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.659 AVG Training Acc 61.94 % AVG Validation Acc 61.77 %\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.654 AVG Validation Loss:0.659 AVG Training Acc 61.93 % AVG Validation Acc 61.77 %\n",
      "Epoch:70/200 AVG Training Loss:0.655 AVG Validation Loss:0.660 AVG Training Acc 61.93 % AVG Validation Acc 61.77 %\n",
      "Epoch:80/200 AVG Training Loss:0.654 AVG Validation Loss:0.660 AVG Training Acc 61.96 % AVG Validation Acc 61.77 %\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.653 AVG Validation Loss:0.660 AVG Training Acc 61.97 % AVG Validation Acc 61.77 %\n",
      "Epoch:100/200 AVG Training Loss:0.654 AVG Validation Loss:0.660 AVG Training Acc 61.91 % AVG Validation Acc 61.77 %\n",
      "Epoch:110/200 AVG Training Loss:0.654 AVG Validation Loss:0.660 AVG Training Acc 61.96 % AVG Validation Acc 61.77 %\n",
      "Epoch   117: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.655 AVG Validation Loss:0.660 AVG Training Acc 62.04 % AVG Validation Acc 61.77 %\n",
      "Epoch:130/200 AVG Training Loss:0.653 AVG Validation Loss:0.660 AVG Training Acc 62.02 % AVG Validation Acc 61.77 %\n",
      "Epoch:140/200 AVG Training Loss:0.653 AVG Validation Loss:0.660 AVG Training Acc 62.01 % AVG Validation Acc 61.77 %\n",
      "Epoch   148: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.654 AVG Validation Loss:0.660 AVG Training Acc 62.00 % AVG Validation Acc 61.77 %\n",
      "Epoch:160/200 AVG Training Loss:0.654 AVG Validation Loss:0.660 AVG Training Acc 62.00 % AVG Validation Acc 61.77 %\n",
      "Epoch:170/200 AVG Training Loss:0.654 AVG Validation Loss:0.660 AVG Training Acc 61.92 % AVG Validation Acc 61.77 %\n",
      "Epoch   179: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.654 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.77 %\n",
      "Epoch:190/200 AVG Training Loss:0.654 AVG Validation Loss:0.660 AVG Training Acc 61.95 % AVG Validation Acc 61.77 %\n",
      "Epoch:200/200 AVG Training Loss:0.654 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.77 %\n",
      "Split 46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773088d725e14ed79f8a48477a9e7e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.661 AVG Validation Loss:0.666 AVG Training Acc 61.85 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.665 AVG Training Acc 61.83 % AVG Validation Acc 61.91 %\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:60/200 AVG Training Loss:0.657 AVG Validation Loss:0.661 AVG Training Acc 61.88 % AVG Validation Acc 62.00 %\n",
      "Epoch:70/200 AVG Training Loss:0.655 AVG Validation Loss:0.659 AVG Training Acc 61.99 % AVG Validation Acc 62.09 %\n",
      "Epoch:80/200 AVG Training Loss:0.653 AVG Validation Loss:0.660 AVG Training Acc 62.10 % AVG Validation Acc 62.09 %\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:90/200 AVG Training Loss:0.651 AVG Validation Loss:0.660 AVG Training Acc 62.09 % AVG Validation Acc 62.09 %\n",
      "Epoch:100/200 AVG Training Loss:0.650 AVG Validation Loss:0.662 AVG Training Acc 62.26 % AVG Validation Acc 62.09 %\n",
      "Epoch:110/200 AVG Training Loss:0.650 AVG Validation Loss:0.661 AVG Training Acc 62.16 % AVG Validation Acc 62.09 %\n",
      "Epoch:120/200 AVG Training Loss:0.650 AVG Validation Loss:0.663 AVG Training Acc 62.08 % AVG Validation Acc 62.00 %\n",
      "Epoch   120: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:130/200 AVG Training Loss:0.650 AVG Validation Loss:0.663 AVG Training Acc 62.15 % AVG Validation Acc 62.00 %\n",
      "Epoch:140/200 AVG Training Loss:0.649 AVG Validation Loss:0.662 AVG Training Acc 62.18 % AVG Validation Acc 62.00 %\n",
      "Epoch:150/200 AVG Training Loss:0.649 AVG Validation Loss:0.663 AVG Training Acc 62.16 % AVG Validation Acc 62.00 %\n",
      "Epoch   151: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:160/200 AVG Training Loss:0.649 AVG Validation Loss:0.663 AVG Training Acc 62.12 % AVG Validation Acc 62.00 %\n",
      "Epoch:170/200 AVG Training Loss:0.649 AVG Validation Loss:0.663 AVG Training Acc 62.08 % AVG Validation Acc 62.00 %\n",
      "Epoch:180/200 AVG Training Loss:0.650 AVG Validation Loss:0.663 AVG Training Acc 62.13 % AVG Validation Acc 62.00 %\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:190/200 AVG Training Loss:0.649 AVG Validation Loss:0.663 AVG Training Acc 62.14 % AVG Validation Acc 62.00 %\n",
      "Epoch:200/200 AVG Training Loss:0.649 AVG Validation Loss:0.663 AVG Training Acc 62.15 % AVG Validation Acc 62.09 %\n",
      "Split 47\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a629e95f1549adbba97fbee92b4578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.660 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.659 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.659 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch:60/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:70/200 AVG Training Loss:0.659 AVG Validation Loss:0.660 AVG Training Acc 61.85 % AVG Validation Acc 61.91 %\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:80/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:90/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:100/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:110/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:120/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch:130/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch   137: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:140/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch:150/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:160/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:170/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:180/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.86 % AVG Validation Acc 61.91 %\n",
      "Epoch:190/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch:200/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Split 48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd251bafec345548b031bf39beaa025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.660 AVG Validation Loss:0.661 AVG Training Acc 61.86 % AVG Validation Acc 61.91 %\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.658 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.82 %\n",
      "Epoch:40/200 AVG Training Loss:0.655 AVG Validation Loss:0.662 AVG Training Acc 61.92 % AVG Validation Acc 61.37 %\n",
      "Epoch:50/200 AVG Training Loss:0.653 AVG Validation Loss:0.664 AVG Training Acc 61.96 % AVG Validation Acc 61.46 %\n",
      "Epoch:60/200 AVG Training Loss:0.652 AVG Validation Loss:0.664 AVG Training Acc 61.97 % AVG Validation Acc 61.19 %\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.651 AVG Validation Loss:0.664 AVG Training Acc 62.12 % AVG Validation Acc 61.46 %\n",
      "Epoch:80/200 AVG Training Loss:0.649 AVG Validation Loss:0.667 AVG Training Acc 62.21 % AVG Validation Acc 61.28 %\n",
      "Epoch:90/200 AVG Training Loss:0.649 AVG Validation Loss:0.666 AVG Training Acc 62.14 % AVG Validation Acc 61.46 %\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.649 AVG Validation Loss:0.666 AVG Training Acc 62.18 % AVG Validation Acc 61.10 %\n",
      "Epoch:110/200 AVG Training Loss:0.649 AVG Validation Loss:0.666 AVG Training Acc 62.24 % AVG Validation Acc 61.28 %\n",
      "Epoch:120/200 AVG Training Loss:0.649 AVG Validation Loss:0.666 AVG Training Acc 61.99 % AVG Validation Acc 61.28 %\n",
      "Epoch   122: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.649 AVG Validation Loss:0.666 AVG Training Acc 62.38 % AVG Validation Acc 61.28 %\n",
      "Epoch:140/200 AVG Training Loss:0.648 AVG Validation Loss:0.666 AVG Training Acc 62.19 % AVG Validation Acc 61.19 %\n",
      "Epoch:150/200 AVG Training Loss:0.648 AVG Validation Loss:0.667 AVG Training Acc 62.11 % AVG Validation Acc 61.28 %\n",
      "Epoch   153: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.649 AVG Validation Loss:0.666 AVG Training Acc 62.20 % AVG Validation Acc 61.28 %\n",
      "Epoch:170/200 AVG Training Loss:0.649 AVG Validation Loss:0.666 AVG Training Acc 62.08 % AVG Validation Acc 61.28 %\n",
      "Epoch:180/200 AVG Training Loss:0.649 AVG Validation Loss:0.666 AVG Training Acc 62.16 % AVG Validation Acc 61.10 %\n",
      "Epoch   184: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.648 AVG Validation Loss:0.666 AVG Training Acc 62.34 % AVG Validation Acc 61.19 %\n",
      "Epoch:200/200 AVG Training Loss:0.649 AVG Validation Loss:0.666 AVG Training Acc 62.05 % AVG Validation Acc 61.19 %\n",
      "Split 49\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c0f6d999c147318d717ed29efece08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.661 AVG Validation Loss:0.666 AVG Training Acc 61.86 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.660 AVG Validation Loss:0.667 AVG Training Acc 61.84 % AVG Validation Acc 61.91 %\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.657 AVG Validation Loss:0.666 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.655 AVG Validation Loss:0.668 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.651 AVG Validation Loss:0.672 AVG Training Acc 62.06 % AVG Validation Acc 61.91 %\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.649 AVG Validation Loss:0.675 AVG Training Acc 62.17 % AVG Validation Acc 61.55 %\n",
      "Epoch:70/200 AVG Training Loss:0.648 AVG Validation Loss:0.676 AVG Training Acc 62.18 % AVG Validation Acc 61.55 %\n",
      "Epoch:80/200 AVG Training Loss:0.648 AVG Validation Loss:0.677 AVG Training Acc 62.16 % AVG Validation Acc 61.46 %\n",
      "Epoch    88: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.647 AVG Validation Loss:0.675 AVG Training Acc 62.21 % AVG Validation Acc 61.46 %\n",
      "Epoch:100/200 AVG Training Loss:0.647 AVG Validation Loss:0.677 AVG Training Acc 62.24 % AVG Validation Acc 61.19 %\n",
      "Epoch:110/200 AVG Training Loss:0.647 AVG Validation Loss:0.679 AVG Training Acc 62.25 % AVG Validation Acc 61.37 %\n",
      "Epoch   119: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.646 AVG Validation Loss:0.680 AVG Training Acc 62.21 % AVG Validation Acc 61.64 %\n",
      "Epoch:130/200 AVG Training Loss:0.648 AVG Validation Loss:0.678 AVG Training Acc 62.16 % AVG Validation Acc 61.37 %\n",
      "Epoch:140/200 AVG Training Loss:0.647 AVG Validation Loss:0.678 AVG Training Acc 62.29 % AVG Validation Acc 61.37 %\n",
      "Epoch:150/200 AVG Training Loss:0.647 AVG Validation Loss:0.679 AVG Training Acc 62.12 % AVG Validation Acc 61.10 %\n",
      "Epoch   150: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.646 AVG Validation Loss:0.678 AVG Training Acc 62.21 % AVG Validation Acc 61.64 %\n",
      "Epoch:170/200 AVG Training Loss:0.648 AVG Validation Loss:0.678 AVG Training Acc 62.33 % AVG Validation Acc 61.46 %\n",
      "Epoch:180/200 AVG Training Loss:0.648 AVG Validation Loss:0.677 AVG Training Acc 62.07 % AVG Validation Acc 61.55 %\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.646 AVG Validation Loss:0.678 AVG Training Acc 62.30 % AVG Validation Acc 61.55 %\n",
      "Epoch:200/200 AVG Training Loss:0.646 AVG Validation Loss:0.679 AVG Training Acc 62.19 % AVG Validation Acc 61.73 %\n",
      "Split 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80fe72a2193b4804b7798ecef6e9a892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.658 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.659 AVG Validation Loss:0.658 AVG Training Acc 61.85 % AVG Validation Acc 61.91 %\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.658 AVG Validation Loss:0.659 AVG Training Acc 61.88 % AVG Validation Acc 61.64 %\n",
      "Epoch:70/200 AVG Training Loss:0.657 AVG Validation Loss:0.659 AVG Training Acc 61.88 % AVG Validation Acc 61.64 %\n",
      "Epoch:80/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.84 % AVG Validation Acc 61.73 %\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.86 % AVG Validation Acc 61.73 %\n",
      "Epoch:100/200 AVG Training Loss:0.657 AVG Validation Loss:0.659 AVG Training Acc 61.79 % AVG Validation Acc 61.82 %\n",
      "Epoch:110/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.95 % AVG Validation Acc 61.64 %\n",
      "Epoch   116: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.87 % AVG Validation Acc 61.73 %\n",
      "Epoch:130/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 61.95 % AVG Validation Acc 61.82 %\n",
      "Epoch:140/200 AVG Training Loss:0.657 AVG Validation Loss:0.659 AVG Training Acc 61.95 % AVG Validation Acc 61.82 %\n",
      "Epoch   147: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.93 % AVG Validation Acc 61.64 %\n",
      "Epoch:160/200 AVG Training Loss:0.658 AVG Validation Loss:0.659 AVG Training Acc 61.81 % AVG Validation Acc 61.82 %\n",
      "Epoch:170/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 61.80 % AVG Validation Acc 61.82 %\n",
      "Epoch   178: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.64 %\n",
      "Epoch:190/200 AVG Training Loss:0.657 AVG Validation Loss:0.659 AVG Training Acc 61.90 % AVG Validation Acc 61.73 %\n",
      "Epoch:200/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.79 % AVG Validation Acc 61.82 %\n",
      "Split 51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a432bf1a735943c38944f449fe581cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.95 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.658 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.658 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.658 AVG Training Acc 61.86 % AVG Validation Acc 62.04 %\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 61.92 % AVG Validation Acc 62.04 %\n",
      "Epoch:70/200 AVG Training Loss:0.655 AVG Validation Loss:0.659 AVG Training Acc 61.98 % AVG Validation Acc 62.04 %\n",
      "Epoch:80/200 AVG Training Loss:0.655 AVG Validation Loss:0.659 AVG Training Acc 61.98 % AVG Validation Acc 62.13 %\n",
      "Epoch    88: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.655 AVG Validation Loss:0.659 AVG Training Acc 62.00 % AVG Validation Acc 62.04 %\n",
      "Epoch:100/200 AVG Training Loss:0.655 AVG Validation Loss:0.660 AVG Training Acc 61.91 % AVG Validation Acc 62.04 %\n",
      "Epoch:110/200 AVG Training Loss:0.655 AVG Validation Loss:0.660 AVG Training Acc 62.01 % AVG Validation Acc 62.13 %\n",
      "Epoch   119: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.655 AVG Validation Loss:0.659 AVG Training Acc 62.04 % AVG Validation Acc 62.04 %\n",
      "Epoch:130/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 62.03 % AVG Validation Acc 61.95 %\n",
      "Epoch:140/200 AVG Training Loss:0.654 AVG Validation Loss:0.659 AVG Training Acc 62.03 % AVG Validation Acc 61.95 %\n",
      "Epoch:150/200 AVG Training Loss:0.655 AVG Validation Loss:0.660 AVG Training Acc 62.01 % AVG Validation Acc 61.95 %\n",
      "Epoch   150: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.655 AVG Validation Loss:0.660 AVG Training Acc 61.98 % AVG Validation Acc 62.13 %\n",
      "Epoch:170/200 AVG Training Loss:0.655 AVG Validation Loss:0.660 AVG Training Acc 61.93 % AVG Validation Acc 62.04 %\n",
      "Epoch:180/200 AVG Training Loss:0.655 AVG Validation Loss:0.660 AVG Training Acc 62.00 % AVG Validation Acc 62.13 %\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.654 AVG Validation Loss:0.660 AVG Training Acc 62.03 % AVG Validation Acc 62.04 %\n",
      "Epoch:200/200 AVG Training Loss:0.655 AVG Validation Loss:0.659 AVG Training Acc 62.01 % AVG Validation Acc 62.13 %\n",
      "Split 52\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb95dc285a37450887791203732cf874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.661 AVG Validation Loss:0.664 AVG Training Acc 61.84 % AVG Validation Acc 61.86 %\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.659 AVG Validation Loss:0.664 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/200 AVG Training Loss:0.657 AVG Validation Loss:0.666 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:60/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:70/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:80/200 AVG Training Loss:0.657 AVG Validation Loss:0.666 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:90/200 AVG Training Loss:0.657 AVG Validation Loss:0.666 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:100/200 AVG Training Loss:0.657 AVG Validation Loss:0.666 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch   109: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:110/200 AVG Training Loss:0.658 AVG Validation Loss:0.666 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:120/200 AVG Training Loss:0.657 AVG Validation Loss:0.666 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:130/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:140/200 AVG Training Loss:0.658 AVG Validation Loss:0.666 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch   140: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.657 AVG Validation Loss:0.666 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:160/200 AVG Training Loss:0.657 AVG Validation Loss:0.666 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:170/200 AVG Training Loss:0.657 AVG Validation Loss:0.666 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.657 AVG Validation Loss:0.666 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:190/200 AVG Training Loss:0.657 AVG Validation Loss:0.666 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:200/200 AVG Training Loss:0.658 AVG Validation Loss:0.666 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Split 53\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9962e4f92d0846b49b72b2770c8fc6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.657 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.657 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.656 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.656 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.655 AVG Training Acc 61.94 % AVG Validation Acc 62.04 %\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.655 AVG Training Acc 61.99 % AVG Validation Acc 61.86 %\n",
      "Epoch:70/200 AVG Training Loss:0.657 AVG Validation Loss:0.655 AVG Training Acc 61.84 % AVG Validation Acc 61.95 %\n",
      "Epoch:80/200 AVG Training Loss:0.656 AVG Validation Loss:0.656 AVG Training Acc 61.84 % AVG Validation Acc 61.95 %\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.655 AVG Validation Loss:0.656 AVG Training Acc 61.81 % AVG Validation Acc 61.95 %\n",
      "Epoch:100/200 AVG Training Loss:0.657 AVG Validation Loss:0.656 AVG Training Acc 61.91 % AVG Validation Acc 61.95 %\n",
      "Epoch:110/200 AVG Training Loss:0.656 AVG Validation Loss:0.656 AVG Training Acc 61.87 % AVG Validation Acc 61.86 %\n",
      "Epoch:120/200 AVG Training Loss:0.656 AVG Validation Loss:0.656 AVG Training Acc 61.85 % AVG Validation Acc 61.86 %\n",
      "Epoch   120: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.656 AVG Validation Loss:0.656 AVG Training Acc 61.92 % AVG Validation Acc 61.95 %\n",
      "Epoch:140/200 AVG Training Loss:0.656 AVG Validation Loss:0.656 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:150/200 AVG Training Loss:0.656 AVG Validation Loss:0.656 AVG Training Acc 61.91 % AVG Validation Acc 61.77 %\n",
      "Epoch   151: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.656 AVG Validation Loss:0.656 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:170/200 AVG Training Loss:0.656 AVG Validation Loss:0.656 AVG Training Acc 61.92 % AVG Validation Acc 61.86 %\n",
      "Epoch:180/200 AVG Training Loss:0.656 AVG Validation Loss:0.656 AVG Training Acc 61.95 % AVG Validation Acc 61.86 %\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.656 AVG Validation Loss:0.656 AVG Training Acc 61.86 % AVG Validation Acc 61.86 %\n",
      "Epoch:200/200 AVG Training Loss:0.656 AVG Validation Loss:0.656 AVG Training Acc 61.85 % AVG Validation Acc 61.86 %\n",
      "Split 54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687ad34624464666a03f66627b39936a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.662 AVG Training Acc 61.88 % AVG Validation Acc 61.86 %\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.664 AVG Training Acc 61.88 % AVG Validation Acc 61.86 %\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.657 AVG Validation Loss:0.663 AVG Training Acc 61.90 % AVG Validation Acc 61.77 %\n",
      "Epoch:50/200 AVG Training Loss:0.655 AVG Validation Loss:0.678 AVG Training Acc 61.91 % AVG Validation Acc 61.59 %\n",
      "Epoch:60/200 AVG Training Loss:0.654 AVG Validation Loss:0.682 AVG Training Acc 61.99 % AVG Validation Acc 61.77 %\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.651 AVG Validation Loss:0.679 AVG Training Acc 61.98 % AVG Validation Acc 61.41 %\n",
      "Epoch:80/200 AVG Training Loss:0.650 AVG Validation Loss:0.677 AVG Training Acc 62.08 % AVG Validation Acc 61.23 %\n",
      "Epoch:90/200 AVG Training Loss:0.651 AVG Validation Loss:0.679 AVG Training Acc 62.08 % AVG Validation Acc 61.14 %\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.651 AVG Validation Loss:0.681 AVG Training Acc 62.25 % AVG Validation Acc 61.32 %\n",
      "Epoch:110/200 AVG Training Loss:0.650 AVG Validation Loss:0.680 AVG Training Acc 62.11 % AVG Validation Acc 61.23 %\n",
      "Epoch:120/200 AVG Training Loss:0.649 AVG Validation Loss:0.680 AVG Training Acc 62.00 % AVG Validation Acc 61.14 %\n",
      "Epoch   123: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.651 AVG Validation Loss:0.682 AVG Training Acc 62.15 % AVG Validation Acc 61.23 %\n",
      "Epoch:140/200 AVG Training Loss:0.651 AVG Validation Loss:0.681 AVG Training Acc 62.11 % AVG Validation Acc 61.32 %\n",
      "Epoch:150/200 AVG Training Loss:0.651 AVG Validation Loss:0.682 AVG Training Acc 62.11 % AVG Validation Acc 61.23 %\n",
      "Epoch   154: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.651 AVG Validation Loss:0.679 AVG Training Acc 61.98 % AVG Validation Acc 61.14 %\n",
      "Epoch:170/200 AVG Training Loss:0.650 AVG Validation Loss:0.680 AVG Training Acc 62.04 % AVG Validation Acc 61.14 %\n",
      "Epoch:180/200 AVG Training Loss:0.651 AVG Validation Loss:0.680 AVG Training Acc 62.16 % AVG Validation Acc 61.41 %\n",
      "Epoch   185: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.651 AVG Validation Loss:0.681 AVG Training Acc 62.08 % AVG Validation Acc 61.41 %\n",
      "Epoch:200/200 AVG Training Loss:0.651 AVG Validation Loss:0.680 AVG Training Acc 62.04 % AVG Validation Acc 61.23 %\n",
      "Split 55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdacaa836fe48448890b94b720428a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.661 AVG Validation Loss:0.663 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.659 AVG Validation Loss:0.663 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.664 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.667 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/200 AVG Training Loss:0.657 AVG Validation Loss:0.667 AVG Training Acc 61.94 % AVG Validation Acc 61.86 %\n",
      "Epoch:60/200 AVG Training Loss:0.657 AVG Validation Loss:0.668 AVG Training Acc 61.93 % AVG Validation Acc 61.86 %\n",
      "Epoch:70/200 AVG Training Loss:0.658 AVG Validation Loss:0.668 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:80/200 AVG Training Loss:0.657 AVG Validation Loss:0.669 AVG Training Acc 61.97 % AVG Validation Acc 61.86 %\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.657 AVG Validation Loss:0.669 AVG Training Acc 61.97 % AVG Validation Acc 61.86 %\n",
      "Epoch:100/200 AVG Training Loss:0.657 AVG Validation Loss:0.669 AVG Training Acc 61.99 % AVG Validation Acc 61.86 %\n",
      "Epoch:110/200 AVG Training Loss:0.657 AVG Validation Loss:0.669 AVG Training Acc 61.92 % AVG Validation Acc 61.86 %\n",
      "Epoch   111: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.657 AVG Validation Loss:0.669 AVG Training Acc 61.99 % AVG Validation Acc 61.86 %\n",
      "Epoch:130/200 AVG Training Loss:0.657 AVG Validation Loss:0.669 AVG Training Acc 61.98 % AVG Validation Acc 61.86 %\n",
      "Epoch:140/200 AVG Training Loss:0.657 AVG Validation Loss:0.668 AVG Training Acc 61.95 % AVG Validation Acc 61.86 %\n",
      "Epoch   142: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.657 AVG Validation Loss:0.669 AVG Training Acc 61.99 % AVG Validation Acc 61.86 %\n",
      "Epoch:160/200 AVG Training Loss:0.657 AVG Validation Loss:0.669 AVG Training Acc 61.97 % AVG Validation Acc 61.86 %\n",
      "Epoch:170/200 AVG Training Loss:0.657 AVG Validation Loss:0.669 AVG Training Acc 61.96 % AVG Validation Acc 61.86 %\n",
      "Epoch   173: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.657 AVG Validation Loss:0.669 AVG Training Acc 61.92 % AVG Validation Acc 61.86 %\n",
      "Epoch:190/200 AVG Training Loss:0.656 AVG Validation Loss:0.669 AVG Training Acc 61.94 % AVG Validation Acc 61.86 %\n",
      "Epoch:200/200 AVG Training Loss:0.657 AVG Validation Loss:0.669 AVG Training Acc 61.96 % AVG Validation Acc 61.86 %\n",
      "Split 56\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93445b174a504fdb95c7f5e6327d6736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.663 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.93 % AVG Validation Acc 61.91 %\n",
      "Epoch:70/200 AVG Training Loss:0.657 AVG Validation Loss:0.661 AVG Training Acc 61.82 % AVG Validation Acc 61.91 %\n",
      "Epoch:80/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 62.00 % AVG Validation Acc 61.91 %\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Epoch:100/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:110/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch   113: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.95 % AVG Validation Acc 61.91 %\n",
      "Epoch:130/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch:140/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch   144: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch:160/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:170/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.85 % AVG Validation Acc 61.91 %\n",
      "Epoch:190/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch:200/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Split 57\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf9a38f6a194302ad0cc964a4443224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.663 AVG Validation Loss:0.658 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.662 AVG Validation Loss:0.657 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.656 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.659 AVG Validation Loss:0.656 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Epoch:60/200 AVG Training Loss:0.657 AVG Validation Loss:0.656 AVG Training Acc 61.92 % AVG Validation Acc 61.82 %\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.656 AVG Validation Loss:0.656 AVG Training Acc 61.96 % AVG Validation Acc 61.82 %\n",
      "Epoch:80/200 AVG Training Loss:0.656 AVG Validation Loss:0.658 AVG Training Acc 61.94 % AVG Validation Acc 61.82 %\n",
      "Epoch:90/200 AVG Training Loss:0.656 AVG Validation Loss:0.658 AVG Training Acc 61.93 % AVG Validation Acc 61.82 %\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 61.93 % AVG Validation Acc 61.82 %\n",
      "Epoch:110/200 AVG Training Loss:0.655 AVG Validation Loss:0.659 AVG Training Acc 61.93 % AVG Validation Acc 61.82 %\n",
      "Epoch:120/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 61.97 % AVG Validation Acc 61.82 %\n",
      "Epoch   123: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 61.95 % AVG Validation Acc 61.82 %\n",
      "Epoch:140/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 61.97 % AVG Validation Acc 61.82 %\n",
      "Epoch:150/200 AVG Training Loss:0.655 AVG Validation Loss:0.659 AVG Training Acc 62.00 % AVG Validation Acc 61.82 %\n",
      "Epoch   154: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 61.92 % AVG Validation Acc 61.82 %\n",
      "Epoch:170/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 61.98 % AVG Validation Acc 61.82 %\n",
      "Epoch:180/200 AVG Training Loss:0.655 AVG Validation Loss:0.659 AVG Training Acc 61.95 % AVG Validation Acc 61.82 %\n",
      "Epoch   185: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.655 AVG Validation Loss:0.659 AVG Training Acc 61.96 % AVG Validation Acc 61.82 %\n",
      "Epoch:200/200 AVG Training Loss:0.656 AVG Validation Loss:0.659 AVG Training Acc 61.97 % AVG Validation Acc 61.82 %\n",
      "Split 58\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32cea6c7c44646fe83728a48d2434d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.663 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.659 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.655 AVG Validation Loss:0.663 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.653 AVG Validation Loss:0.665 AVG Training Acc 61.95 % AVG Validation Acc 61.73 %\n",
      "Epoch:60/200 AVG Training Loss:0.650 AVG Validation Loss:0.667 AVG Training Acc 62.15 % AVG Validation Acc 61.64 %\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.648 AVG Validation Loss:0.669 AVG Training Acc 62.26 % AVG Validation Acc 61.82 %\n",
      "Epoch:80/200 AVG Training Loss:0.647 AVG Validation Loss:0.669 AVG Training Acc 61.93 % AVG Validation Acc 62.09 %\n",
      "Epoch:90/200 AVG Training Loss:0.647 AVG Validation Loss:0.670 AVG Training Acc 61.93 % AVG Validation Acc 61.82 %\n",
      "Epoch    98: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.645 AVG Validation Loss:0.672 AVG Training Acc 62.24 % AVG Validation Acc 62.00 %\n",
      "Epoch:110/200 AVG Training Loss:0.646 AVG Validation Loss:0.671 AVG Training Acc 62.15 % AVG Validation Acc 61.73 %\n",
      "Epoch:120/200 AVG Training Loss:0.645 AVG Validation Loss:0.671 AVG Training Acc 62.02 % AVG Validation Acc 61.64 %\n",
      "Epoch   129: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.645 AVG Validation Loss:0.671 AVG Training Acc 62.43 % AVG Validation Acc 61.64 %\n",
      "Epoch:140/200 AVG Training Loss:0.645 AVG Validation Loss:0.672 AVG Training Acc 62.00 % AVG Validation Acc 61.73 %\n",
      "Epoch:150/200 AVG Training Loss:0.645 AVG Validation Loss:0.671 AVG Training Acc 62.29 % AVG Validation Acc 61.64 %\n",
      "Epoch:160/200 AVG Training Loss:0.645 AVG Validation Loss:0.672 AVG Training Acc 62.09 % AVG Validation Acc 61.73 %\n",
      "Epoch   160: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:170/200 AVG Training Loss:0.646 AVG Validation Loss:0.672 AVG Training Acc 62.16 % AVG Validation Acc 61.82 %\n",
      "Epoch:180/200 AVG Training Loss:0.645 AVG Validation Loss:0.671 AVG Training Acc 62.19 % AVG Validation Acc 61.82 %\n",
      "Epoch:190/200 AVG Training Loss:0.646 AVG Validation Loss:0.672 AVG Training Acc 62.17 % AVG Validation Acc 61.91 %\n",
      "Epoch   191: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:200/200 AVG Training Loss:0.645 AVG Validation Loss:0.672 AVG Training Acc 62.22 % AVG Validation Acc 61.82 %\n",
      "Split 59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682d1fc174da4cb9bf74404da0af1896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.661 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.666 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.666 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.664 AVG Training Acc 61.79 % AVG Validation Acc 61.91 %\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:50/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 62.01 % AVG Validation Acc 61.37 %\n",
      "Epoch:60/200 AVG Training Loss:0.653 AVG Validation Loss:0.662 AVG Training Acc 61.95 % AVG Validation Acc 62.09 %\n",
      "Epoch:70/200 AVG Training Loss:0.651 AVG Validation Loss:0.664 AVG Training Acc 62.16 % AVG Validation Acc 61.64 %\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.649 AVG Validation Loss:0.665 AVG Training Acc 62.18 % AVG Validation Acc 61.64 %\n",
      "Epoch:90/200 AVG Training Loss:0.648 AVG Validation Loss:0.665 AVG Training Acc 62.36 % AVG Validation Acc 61.73 %\n",
      "Epoch:100/200 AVG Training Loss:0.648 AVG Validation Loss:0.666 AVG Training Acc 62.18 % AVG Validation Acc 61.55 %\n",
      "Epoch   104: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:110/200 AVG Training Loss:0.647 AVG Validation Loss:0.666 AVG Training Acc 62.17 % AVG Validation Acc 61.64 %\n",
      "Epoch:120/200 AVG Training Loss:0.648 AVG Validation Loss:0.666 AVG Training Acc 62.32 % AVG Validation Acc 61.46 %\n",
      "Epoch:130/200 AVG Training Loss:0.647 AVG Validation Loss:0.666 AVG Training Acc 62.37 % AVG Validation Acc 61.37 %\n",
      "Epoch   135: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:140/200 AVG Training Loss:0.647 AVG Validation Loss:0.666 AVG Training Acc 62.17 % AVG Validation Acc 61.46 %\n",
      "Epoch:150/200 AVG Training Loss:0.648 AVG Validation Loss:0.666 AVG Training Acc 62.18 % AVG Validation Acc 61.73 %\n",
      "Epoch:160/200 AVG Training Loss:0.647 AVG Validation Loss:0.667 AVG Training Acc 62.24 % AVG Validation Acc 61.46 %\n",
      "Epoch   166: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:170/200 AVG Training Loss:0.648 AVG Validation Loss:0.666 AVG Training Acc 62.37 % AVG Validation Acc 61.64 %\n",
      "Epoch:180/200 AVG Training Loss:0.647 AVG Validation Loss:0.667 AVG Training Acc 62.31 % AVG Validation Acc 61.55 %\n",
      "Epoch:190/200 AVG Training Loss:0.648 AVG Validation Loss:0.666 AVG Training Acc 62.10 % AVG Validation Acc 61.55 %\n",
      "Epoch   197: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:200/200 AVG Training Loss:0.648 AVG Validation Loss:0.666 AVG Training Acc 62.19 % AVG Validation Acc 61.55 %\n",
      "Split 60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc34d2b7fa7043ceb24633cf128ccbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.664 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.665 AVG Training Acc 61.82 % AVG Validation Acc 61.91 %\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.82 %\n",
      "Epoch:50/200 AVG Training Loss:0.654 AVG Validation Loss:0.667 AVG Training Acc 61.94 % AVG Validation Acc 61.46 %\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.653 AVG Validation Loss:0.667 AVG Training Acc 62.04 % AVG Validation Acc 61.46 %\n",
      "Epoch:70/200 AVG Training Loss:0.651 AVG Validation Loss:0.668 AVG Training Acc 62.13 % AVG Validation Acc 61.46 %\n",
      "Epoch:80/200 AVG Training Loss:0.651 AVG Validation Loss:0.669 AVG Training Acc 62.17 % AVG Validation Acc 61.46 %\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.651 AVG Validation Loss:0.670 AVG Training Acc 62.14 % AVG Validation Acc 61.46 %\n",
      "Epoch:100/200 AVG Training Loss:0.651 AVG Validation Loss:0.670 AVG Training Acc 62.10 % AVG Validation Acc 61.46 %\n",
      "Epoch:110/200 AVG Training Loss:0.652 AVG Validation Loss:0.670 AVG Training Acc 62.20 % AVG Validation Acc 61.46 %\n",
      "Epoch:120/200 AVG Training Loss:0.651 AVG Validation Loss:0.669 AVG Training Acc 62.14 % AVG Validation Acc 61.46 %\n",
      "Epoch   120: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.651 AVG Validation Loss:0.669 AVG Training Acc 62.17 % AVG Validation Acc 61.46 %\n",
      "Epoch:140/200 AVG Training Loss:0.651 AVG Validation Loss:0.670 AVG Training Acc 62.04 % AVG Validation Acc 61.46 %\n",
      "Epoch:150/200 AVG Training Loss:0.650 AVG Validation Loss:0.670 AVG Training Acc 62.07 % AVG Validation Acc 61.46 %\n",
      "Epoch   151: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.650 AVG Validation Loss:0.669 AVG Training Acc 62.07 % AVG Validation Acc 61.46 %\n",
      "Epoch:170/200 AVG Training Loss:0.651 AVG Validation Loss:0.670 AVG Training Acc 62.17 % AVG Validation Acc 61.46 %\n",
      "Epoch:180/200 AVG Training Loss:0.651 AVG Validation Loss:0.670 AVG Training Acc 62.17 % AVG Validation Acc 61.46 %\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.652 AVG Validation Loss:0.670 AVG Training Acc 62.12 % AVG Validation Acc 61.46 %\n",
      "Epoch:200/200 AVG Training Loss:0.650 AVG Validation Loss:0.670 AVG Training Acc 62.18 % AVG Validation Acc 61.46 %\n",
      "Split 61\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b139535cdce442f59486969695672e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.661 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.663 AVG Training Acc 61.88 % AVG Validation Acc 61.95 %\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.664 AVG Training Acc 61.88 % AVG Validation Acc 61.95 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.666 AVG Training Acc 61.86 % AVG Validation Acc 61.95 %\n",
      "Epoch:50/200 AVG Training Loss:0.657 AVG Validation Loss:0.666 AVG Training Acc 61.92 % AVG Validation Acc 61.95 %\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.666 AVG Training Acc 61.90 % AVG Validation Acc 61.95 %\n",
      "Epoch:70/200 AVG Training Loss:0.656 AVG Validation Loss:0.666 AVG Training Acc 61.88 % AVG Validation Acc 61.95 %\n",
      "Epoch:80/200 AVG Training Loss:0.656 AVG Validation Loss:0.666 AVG Training Acc 61.96 % AVG Validation Acc 61.95 %\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.656 AVG Validation Loss:0.666 AVG Training Acc 61.98 % AVG Validation Acc 61.95 %\n",
      "Epoch:100/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 61.97 % AVG Validation Acc 61.95 %\n",
      "Epoch:110/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 61.97 % AVG Validation Acc 61.95 %\n",
      "Epoch   114: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.656 AVG Validation Loss:0.666 AVG Training Acc 61.85 % AVG Validation Acc 61.95 %\n",
      "Epoch:130/200 AVG Training Loss:0.656 AVG Validation Loss:0.666 AVG Training Acc 61.94 % AVG Validation Acc 61.95 %\n",
      "Epoch:140/200 AVG Training Loss:0.656 AVG Validation Loss:0.666 AVG Training Acc 61.84 % AVG Validation Acc 61.95 %\n",
      "Epoch   145: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.656 AVG Validation Loss:0.666 AVG Training Acc 61.94 % AVG Validation Acc 61.95 %\n",
      "Epoch:160/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 61.95 % AVG Validation Acc 61.95 %\n",
      "Epoch:170/200 AVG Training Loss:0.656 AVG Validation Loss:0.666 AVG Training Acc 61.92 % AVG Validation Acc 61.95 %\n",
      "Epoch   176: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.656 AVG Validation Loss:0.666 AVG Training Acc 61.99 % AVG Validation Acc 61.95 %\n",
      "Epoch:190/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 61.92 % AVG Validation Acc 61.95 %\n",
      "Epoch:200/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 61.95 % AVG Validation Acc 61.95 %\n",
      "Split 62\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4eb1e37d59478c99a6a624ccd9900f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.662 AVG Training Acc 61.88 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.663 AVG Training Acc 61.88 % AVG Validation Acc 61.86 %\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.77 %\n",
      "Epoch:50/200 AVG Training Loss:0.657 AVG Validation Loss:0.663 AVG Training Acc 61.88 % AVG Validation Acc 61.68 %\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.663 AVG Training Acc 61.95 % AVG Validation Acc 61.59 %\n",
      "Epoch:70/200 AVG Training Loss:0.656 AVG Validation Loss:0.663 AVG Training Acc 61.91 % AVG Validation Acc 61.77 %\n",
      "Epoch:80/200 AVG Training Loss:0.656 AVG Validation Loss:0.663 AVG Training Acc 61.85 % AVG Validation Acc 61.68 %\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.655 AVG Validation Loss:0.663 AVG Training Acc 61.97 % AVG Validation Acc 61.68 %\n",
      "Epoch:100/200 AVG Training Loss:0.655 AVG Validation Loss:0.663 AVG Training Acc 61.92 % AVG Validation Acc 61.68 %\n",
      "Epoch:110/200 AVG Training Loss:0.656 AVG Validation Loss:0.663 AVG Training Acc 61.77 % AVG Validation Acc 61.77 %\n",
      "Epoch   114: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.656 AVG Validation Loss:0.663 AVG Training Acc 61.86 % AVG Validation Acc 61.77 %\n",
      "Epoch:130/200 AVG Training Loss:0.656 AVG Validation Loss:0.663 AVG Training Acc 61.84 % AVG Validation Acc 61.68 %\n",
      "Epoch:140/200 AVG Training Loss:0.656 AVG Validation Loss:0.663 AVG Training Acc 62.01 % AVG Validation Acc 61.68 %\n",
      "Epoch   145: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.656 AVG Validation Loss:0.663 AVG Training Acc 61.88 % AVG Validation Acc 61.77 %\n",
      "Epoch:160/200 AVG Training Loss:0.656 AVG Validation Loss:0.663 AVG Training Acc 61.74 % AVG Validation Acc 61.59 %\n",
      "Epoch:170/200 AVG Training Loss:0.655 AVG Validation Loss:0.663 AVG Training Acc 61.86 % AVG Validation Acc 61.68 %\n",
      "Epoch   176: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.656 AVG Validation Loss:0.663 AVG Training Acc 61.78 % AVG Validation Acc 61.77 %\n",
      "Epoch:190/200 AVG Training Loss:0.656 AVG Validation Loss:0.663 AVG Training Acc 61.91 % AVG Validation Acc 61.77 %\n",
      "Epoch:200/200 AVG Training Loss:0.656 AVG Validation Loss:0.663 AVG Training Acc 61.97 % AVG Validation Acc 61.59 %\n",
      "Split 63\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec8a395b0494c19ababc0e4c537f16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.661 AVG Validation Loss:0.659 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:60/200 AVG Training Loss:0.658 AVG Validation Loss:0.661 AVG Training Acc 61.92 % AVG Validation Acc 61.86 %\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:80/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:90/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.92 % AVG Validation Acc 61.86 %\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:110/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:120/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.94 % AVG Validation Acc 61.86 %\n",
      "Epoch   122: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:140/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:150/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.92 % AVG Validation Acc 61.86 %\n",
      "Epoch   153: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:170/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:180/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.92 % AVG Validation Acc 61.86 %\n",
      "Epoch   184: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:200/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.88 % AVG Validation Acc 61.86 %\n",
      "Split 64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6958a7280de145e1a5039b556226ecb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.663 AVG Validation Loss:0.657 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.655 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:30/200 AVG Training Loss:0.661 AVG Validation Loss:0.657 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.660 AVG Validation Loss:0.655 AVG Training Acc 61.86 % AVG Validation Acc 61.86 %\n",
      "Epoch:50/200 AVG Training Loss:0.659 AVG Validation Loss:0.654 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:60/200 AVG Training Loss:0.657 AVG Validation Loss:0.653 AVG Training Acc 61.93 % AVG Validation Acc 61.86 %\n",
      "Epoch:70/200 AVG Training Loss:0.655 AVG Validation Loss:0.650 AVG Training Acc 61.95 % AVG Validation Acc 61.95 %\n",
      "Epoch:80/200 AVG Training Loss:0.654 AVG Validation Loss:0.650 AVG Training Acc 62.01 % AVG Validation Acc 62.13 %\n",
      "Epoch:90/200 AVG Training Loss:0.652 AVG Validation Loss:0.649 AVG Training Acc 62.21 % AVG Validation Acc 61.95 %\n",
      "Epoch    94: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:100/200 AVG Training Loss:0.652 AVG Validation Loss:0.649 AVG Training Acc 62.21 % AVG Validation Acc 62.04 %\n",
      "Epoch:110/200 AVG Training Loss:0.651 AVG Validation Loss:0.649 AVG Training Acc 62.23 % AVG Validation Acc 61.95 %\n",
      "Epoch:120/200 AVG Training Loss:0.651 AVG Validation Loss:0.649 AVG Training Acc 62.26 % AVG Validation Acc 61.95 %\n",
      "Epoch   125: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:130/200 AVG Training Loss:0.650 AVG Validation Loss:0.648 AVG Training Acc 62.26 % AVG Validation Acc 61.95 %\n",
      "Epoch:140/200 AVG Training Loss:0.650 AVG Validation Loss:0.649 AVG Training Acc 62.10 % AVG Validation Acc 61.86 %\n",
      "Epoch:150/200 AVG Training Loss:0.650 AVG Validation Loss:0.649 AVG Training Acc 62.08 % AVG Validation Acc 61.95 %\n",
      "Epoch   156: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:160/200 AVG Training Loss:0.650 AVG Validation Loss:0.649 AVG Training Acc 62.25 % AVG Validation Acc 61.86 %\n",
      "Epoch:170/200 AVG Training Loss:0.650 AVG Validation Loss:0.648 AVG Training Acc 62.22 % AVG Validation Acc 61.95 %\n",
      "Epoch:180/200 AVG Training Loss:0.650 AVG Validation Loss:0.649 AVG Training Acc 62.28 % AVG Validation Acc 61.86 %\n",
      "Epoch   187: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:190/200 AVG Training Loss:0.650 AVG Validation Loss:0.649 AVG Training Acc 62.08 % AVG Validation Acc 61.86 %\n",
      "Epoch:200/200 AVG Training Loss:0.649 AVG Validation Loss:0.649 AVG Training Acc 62.30 % AVG Validation Acc 61.86 %\n",
      "Split 65\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c029b53bc854585b511ac612abf3fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.664 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.660 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.661 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.661 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:60/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:70/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.93 % AVG Validation Acc 61.77 %\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:80/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.85 % AVG Validation Acc 61.77 %\n",
      "Epoch:90/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.96 % AVG Validation Acc 61.77 %\n",
      "Epoch:100/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.93 % AVG Validation Acc 61.77 %\n",
      "Epoch   109: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:110/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.77 %\n",
      "Epoch:120/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.97 % AVG Validation Acc 61.77 %\n",
      "Epoch:130/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.77 %\n",
      "Epoch:140/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.95 % AVG Validation Acc 61.77 %\n",
      "Epoch   140: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.91 % AVG Validation Acc 61.77 %\n",
      "Epoch:160/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.93 % AVG Validation Acc 61.77 %\n",
      "Epoch:170/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.92 % AVG Validation Acc 61.77 %\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.658 AVG Validation Loss:0.662 AVG Training Acc 61.85 % AVG Validation Acc 61.77 %\n",
      "Epoch:190/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.93 % AVG Validation Acc 61.77 %\n",
      "Epoch:200/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.89 % AVG Validation Acc 61.77 %\n",
      "Split 66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad0af93305349d1acbb45a0c3c9ee61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.664 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.93 % AVG Validation Acc 61.91 %\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.666 AVG Training Acc 61.95 % AVG Validation Acc 61.73 %\n",
      "Epoch:70/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 61.95 % AVG Validation Acc 61.73 %\n",
      "Epoch:80/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 61.96 % AVG Validation Acc 61.73 %\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 61.97 % AVG Validation Acc 61.73 %\n",
      "Epoch:100/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 61.96 % AVG Validation Acc 61.73 %\n",
      "Epoch:110/200 AVG Training Loss:0.654 AVG Validation Loss:0.665 AVG Training Acc 61.99 % AVG Validation Acc 61.73 %\n",
      "Epoch   113: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.654 AVG Validation Loss:0.665 AVG Training Acc 61.98 % AVG Validation Acc 61.73 %\n",
      "Epoch:130/200 AVG Training Loss:0.654 AVG Validation Loss:0.665 AVG Training Acc 62.02 % AVG Validation Acc 61.73 %\n",
      "Epoch:140/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 61.99 % AVG Validation Acc 61.73 %\n",
      "Epoch   144: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.654 AVG Validation Loss:0.665 AVG Training Acc 61.98 % AVG Validation Acc 61.73 %\n",
      "Epoch:160/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.98 % AVG Validation Acc 61.73 %\n",
      "Epoch:170/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 61.97 % AVG Validation Acc 61.73 %\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 62.00 % AVG Validation Acc 61.73 %\n",
      "Epoch:190/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 61.95 % AVG Validation Acc 61.73 %\n",
      "Epoch:200/200 AVG Training Loss:0.655 AVG Validation Loss:0.665 AVG Training Acc 62.00 % AVG Validation Acc 61.73 %\n",
      "Split 67\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ccb8025346f4c8882caea20a6cf64b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.658 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.661 AVG Validation Loss:0.664 AVG Training Acc 61.83 % AVG Validation Acc 61.91 %\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 61.96 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.670 AVG Training Acc 61.96 % AVG Validation Acc 62.00 %\n",
      "Epoch:60/200 AVG Training Loss:0.654 AVG Validation Loss:0.673 AVG Training Acc 62.05 % AVG Validation Acc 61.82 %\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.654 AVG Validation Loss:0.676 AVG Training Acc 62.05 % AVG Validation Acc 61.55 %\n",
      "Epoch:80/200 AVG Training Loss:0.653 AVG Validation Loss:0.675 AVG Training Acc 62.03 % AVG Validation Acc 61.73 %\n",
      "Epoch:90/200 AVG Training Loss:0.653 AVG Validation Loss:0.677 AVG Training Acc 62.13 % AVG Validation Acc 61.64 %\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.652 AVG Validation Loss:0.676 AVG Training Acc 62.06 % AVG Validation Acc 61.64 %\n",
      "Epoch:110/200 AVG Training Loss:0.653 AVG Validation Loss:0.676 AVG Training Acc 62.06 % AVG Validation Acc 61.64 %\n",
      "Epoch:120/200 AVG Training Loss:0.653 AVG Validation Loss:0.677 AVG Training Acc 62.20 % AVG Validation Acc 61.55 %\n",
      "Epoch   124: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.652 AVG Validation Loss:0.677 AVG Training Acc 62.11 % AVG Validation Acc 61.64 %\n",
      "Epoch:140/200 AVG Training Loss:0.652 AVG Validation Loss:0.677 AVG Training Acc 62.17 % AVG Validation Acc 61.73 %\n",
      "Epoch:150/200 AVG Training Loss:0.652 AVG Validation Loss:0.677 AVG Training Acc 62.12 % AVG Validation Acc 61.64 %\n",
      "Epoch   155: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.652 AVG Validation Loss:0.677 AVG Training Acc 62.00 % AVG Validation Acc 61.64 %\n",
      "Epoch:170/200 AVG Training Loss:0.653 AVG Validation Loss:0.677 AVG Training Acc 61.97 % AVG Validation Acc 61.73 %\n",
      "Epoch:180/200 AVG Training Loss:0.652 AVG Validation Loss:0.677 AVG Training Acc 62.21 % AVG Validation Acc 61.55 %\n",
      "Epoch   186: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.652 AVG Validation Loss:0.677 AVG Training Acc 62.16 % AVG Validation Acc 61.64 %\n",
      "Epoch:200/200 AVG Training Loss:0.652 AVG Validation Loss:0.676 AVG Training Acc 62.13 % AVG Validation Acc 61.64 %\n",
      "Split 68\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc83854c29a4e7fa6bf46cf41280a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.660 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.658 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.664 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.664 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:70/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Epoch:80/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch:100/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:110/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch   112: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:130/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:140/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch   143: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:160/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Epoch:170/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch   174: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Epoch:190/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch:200/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Split 69\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ab44c5a3884e4f8a45107cb7080862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.662 AVG Training Acc 61.86 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.662 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.663 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.654 AVG Validation Loss:0.665 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch:60/200 AVG Training Loss:0.652 AVG Validation Loss:0.666 AVG Training Acc 62.06 % AVG Validation Acc 61.64 %\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.651 AVG Validation Loss:0.667 AVG Training Acc 62.12 % AVG Validation Acc 61.64 %\n",
      "Epoch:80/200 AVG Training Loss:0.651 AVG Validation Loss:0.668 AVG Training Acc 62.08 % AVG Validation Acc 61.55 %\n",
      "Epoch:90/200 AVG Training Loss:0.650 AVG Validation Loss:0.669 AVG Training Acc 62.13 % AVG Validation Acc 61.46 %\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.650 AVG Validation Loss:0.669 AVG Training Acc 62.07 % AVG Validation Acc 61.37 %\n",
      "Epoch:110/200 AVG Training Loss:0.651 AVG Validation Loss:0.669 AVG Training Acc 62.10 % AVG Validation Acc 61.46 %\n",
      "Epoch:120/200 AVG Training Loss:0.651 AVG Validation Loss:0.669 AVG Training Acc 62.24 % AVG Validation Acc 61.46 %\n",
      "Epoch   122: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.650 AVG Validation Loss:0.668 AVG Training Acc 62.16 % AVG Validation Acc 61.55 %\n",
      "Epoch:140/200 AVG Training Loss:0.651 AVG Validation Loss:0.669 AVG Training Acc 62.02 % AVG Validation Acc 61.37 %\n",
      "Epoch:150/200 AVG Training Loss:0.650 AVG Validation Loss:0.670 AVG Training Acc 62.00 % AVG Validation Acc 61.46 %\n",
      "Epoch   153: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.651 AVG Validation Loss:0.670 AVG Training Acc 62.11 % AVG Validation Acc 61.46 %\n",
      "Epoch:170/200 AVG Training Loss:0.650 AVG Validation Loss:0.669 AVG Training Acc 62.08 % AVG Validation Acc 61.46 %\n",
      "Epoch:180/200 AVG Training Loss:0.650 AVG Validation Loss:0.669 AVG Training Acc 62.19 % AVG Validation Acc 61.37 %\n",
      "Epoch   184: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.650 AVG Validation Loss:0.670 AVG Training Acc 62.08 % AVG Validation Acc 61.37 %\n",
      "Epoch:200/200 AVG Training Loss:0.650 AVG Validation Loss:0.668 AVG Training Acc 62.11 % AVG Validation Acc 61.37 %\n",
      "Split 70\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e20482abeac4b11a5a518c70e7775c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.658 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.657 AVG Training Acc 61.93 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.657 AVG Training Acc 61.92 % AVG Validation Acc 61.82 %\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 62.00 %\n",
      "Epoch:70/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 62.02 % AVG Validation Acc 62.00 %\n",
      "Epoch:80/200 AVG Training Loss:0.655 AVG Validation Loss:0.657 AVG Training Acc 62.07 % AVG Validation Acc 62.00 %\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 62.03 % AVG Validation Acc 61.91 %\n",
      "Epoch:100/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 62.01 % AVG Validation Acc 61.91 %\n",
      "Epoch:110/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 62.03 % AVG Validation Acc 62.09 %\n",
      "Epoch:120/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 62.09 % AVG Validation Acc 61.82 %\n",
      "Epoch   120: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.655 AVG Validation Loss:0.659 AVG Training Acc 62.00 % AVG Validation Acc 62.00 %\n",
      "Epoch:140/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 62.04 % AVG Validation Acc 61.91 %\n",
      "Epoch:150/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 62.03 % AVG Validation Acc 62.09 %\n",
      "Epoch   151: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 62.01 % AVG Validation Acc 62.00 %\n",
      "Epoch:170/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 61.99 % AVG Validation Acc 62.00 %\n",
      "Epoch:180/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 62.00 % AVG Validation Acc 62.00 %\n",
      "Epoch   182: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.655 AVG Validation Loss:0.658 AVG Training Acc 61.88 % AVG Validation Acc 62.00 %\n",
      "Epoch:200/200 AVG Training Loss:0.654 AVG Validation Loss:0.658 AVG Training Acc 62.01 % AVG Validation Acc 62.00 %\n",
      "Split 71\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0276f00337ee49d8bbf4ae8932c177cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.657 AVG Training Acc 61.88 % AVG Validation Acc 61.95 %\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.657 AVG Training Acc 61.88 % AVG Validation Acc 61.95 %\n",
      "Epoch:50/200 AVG Training Loss:0.657 AVG Validation Loss:0.657 AVG Training Acc 61.88 % AVG Validation Acc 61.95 %\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.657 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:70/200 AVG Training Loss:0.657 AVG Validation Loss:0.657 AVG Training Acc 61.90 % AVG Validation Acc 61.95 %\n",
      "Epoch:80/200 AVG Training Loss:0.658 AVG Validation Loss:0.657 AVG Training Acc 61.92 % AVG Validation Acc 61.95 %\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.656 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:100/200 AVG Training Loss:0.656 AVG Validation Loss:0.657 AVG Training Acc 61.90 % AVG Validation Acc 61.95 %\n",
      "Epoch:110/200 AVG Training Loss:0.656 AVG Validation Loss:0.657 AVG Training Acc 61.94 % AVG Validation Acc 61.95 %\n",
      "Epoch   116: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.656 AVG Validation Loss:0.657 AVG Training Acc 61.93 % AVG Validation Acc 61.95 %\n",
      "Epoch:130/200 AVG Training Loss:0.656 AVG Validation Loss:0.657 AVG Training Acc 61.90 % AVG Validation Acc 61.95 %\n",
      "Epoch:140/200 AVG Training Loss:0.656 AVG Validation Loss:0.657 AVG Training Acc 61.92 % AVG Validation Acc 61.95 %\n",
      "Epoch   147: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.657 AVG Validation Loss:0.657 AVG Training Acc 61.92 % AVG Validation Acc 61.95 %\n",
      "Epoch:160/200 AVG Training Loss:0.656 AVG Validation Loss:0.657 AVG Training Acc 61.87 % AVG Validation Acc 61.95 %\n",
      "Epoch:170/200 AVG Training Loss:0.656 AVG Validation Loss:0.657 AVG Training Acc 61.92 % AVG Validation Acc 61.95 %\n",
      "Epoch   178: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.656 AVG Validation Loss:0.657 AVG Training Acc 61.90 % AVG Validation Acc 61.95 %\n",
      "Epoch:190/200 AVG Training Loss:0.656 AVG Validation Loss:0.657 AVG Training Acc 61.93 % AVG Validation Acc 61.95 %\n",
      "Epoch:200/200 AVG Training Loss:0.655 AVG Validation Loss:0.657 AVG Training Acc 61.92 % AVG Validation Acc 61.95 %\n",
      "Split 72\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c722dac2aa4868b24edc640761e4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.660 AVG Training Acc 61.91 % AVG Validation Acc 61.68 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.662 AVG Training Acc 61.91 % AVG Validation Acc 61.68 %\n",
      "Epoch:50/200 AVG Training Loss:0.659 AVG Validation Loss:0.665 AVG Training Acc 61.92 % AVG Validation Acc 61.68 %\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.659 AVG Validation Loss:0.667 AVG Training Acc 61.92 % AVG Validation Acc 61.59 %\n",
      "Epoch:70/200 AVG Training Loss:0.658 AVG Validation Loss:0.667 AVG Training Acc 61.94 % AVG Validation Acc 61.68 %\n",
      "Epoch:80/200 AVG Training Loss:0.658 AVG Validation Loss:0.667 AVG Training Acc 61.93 % AVG Validation Acc 61.59 %\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.657 AVG Validation Loss:0.667 AVG Training Acc 61.93 % AVG Validation Acc 61.59 %\n",
      "Epoch:100/200 AVG Training Loss:0.658 AVG Validation Loss:0.667 AVG Training Acc 61.92 % AVG Validation Acc 61.59 %\n",
      "Epoch:110/200 AVG Training Loss:0.658 AVG Validation Loss:0.667 AVG Training Acc 61.93 % AVG Validation Acc 61.59 %\n",
      "Epoch   118: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.658 AVG Validation Loss:0.667 AVG Training Acc 61.94 % AVG Validation Acc 61.59 %\n",
      "Epoch:130/200 AVG Training Loss:0.658 AVG Validation Loss:0.667 AVG Training Acc 61.92 % AVG Validation Acc 61.59 %\n",
      "Epoch:140/200 AVG Training Loss:0.658 AVG Validation Loss:0.667 AVG Training Acc 61.93 % AVG Validation Acc 61.59 %\n",
      "Epoch   149: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.658 AVG Validation Loss:0.667 AVG Training Acc 61.93 % AVG Validation Acc 61.59 %\n",
      "Epoch:160/200 AVG Training Loss:0.658 AVG Validation Loss:0.667 AVG Training Acc 61.93 % AVG Validation Acc 61.59 %\n",
      "Epoch:170/200 AVG Training Loss:0.658 AVG Validation Loss:0.667 AVG Training Acc 61.91 % AVG Validation Acc 61.59 %\n",
      "Epoch:180/200 AVG Training Loss:0.658 AVG Validation Loss:0.667 AVG Training Acc 61.93 % AVG Validation Acc 61.59 %\n",
      "Epoch   180: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.659 AVG Validation Loss:0.667 AVG Training Acc 61.90 % AVG Validation Acc 61.59 %\n",
      "Epoch:200/200 AVG Training Loss:0.659 AVG Validation Loss:0.667 AVG Training Acc 61.94 % AVG Validation Acc 61.59 %\n",
      "Split 73\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c77b0dd0fe04dc0bacb9c5c4b7add39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.663 AVG Validation Loss:0.663 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.659 AVG Validation Loss:0.663 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.663 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.664 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch:60/200 AVG Training Loss:0.659 AVG Validation Loss:0.664 AVG Training Acc 61.93 % AVG Validation Acc 61.86 %\n",
      "Epoch:70/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:80/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.92 % AVG Validation Acc 61.86 %\n",
      "Epoch:90/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 61.93 % AVG Validation Acc 61.86 %\n",
      "Epoch:100/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch   109: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:110/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.92 % AVG Validation Acc 61.86 %\n",
      "Epoch:120/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:130/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 61.94 % AVG Validation Acc 61.86 %\n",
      "Epoch:140/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch   140: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 61.92 % AVG Validation Acc 61.86 %\n",
      "Epoch:160/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 61.94 % AVG Validation Acc 61.86 %\n",
      "Epoch:170/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 61.94 % AVG Validation Acc 61.86 %\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.92 % AVG Validation Acc 61.86 %\n",
      "Epoch:190/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.94 % AVG Validation Acc 61.86 %\n",
      "Epoch:200/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.98 % AVG Validation Acc 61.86 %\n",
      "Split 74\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e3173b86e340528864854288ad9b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.663 AVG Training Acc 61.82 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.664 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.656 AVG Validation Loss:0.668 AVG Training Acc 61.96 % AVG Validation Acc 61.86 %\n",
      "Epoch:50/200 AVG Training Loss:0.655 AVG Validation Loss:0.668 AVG Training Acc 61.93 % AVG Validation Acc 61.77 %\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.654 AVG Validation Loss:0.669 AVG Training Acc 62.03 % AVG Validation Acc 61.68 %\n",
      "Epoch:70/200 AVG Training Loss:0.654 AVG Validation Loss:0.669 AVG Training Acc 62.05 % AVG Validation Acc 61.68 %\n",
      "Epoch:80/200 AVG Training Loss:0.654 AVG Validation Loss:0.670 AVG Training Acc 62.05 % AVG Validation Acc 61.68 %\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.653 AVG Validation Loss:0.670 AVG Training Acc 62.07 % AVG Validation Acc 61.68 %\n",
      "Epoch:100/200 AVG Training Loss:0.653 AVG Validation Loss:0.670 AVG Training Acc 62.03 % AVG Validation Acc 61.68 %\n",
      "Epoch:110/200 AVG Training Loss:0.653 AVG Validation Loss:0.670 AVG Training Acc 62.01 % AVG Validation Acc 61.68 %\n",
      "Epoch   118: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.654 AVG Validation Loss:0.670 AVG Training Acc 62.00 % AVG Validation Acc 61.68 %\n",
      "Epoch:130/200 AVG Training Loss:0.653 AVG Validation Loss:0.671 AVG Training Acc 62.01 % AVG Validation Acc 61.68 %\n",
      "Epoch:140/200 AVG Training Loss:0.653 AVG Validation Loss:0.671 AVG Training Acc 62.01 % AVG Validation Acc 61.68 %\n",
      "Epoch   149: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.654 AVG Validation Loss:0.671 AVG Training Acc 62.02 % AVG Validation Acc 61.68 %\n",
      "Epoch:160/200 AVG Training Loss:0.654 AVG Validation Loss:0.670 AVG Training Acc 61.96 % AVG Validation Acc 61.68 %\n",
      "Epoch:170/200 AVG Training Loss:0.653 AVG Validation Loss:0.671 AVG Training Acc 62.03 % AVG Validation Acc 61.68 %\n",
      "Epoch:180/200 AVG Training Loss:0.653 AVG Validation Loss:0.670 AVG Training Acc 62.06 % AVG Validation Acc 61.68 %\n",
      "Epoch   180: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.653 AVG Validation Loss:0.671 AVG Training Acc 62.08 % AVG Validation Acc 61.68 %\n",
      "Epoch:200/200 AVG Training Loss:0.653 AVG Validation Loss:0.671 AVG Training Acc 62.10 % AVG Validation Acc 61.68 %\n",
      "Split 75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196711cfab8f4d5789c72ff8fe8f31c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.660 AVG Training Acc 61.88 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch:30/200 AVG Training Loss:0.661 AVG Validation Loss:0.657 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.660 AVG Validation Loss:0.659 AVG Training Acc 61.86 % AVG Validation Acc 61.86 %\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.658 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:60/200 AVG Training Loss:0.654 AVG Validation Loss:0.659 AVG Training Acc 62.09 % AVG Validation Acc 61.95 %\n",
      "Epoch:70/200 AVG Training Loss:0.651 AVG Validation Loss:0.660 AVG Training Acc 62.13 % AVG Validation Acc 61.41 %\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.651 AVG Validation Loss:0.660 AVG Training Acc 62.13 % AVG Validation Acc 61.41 %\n",
      "Epoch:90/200 AVG Training Loss:0.650 AVG Validation Loss:0.661 AVG Training Acc 62.08 % AVG Validation Acc 61.32 %\n",
      "Epoch:100/200 AVG Training Loss:0.649 AVG Validation Loss:0.661 AVG Training Acc 62.15 % AVG Validation Acc 61.32 %\n",
      "Epoch   106: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:110/200 AVG Training Loss:0.649 AVG Validation Loss:0.661 AVG Training Acc 62.41 % AVG Validation Acc 61.14 %\n",
      "Epoch:120/200 AVG Training Loss:0.650 AVG Validation Loss:0.661 AVG Training Acc 62.41 % AVG Validation Acc 61.32 %\n",
      "Epoch:130/200 AVG Training Loss:0.650 AVG Validation Loss:0.662 AVG Training Acc 62.24 % AVG Validation Acc 61.23 %\n",
      "Epoch   137: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:140/200 AVG Training Loss:0.649 AVG Validation Loss:0.661 AVG Training Acc 62.37 % AVG Validation Acc 61.41 %\n",
      "Epoch:150/200 AVG Training Loss:0.649 AVG Validation Loss:0.661 AVG Training Acc 62.14 % AVG Validation Acc 61.41 %\n",
      "Epoch:160/200 AVG Training Loss:0.650 AVG Validation Loss:0.661 AVG Training Acc 62.14 % AVG Validation Acc 61.14 %\n",
      "Epoch   168: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:170/200 AVG Training Loss:0.649 AVG Validation Loss:0.661 AVG Training Acc 62.09 % AVG Validation Acc 61.32 %\n",
      "Epoch:180/200 AVG Training Loss:0.649 AVG Validation Loss:0.661 AVG Training Acc 62.36 % AVG Validation Acc 61.32 %\n",
      "Epoch:190/200 AVG Training Loss:0.650 AVG Validation Loss:0.661 AVG Training Acc 62.28 % AVG Validation Acc 61.23 %\n",
      "Epoch   199: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:200/200 AVG Training Loss:0.649 AVG Validation Loss:0.661 AVG Training Acc 62.27 % AVG Validation Acc 61.32 %\n",
      "Split 76\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75db71015e1a4153b86a874206bc6e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.661 AVG Validation Loss:0.667 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.659 AVG Validation Loss:0.666 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:60/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:70/200 AVG Training Loss:0.659 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:80/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:90/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:100/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch   108: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:110/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:120/200 AVG Training Loss:0.658 AVG Validation Loss:0.664 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:130/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch   139: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:140/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:150/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:160/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:170/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch   170: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.658 AVG Validation Loss:0.664 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:190/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:200/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Split 77\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e1d1ca0d77449f81e172cb6c4fa86f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.87 % AVG Validation Acc 61.91 %\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.660 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.660 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:60/200 AVG Training Loss:0.658 AVG Validation Loss:0.658 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Epoch:70/200 AVG Training Loss:0.657 AVG Validation Loss:0.657 AVG Training Acc 61.99 % AVG Validation Acc 61.91 %\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.654 AVG Validation Loss:0.657 AVG Training Acc 62.11 % AVG Validation Acc 62.00 %\n",
      "Epoch:90/200 AVG Training Loss:0.654 AVG Validation Loss:0.657 AVG Training Acc 62.15 % AVG Validation Acc 62.00 %\n",
      "Epoch:100/200 AVG Training Loss:0.653 AVG Validation Loss:0.657 AVG Training Acc 61.98 % AVG Validation Acc 61.91 %\n",
      "Epoch   108: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:110/200 AVG Training Loss:0.653 AVG Validation Loss:0.657 AVG Training Acc 62.04 % AVG Validation Acc 61.91 %\n",
      "Epoch:120/200 AVG Training Loss:0.653 AVG Validation Loss:0.657 AVG Training Acc 62.17 % AVG Validation Acc 61.82 %\n",
      "Epoch:130/200 AVG Training Loss:0.653 AVG Validation Loss:0.657 AVG Training Acc 62.16 % AVG Validation Acc 61.91 %\n",
      "Epoch   139: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:140/200 AVG Training Loss:0.653 AVG Validation Loss:0.657 AVG Training Acc 62.15 % AVG Validation Acc 61.91 %\n",
      "Epoch:150/200 AVG Training Loss:0.653 AVG Validation Loss:0.657 AVG Training Acc 62.16 % AVG Validation Acc 61.82 %\n",
      "Epoch:160/200 AVG Training Loss:0.652 AVG Validation Loss:0.657 AVG Training Acc 62.16 % AVG Validation Acc 61.82 %\n",
      "Epoch:170/200 AVG Training Loss:0.653 AVG Validation Loss:0.657 AVG Training Acc 62.07 % AVG Validation Acc 61.82 %\n",
      "Epoch   170: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:180/200 AVG Training Loss:0.653 AVG Validation Loss:0.657 AVG Training Acc 62.15 % AVG Validation Acc 61.91 %\n",
      "Epoch:190/200 AVG Training Loss:0.653 AVG Validation Loss:0.657 AVG Training Acc 62.14 % AVG Validation Acc 61.82 %\n",
      "Epoch:200/200 AVG Training Loss:0.652 AVG Validation Loss:0.657 AVG Training Acc 62.13 % AVG Validation Acc 61.82 %\n",
      "Split 78\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527a56a99b9c42919171930c6e31f45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.658 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.657 AVG Validation Loss:0.659 AVG Training Acc 61.93 % AVG Validation Acc 61.91 %\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.659 AVG Training Acc 62.04 % AVG Validation Acc 61.91 %\n",
      "Epoch:60/200 AVG Training Loss:0.654 AVG Validation Loss:0.659 AVG Training Acc 62.26 % AVG Validation Acc 61.73 %\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.653 AVG Validation Loss:0.659 AVG Training Acc 62.12 % AVG Validation Acc 61.55 %\n",
      "Epoch:80/200 AVG Training Loss:0.653 AVG Validation Loss:0.660 AVG Training Acc 62.22 % AVG Validation Acc 61.28 %\n",
      "Epoch:90/200 AVG Training Loss:0.654 AVG Validation Loss:0.660 AVG Training Acc 62.08 % AVG Validation Acc 61.37 %\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.651 AVG Validation Loss:0.659 AVG Training Acc 62.16 % AVG Validation Acc 61.46 %\n",
      "Epoch:110/200 AVG Training Loss:0.652 AVG Validation Loss:0.660 AVG Training Acc 62.16 % AVG Validation Acc 61.37 %\n",
      "Epoch:120/200 AVG Training Loss:0.652 AVG Validation Loss:0.659 AVG Training Acc 62.19 % AVG Validation Acc 61.28 %\n",
      "Epoch   122: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.652 AVG Validation Loss:0.661 AVG Training Acc 62.18 % AVG Validation Acc 61.28 %\n",
      "Epoch:140/200 AVG Training Loss:0.653 AVG Validation Loss:0.660 AVG Training Acc 62.17 % AVG Validation Acc 61.37 %\n",
      "Epoch:150/200 AVG Training Loss:0.652 AVG Validation Loss:0.660 AVG Training Acc 62.18 % AVG Validation Acc 61.28 %\n",
      "Epoch   153: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.653 AVG Validation Loss:0.660 AVG Training Acc 62.21 % AVG Validation Acc 61.46 %\n",
      "Epoch:170/200 AVG Training Loss:0.652 AVG Validation Loss:0.660 AVG Training Acc 62.14 % AVG Validation Acc 61.28 %\n",
      "Epoch:180/200 AVG Training Loss:0.652 AVG Validation Loss:0.661 AVG Training Acc 62.28 % AVG Validation Acc 61.28 %\n",
      "Epoch   184: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.652 AVG Validation Loss:0.660 AVG Training Acc 62.20 % AVG Validation Acc 61.28 %\n",
      "Epoch:200/200 AVG Training Loss:0.653 AVG Validation Loss:0.660 AVG Training Acc 62.19 % AVG Validation Acc 61.46 %\n",
      "Split 79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500c1d2d9f694bef9e302d816ea2f244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.660 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.661 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/200 AVG Training Loss:0.658 AVG Validation Loss:0.663 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Epoch:60/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:70/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.95 % AVG Validation Acc 61.91 %\n",
      "Epoch:80/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.95 % AVG Validation Acc 61.91 %\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.95 % AVG Validation Acc 61.91 %\n",
      "Epoch:100/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.95 % AVG Validation Acc 61.91 %\n",
      "Epoch:110/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Epoch   111: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.94 % AVG Validation Acc 61.91 %\n",
      "Epoch:130/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.93 % AVG Validation Acc 61.91 %\n",
      "Epoch:140/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.93 % AVG Validation Acc 61.91 %\n",
      "Epoch   142: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.656 AVG Validation Loss:0.664 AVG Training Acc 61.99 % AVG Validation Acc 61.91 %\n",
      "Epoch:160/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Epoch:170/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Epoch   173: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.94 % AVG Validation Acc 61.91 %\n",
      "Epoch:190/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Epoch:200/200 AVG Training Loss:0.657 AVG Validation Loss:0.664 AVG Training Acc 61.92 % AVG Validation Acc 61.91 %\n",
      "Split 80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cd4756296140e98208a9546e86430f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.664 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.659 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.658 AVG Validation Loss:0.666 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.84 % AVG Validation Acc 61.91 %\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.666 AVG Training Acc 61.82 % AVG Validation Acc 61.91 %\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.666 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:70/200 AVG Training Loss:0.656 AVG Validation Loss:0.666 AVG Training Acc 61.93 % AVG Validation Acc 61.91 %\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:80/200 AVG Training Loss:0.656 AVG Validation Loss:0.667 AVG Training Acc 61.83 % AVG Validation Acc 61.91 %\n",
      "Epoch:90/200 AVG Training Loss:0.656 AVG Validation Loss:0.667 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch:100/200 AVG Training Loss:0.655 AVG Validation Loss:0.667 AVG Training Acc 61.94 % AVG Validation Acc 61.91 %\n",
      "Epoch:110/200 AVG Training Loss:0.655 AVG Validation Loss:0.667 AVG Training Acc 61.96 % AVG Validation Acc 61.91 %\n",
      "Epoch   110: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.656 AVG Validation Loss:0.667 AVG Training Acc 61.93 % AVG Validation Acc 61.91 %\n",
      "Epoch:130/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 61.88 % AVG Validation Acc 61.91 %\n",
      "Epoch:140/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 61.78 % AVG Validation Acc 61.91 %\n",
      "Epoch   141: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.655 AVG Validation Loss:0.667 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:160/200 AVG Training Loss:0.656 AVG Validation Loss:0.667 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch:170/200 AVG Training Loss:0.656 AVG Validation Loss:0.666 AVG Training Acc 61.93 % AVG Validation Acc 61.91 %\n",
      "Epoch   172: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.655 AVG Validation Loss:0.667 AVG Training Acc 61.84 % AVG Validation Acc 61.91 %\n",
      "Epoch:190/200 AVG Training Loss:0.656 AVG Validation Loss:0.667 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:200/200 AVG Training Loss:0.655 AVG Validation Loss:0.666 AVG Training Acc 61.84 % AVG Validation Acc 61.91 %\n",
      "Split 81\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c0c609a96e45e691f105c2b81ed514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.658 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:40/200 AVG Training Loss:0.660 AVG Validation Loss:0.658 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch:50/200 AVG Training Loss:0.659 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.658 AVG Validation Loss:0.659 AVG Training Acc 61.85 % AVG Validation Acc 61.95 %\n",
      "Epoch:70/200 AVG Training Loss:0.658 AVG Validation Loss:0.659 AVG Training Acc 61.87 % AVG Validation Acc 61.95 %\n",
      "Epoch:80/200 AVG Training Loss:0.658 AVG Validation Loss:0.659 AVG Training Acc 61.90 % AVG Validation Acc 61.95 %\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.657 AVG Validation Loss:0.659 AVG Training Acc 61.88 % AVG Validation Acc 61.95 %\n",
      "Epoch:100/200 AVG Training Loss:0.657 AVG Validation Loss:0.659 AVG Training Acc 61.88 % AVG Validation Acc 61.95 %\n",
      "Epoch:110/200 AVG Training Loss:0.657 AVG Validation Loss:0.659 AVG Training Acc 61.86 % AVG Validation Acc 61.95 %\n",
      "Epoch   117: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.657 AVG Validation Loss:0.659 AVG Training Acc 61.81 % AVG Validation Acc 61.95 %\n",
      "Epoch:130/200 AVG Training Loss:0.658 AVG Validation Loss:0.659 AVG Training Acc 61.88 % AVG Validation Acc 61.95 %\n",
      "Epoch:140/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.94 % AVG Validation Acc 61.95 %\n",
      "Epoch   148: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.657 AVG Validation Loss:0.659 AVG Training Acc 61.92 % AVG Validation Acc 61.95 %\n",
      "Epoch:160/200 AVG Training Loss:0.657 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.95 %\n",
      "Epoch:170/200 AVG Training Loss:0.658 AVG Validation Loss:0.659 AVG Training Acc 61.89 % AVG Validation Acc 61.95 %\n",
      "Epoch   179: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.658 AVG Validation Loss:0.659 AVG Training Acc 61.85 % AVG Validation Acc 61.95 %\n",
      "Epoch:190/200 AVG Training Loss:0.658 AVG Validation Loss:0.659 AVG Training Acc 61.98 % AVG Validation Acc 61.95 %\n",
      "Epoch:200/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.95 %\n",
      "Split 82\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad3a658c32e4b53954723066f60b685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.663 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.661 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch:50/200 AVG Training Loss:0.657 AVG Validation Loss:0.659 AVG Training Acc 61.97 % AVG Validation Acc 61.77 %\n",
      "Epoch:60/200 AVG Training Loss:0.656 AVG Validation Loss:0.660 AVG Training Acc 61.92 % AVG Validation Acc 61.95 %\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.654 AVG Validation Loss:0.660 AVG Training Acc 61.99 % AVG Validation Acc 61.95 %\n",
      "Epoch:80/200 AVG Training Loss:0.653 AVG Validation Loss:0.659 AVG Training Acc 61.97 % AVG Validation Acc 61.95 %\n",
      "Epoch:90/200 AVG Training Loss:0.653 AVG Validation Loss:0.660 AVG Training Acc 62.01 % AVG Validation Acc 62.04 %\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.654 AVG Validation Loss:0.660 AVG Training Acc 62.09 % AVG Validation Acc 61.95 %\n",
      "Epoch:110/200 AVG Training Loss:0.652 AVG Validation Loss:0.660 AVG Training Acc 61.98 % AVG Validation Acc 62.04 %\n",
      "Epoch:120/200 AVG Training Loss:0.651 AVG Validation Loss:0.660 AVG Training Acc 62.11 % AVG Validation Acc 62.04 %\n",
      "Epoch   124: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.653 AVG Validation Loss:0.660 AVG Training Acc 62.09 % AVG Validation Acc 62.04 %\n",
      "Epoch:140/200 AVG Training Loss:0.652 AVG Validation Loss:0.660 AVG Training Acc 62.05 % AVG Validation Acc 62.13 %\n",
      "Epoch:150/200 AVG Training Loss:0.653 AVG Validation Loss:0.660 AVG Training Acc 61.97 % AVG Validation Acc 62.13 %\n",
      "Epoch   155: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.652 AVG Validation Loss:0.659 AVG Training Acc 62.13 % AVG Validation Acc 62.04 %\n",
      "Epoch:170/200 AVG Training Loss:0.652 AVG Validation Loss:0.660 AVG Training Acc 62.10 % AVG Validation Acc 61.86 %\n",
      "Epoch:180/200 AVG Training Loss:0.653 AVG Validation Loss:0.659 AVG Training Acc 62.11 % AVG Validation Acc 62.13 %\n",
      "Epoch   186: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.654 AVG Validation Loss:0.659 AVG Training Acc 62.11 % AVG Validation Acc 61.95 %\n",
      "Epoch:200/200 AVG Training Loss:0.653 AVG Validation Loss:0.660 AVG Training Acc 62.05 % AVG Validation Acc 62.04 %\n",
      "Split 83\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa56670bd6b44ab89dbbec7887cf589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.662 AVG Training Acc 61.92 % AVG Validation Acc 61.86 %\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.657 AVG Validation Loss:0.662 AVG Training Acc 61.95 % AVG Validation Acc 61.86 %\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.663 AVG Training Acc 61.92 % AVG Validation Acc 61.68 %\n",
      "Epoch:60/200 AVG Training Loss:0.654 AVG Validation Loss:0.666 AVG Training Acc 61.95 % AVG Validation Acc 61.86 %\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.652 AVG Validation Loss:0.670 AVG Training Acc 62.10 % AVG Validation Acc 61.77 %\n",
      "Epoch:80/200 AVG Training Loss:0.652 AVG Validation Loss:0.672 AVG Training Acc 62.15 % AVG Validation Acc 61.68 %\n",
      "Epoch:90/200 AVG Training Loss:0.652 AVG Validation Loss:0.672 AVG Training Acc 62.11 % AVG Validation Acc 61.86 %\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.651 AVG Validation Loss:0.672 AVG Training Acc 62.20 % AVG Validation Acc 61.68 %\n",
      "Epoch:110/200 AVG Training Loss:0.651 AVG Validation Loss:0.673 AVG Training Acc 62.17 % AVG Validation Acc 61.77 %\n",
      "Epoch:120/200 AVG Training Loss:0.651 AVG Validation Loss:0.673 AVG Training Acc 62.04 % AVG Validation Acc 61.77 %\n",
      "Epoch   123: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.650 AVG Validation Loss:0.673 AVG Training Acc 62.13 % AVG Validation Acc 61.77 %\n",
      "Epoch:140/200 AVG Training Loss:0.652 AVG Validation Loss:0.673 AVG Training Acc 62.16 % AVG Validation Acc 61.68 %\n",
      "Epoch:150/200 AVG Training Loss:0.651 AVG Validation Loss:0.673 AVG Training Acc 62.21 % AVG Validation Acc 61.77 %\n",
      "Epoch   154: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:160/200 AVG Training Loss:0.651 AVG Validation Loss:0.673 AVG Training Acc 62.19 % AVG Validation Acc 61.77 %\n",
      "Epoch:170/200 AVG Training Loss:0.652 AVG Validation Loss:0.673 AVG Training Acc 62.12 % AVG Validation Acc 61.68 %\n",
      "Epoch:180/200 AVG Training Loss:0.651 AVG Validation Loss:0.673 AVG Training Acc 62.18 % AVG Validation Acc 61.77 %\n",
      "Epoch   185: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:190/200 AVG Training Loss:0.651 AVG Validation Loss:0.673 AVG Training Acc 62.12 % AVG Validation Acc 61.77 %\n",
      "Epoch:200/200 AVG Training Loss:0.651 AVG Validation Loss:0.673 AVG Training Acc 62.11 % AVG Validation Acc 61.86 %\n",
      "Split 84\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a423d63df0d45cbb350674427244a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.659 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.662 AVG Validation Loss:0.662 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.659 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.661 AVG Training Acc 61.92 % AVG Validation Acc 61.86 %\n",
      "Epoch:50/200 AVG Training Loss:0.656 AVG Validation Loss:0.662 AVG Training Acc 61.99 % AVG Validation Acc 61.77 %\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.654 AVG Validation Loss:0.662 AVG Training Acc 62.04 % AVG Validation Acc 61.77 %\n",
      "Epoch:70/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 61.98 % AVG Validation Acc 61.77 %\n",
      "Epoch:80/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 61.96 % AVG Validation Acc 61.77 %\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.653 AVG Validation Loss:0.663 AVG Training Acc 62.02 % AVG Validation Acc 61.68 %\n",
      "Epoch:100/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 61.94 % AVG Validation Acc 61.68 %\n",
      "Epoch:110/200 AVG Training Loss:0.653 AVG Validation Loss:0.664 AVG Training Acc 61.99 % AVG Validation Acc 61.68 %\n",
      "Epoch   117: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.653 AVG Validation Loss:0.663 AVG Training Acc 62.06 % AVG Validation Acc 61.59 %\n",
      "Epoch:130/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.04 % AVG Validation Acc 61.59 %\n",
      "Epoch:140/200 AVG Training Loss:0.654 AVG Validation Loss:0.663 AVG Training Acc 61.98 % AVG Validation Acc 61.59 %\n",
      "Epoch   148: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.12 % AVG Validation Acc 61.59 %\n",
      "Epoch:160/200 AVG Training Loss:0.654 AVG Validation Loss:0.664 AVG Training Acc 62.12 % AVG Validation Acc 61.59 %\n",
      "Epoch:170/200 AVG Training Loss:0.653 AVG Validation Loss:0.664 AVG Training Acc 62.03 % AVG Validation Acc 61.59 %\n",
      "Epoch   179: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.653 AVG Validation Loss:0.664 AVG Training Acc 61.99 % AVG Validation Acc 61.59 %\n",
      "Epoch:190/200 AVG Training Loss:0.653 AVG Validation Loss:0.664 AVG Training Acc 62.05 % AVG Validation Acc 61.59 %\n",
      "Epoch:200/200 AVG Training Loss:0.653 AVG Validation Loss:0.663 AVG Training Acc 61.98 % AVG Validation Acc 61.59 %\n",
      "Split 85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644ec7ecbc004458b79557f92a70fa4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.663 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.661 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:40/200 AVG Training Loss:0.659 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:50/200 AVG Training Loss:0.659 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:70/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:80/200 AVG Training Loss:0.659 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch:100/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:110/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch   113: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:120/200 AVG Training Loss:0.659 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:130/200 AVG Training Loss:0.658 AVG Validation Loss:0.661 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch:140/200 AVG Training Loss:0.658 AVG Validation Loss:0.661 AVG Training Acc 61.91 % AVG Validation Acc 61.86 %\n",
      "Epoch   144: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:150/200 AVG Training Loss:0.658 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:160/200 AVG Training Loss:0.659 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:170/200 AVG Training Loss:0.659 AVG Validation Loss:0.660 AVG Training Acc 61.89 % AVG Validation Acc 61.86 %\n",
      "Epoch   175: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:180/200 AVG Training Loss:0.659 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:190/200 AVG Training Loss:0.659 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Epoch:200/200 AVG Training Loss:0.659 AVG Validation Loss:0.660 AVG Training Acc 61.90 % AVG Validation Acc 61.86 %\n",
      "Split 86\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba6697adad5438aa07c27253c1499bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.662 AVG Validation Loss:0.664 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:0.659 AVG Validation Loss:0.664 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 61.91 % AVG Validation Acc 61.91 %\n",
      "Epoch:60/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 61.94 % AVG Validation Acc 61.91 %\n",
      "Epoch:70/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.91 % AVG Validation Acc 61.73 %\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:80/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.75 % AVG Validation Acc 61.73 %\n",
      "Epoch:90/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.80 % AVG Validation Acc 61.82 %\n",
      "Epoch:100/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 61.71 % AVG Validation Acc 61.82 %\n",
      "Epoch   107: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:110/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.89 % AVG Validation Acc 61.73 %\n",
      "Epoch:120/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.76 % AVG Validation Acc 61.91 %\n",
      "Epoch:130/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.88 % AVG Validation Acc 61.73 %\n",
      "Epoch   138: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:140/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 61.74 % AVG Validation Acc 61.82 %\n",
      "Epoch:150/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.94 % AVG Validation Acc 61.73 %\n",
      "Epoch:160/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.70 % AVG Validation Acc 61.82 %\n",
      "Epoch   169: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:170/200 AVG Training Loss:0.656 AVG Validation Loss:0.665 AVG Training Acc 61.72 % AVG Validation Acc 61.73 %\n",
      "Epoch:180/200 AVG Training Loss:0.656 AVG Validation Loss:0.666 AVG Training Acc 61.76 % AVG Validation Acc 61.82 %\n",
      "Epoch:190/200 AVG Training Loss:0.656 AVG Validation Loss:0.666 AVG Training Acc 61.82 % AVG Validation Acc 61.73 %\n",
      "Epoch:200/200 AVG Training Loss:0.657 AVG Validation Loss:0.665 AVG Training Acc 62.03 % AVG Validation Acc 61.73 %\n",
      "Split 87\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1123986aa14d4caa71fa3e1dc228b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.661 AVG Validation Loss:0.663 AVG Training Acc 61.90 % AVG Validation Acc 61.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.661 AVG Validation Loss:0.663 AVG Training Acc 61.89 % AVG Validation Acc 61.91 %\n",
      "Epoch:30/200 AVG Training Loss:0.660 AVG Validation Loss:0.663 AVG Training Acc 61.88 % AVG Validation Acc 62.00 %\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.653 AVG Validation Loss:0.665 AVG Training Acc 62.11 % AVG Validation Acc 61.73 %\n",
      "Epoch:50/200 AVG Training Loss:0.650 AVG Validation Loss:0.665 AVG Training Acc 62.23 % AVG Validation Acc 62.00 %\n",
      "Epoch:60/200 AVG Training Loss:0.649 AVG Validation Loss:0.672 AVG Training Acc 62.54 % AVG Validation Acc 61.64 %\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.647 AVG Validation Loss:0.670 AVG Training Acc 62.57 % AVG Validation Acc 61.64 %\n",
      "Epoch:80/200 AVG Training Loss:0.645 AVG Validation Loss:0.673 AVG Training Acc 62.73 % AVG Validation Acc 61.37 %\n",
      "Epoch:90/200 AVG Training Loss:0.646 AVG Validation Loss:0.676 AVG Training Acc 62.59 % AVG Validation Acc 61.46 %\n",
      "Epoch    97: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/200 AVG Training Loss:0.646 AVG Validation Loss:0.681 AVG Training Acc 62.56 % AVG Validation Acc 61.37 %\n",
      "Epoch:110/200 AVG Training Loss:0.645 AVG Validation Loss:0.682 AVG Training Acc 62.52 % AVG Validation Acc 61.37 %\n",
      "Epoch:120/200 AVG Training Loss:0.645 AVG Validation Loss:0.681 AVG Training Acc 62.63 % AVG Validation Acc 61.37 %\n",
      "Epoch   128: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:130/200 AVG Training Loss:0.645 AVG Validation Loss:0.680 AVG Training Acc 62.58 % AVG Validation Acc 61.46 %\n",
      "Epoch:140/200 AVG Training Loss:0.645 AVG Validation Loss:0.681 AVG Training Acc 62.56 % AVG Validation Acc 61.64 %\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(list(course_programs.keys())[2:]):\n",
    "    \n",
    "    print(i)\n",
    "    threshold_dict = {} #dict to store information in for each threshold\n",
    "    data = deepcopy(course_programs[i])\n",
    "    \n",
    "    data.set_index(['course', 'userid'], drop = True, inplace = True)\n",
    "    data.fillna(0, inplace = True)\n",
    "    \n",
    "    #set X and Y columns\n",
    "    X = data[data.columns[:25]] #different timesteps\n",
    "    y = data[data.columns[-2:]] #the 4 different putative targets\n",
    "    \n",
    "    for k in tqdm(targets):\n",
    "        print(k)\n",
    "        \n",
    "        #Start with train test split\n",
    "        X_train_val, X_test, y_train_val, y_test, = train_test_split(\n",
    "                                    X,\n",
    "                                   y[k], #replace when going for multi-target \n",
    "                                   test_size = 0.20,\n",
    "                                   random_state = 15,\n",
    "                                   shuffle=True,\n",
    "                                   stratify = y[k] #replace when going for multi-target\n",
    "                                    )\n",
    "        \n",
    "        #create dict to store fold performance\n",
    "        foldperf={}\n",
    "        \n",
    "        #reset \"best accuracy for treshold i and target k\"\n",
    "        best_accuracy = 0\n",
    "\n",
    "        #make train_val split\n",
    "        for fold, (train_idx,val_idx) in tqdm(enumerate(splits.split(X_train_val, y_train_val))):\n",
    "\n",
    "            print('Split {}'.format(fold + 1))\n",
    "            \n",
    "            #make split between train and Val\n",
    "            X_train, y_train = X_train_val.iloc[train_idx], y_train_val.iloc[train_idx]\n",
    "            X_val, y_val = X_train_val.iloc[val_idx], y_train_val.iloc[val_idx]\n",
    "            \n",
    "            #apply scaling after \n",
    "            X_train, X_val = normalize(X_train, X_val, 'Standard')\n",
    "            \n",
    "            #second, convert everything to pytorch tensor - we will convert to tensor dataset and \n",
    "            X_train_tensors = Variable(torch.Tensor(X_train.values))\n",
    "            X_val_tensors = Variable(torch.Tensor(X_val.values))\n",
    "\n",
    "            y_train_tensors = Variable(torch.Tensor(y_train.values))\n",
    "            y_val_tensors = Variable(torch.Tensor(y_val.values)) \n",
    "\n",
    "            #reshaping to rows, timestamps, features \n",
    "            X_train_tensors = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], X_train_tensors.shape[1], 1))\n",
    "            X_val_tensors = torch.reshape(X_val_tensors,  (X_val_tensors.shape[0], X_val_tensors.shape[1], 1))\n",
    "        \n",
    "            #convert y tensors to format longtensor\n",
    "            y_train_tensors = y_train_tensors.type(torch.cuda.LongTensor)\n",
    "            y_val_tensors = y_val_tensors.type(torch.cuda.LongTensor)\n",
    "            \n",
    "            #create Tensor Datasets and dataloaders for both Train and Val\n",
    "            train_dataset = TensorDataset(X_train_tensors, y_train_tensors)\n",
    "            val_dataset = TensorDataset(X_val_tensors, y_val_tensors)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "            #creates new model for each \n",
    "            model = LSTM_Uni(num_classes, input_size, hidden_size, num_layers, X_train_tensors.shape[1]).to('cuda') #our lstm class\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "            scheduler = ReduceLROnPlateau(optimizer, \n",
    "                                  'min', \n",
    "                                  patience = 10,\n",
    "                                  cooldown = 20,\n",
    "                                 verbose = True)\n",
    "    \n",
    "            history = {'train_loss': [], 'val_loss': [],'train_acc':[],'val_acc':[], 'precision': [],\n",
    "                      'recall' : [], 'auroc': []}\n",
    "\n",
    "            for epoch in tqdm(range(num_epochs)):\n",
    "                train_loss, train_correct=train_epoch(model,train_loader,criterion,optimizer)\n",
    "                val_loss, val_correct, precision, recall, auroc = valid_epoch(model,val_loader,criterion)\n",
    "\n",
    "                train_loss = train_loss / len(train_loader.sampler)\n",
    "                train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "                val_loss = val_loss / len(val_loader.sampler)\n",
    "                val_acc = val_correct / len(val_loader.sampler) * 100\n",
    "        \n",
    "        \n",
    "                if (epoch+1) % 10 == 0: \n",
    "                    print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Validation Loss:{:.3f} AVG Training Acc {:.2f} % AVG Validation Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                             num_epochs,\n",
    "                                                                                                             train_loss,\n",
    "                                                                                                             val_loss,\n",
    "                                                                                                             train_acc,\n",
    "                                                                                                             val_acc))\n",
    "                history['train_loss'].append(train_loss)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['train_acc'].append(train_acc)\n",
    "                history['val_acc'].append(val_acc)\n",
    "                history['precision'].append(precision)\n",
    "                history['recall'].append(recall)\n",
    "                history['auroc'].append(auroc)\n",
    "                scheduler.step(val_loss)\n",
    "    \n",
    "                if val_acc > best_accuracy:\n",
    "            \n",
    "                #replace best accuracy and save best model\n",
    "                    print(f'New Best Accuracy found: {val_acc:.2f}%\\nEpoch: {epoch + 1}')\n",
    "                    best_accuracy = val_acc\n",
    "                    best = deepcopy(model)\n",
    "                    curr_epoch = epoch + 1\n",
    "                    \n",
    "            #store fold performance\n",
    "            foldperf['fold{}'.format(fold+1)] = history\n",
    "        \n",
    "        #saves fold performance for target \n",
    "        threshold_dict[k] = pd.DataFrame.from_dict(foldperf, orient='index') # convert dict to dataframe\n",
    "        \n",
    "        #explode to get eacxh epoch as a row\n",
    "        threshold_dict[k] = threshold_dict[k].explode(list(threshold_dict[k].columns))\n",
    "        torch.save(best,f\"../Models/{i}/R_Gonz_best_{k}_{curr_epoch}_epochs_relative_clicks.h\")\n",
    "        \n",
    "    # from pandas.io.parsers import ExcelWriter\n",
    "    with pd.ExcelWriter(f\"../Data/Modeling Stage/Results/R_Gonz/Clicks per % duration/25_splits_{i}_{replicas}_replicas_relative_clicks.xlsx\") as writer:  \n",
    "        for sheet in targets:\n",
    "                threshold_dict[sheet].to_excel(writer, sheet_name=str(sheet))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
