{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84eda90e",
   "metadata": {},
   "source": [
    "### Thesis notebook 4.3. - NOVA IMS\n",
    "\n",
    "#### LSTM - Temporal data representation\n",
    "\n",
    "In this notebook, we will finally start our application of temporal representation using LSTMs and bi-directional LSTMs.\n",
    "The argument for the usage of Deep Learning stems from the fact that sequences themselves encode information that can be extracted using Recurrent Neural Networks and, more specifically, Long Short Term Memory Units.\n",
    "\n",
    "#### First Step: Setup a PyTorch environment that enables the use of GPU for training. \n",
    "\n",
    "The following cell wll confirm that the GPU will be the default device to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f27844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "cuda.init()\n",
    "## Get Id of default device\n",
    "torch.cuda.current_device()\n",
    "# 0\n",
    "cuda.Device(0).name() # '0' is the id of your GPU\n",
    "\n",
    "#set all tensors to gpu\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95429e",
   "metadata": {},
   "source": [
    "#### Second Step: Import the relevant packages and declare global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c2d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules/libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "#tqdm to monitor progress\n",
    "from tqdm.notebook import tqdm, trange\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "#time related features\n",
    "from datetime import timedelta\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "#vizualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#imblearn, scalers, kfold and metrics\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer,PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, recall_score, classification_report, average_precision_score, precision_recall_curve\n",
    "\n",
    "#import torch related\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "#and optimizer of learning rate\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "#import pytorch modules\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c3f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables that may come in handy\n",
    "#course threshold sets the % duration that will be considered (1 = 100%)\n",
    "duration_threshold = [0.1, 0.25, 0.33, 0.5, 1]\n",
    "\n",
    "#colors for vizualizations\n",
    "nova_ims_colors = ['#BFD72F', '#5C666C']\n",
    "\n",
    "#standard color for student aggregates\n",
    "student_color = '#474838'\n",
    "\n",
    "#standard color for course aggragates\n",
    "course_color = '#1B3D2F'\n",
    "\n",
    "#standard continuous colormap\n",
    "standard_cmap = 'viridis_r'\n",
    "\n",
    "#Function designed to deal with multiindex and flatten it\n",
    "def flattenHierarchicalCol(col,sep = '_'):\n",
    "    '''converts multiindex columns into single index columns while retaining the hierarchical components'''\n",
    "    if not type(col) is tuple:\n",
    "        return col\n",
    "    else:\n",
    "        new_col = ''\n",
    "        for leveli,level in enumerate(col):\n",
    "            if not level == '':\n",
    "                if not leveli == 0:\n",
    "                    new_col += sep\n",
    "                new_col += level\n",
    "        return new_col\n",
    "    \n",
    "#number of replicas - number of repeats of stratified k fold - in this case 10\n",
    "replicas = 30\n",
    "\n",
    "#names to display on result figures\n",
    "date_names = {\n",
    "             'Date_threshold_10': '10% of Course Duration',   \n",
    "             'Date_threshold_25': '25% of Course Duration', \n",
    "             'Date_threshold_33': '33% of Course Duration', \n",
    "             'Date_threshold_50': '50% of Course Duration', \n",
    "             'Date_threshold_100':'100% of Course Duration', \n",
    "            }\n",
    "\n",
    "target_names = {\n",
    "                'exam_fail' : 'At risk - Exam Grade',\n",
    "                'final_fail' : 'At risk - Final Grade', \n",
    "                'exam_gifted' : 'High performer - Exam Grade', \n",
    "                'final_gifted': 'High performer - Final Grade'\n",
    "                }\n",
    "\n",
    "#targets\n",
    "targets = ['exam_fail' , 'final_fail' , 'exam_gifted' , 'final_gifted']\n",
    "temporal_columns = ['0 to 4%', '4 to 8%', '8 to 12%', '12 to 16%', '16 to 20%', '20 to 24%',\n",
    "       '24 to 28%', '28 to 32%', '32 to 36%', '36 to 40%', '40 to 44%',\n",
    "       '44 to 48%', '48 to 52%', '52 to 56%', '56 to 60%', '60 to 64%',\n",
    "       '64 to 68%', '68 to 72%', '72 to 76%', '76 to 80%', '80 to 84%',\n",
    "       '84 to 88%', '88 to 92%', '92 to 96%', '96 to 100%']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c5ddb6",
   "metadata": {},
   "source": [
    "#### Step 3: Import data and take a preliminary look at it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8a23ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports dataframes\n",
    "course_programs = pd.read_excel(\"../Data/Modeling Stage/Nova_IMS_Temporal_Datasets_25_splits.xlsx\", \n",
    "                                dtype = {\n",
    "                                    'course_encoding' : int,\n",
    "                                    'userid' : int},\n",
    "                               sheet_name = None)\n",
    "\n",
    "#save tables \n",
    "student_list = pd.read_csv('../Data/Modeling Stage/Nova_IMS_Filtered_targets.csv', \n",
    "                         dtype = {\n",
    "                                   'course_encoding': int,\n",
    "                                   'userid' : int,\n",
    "                                   })\n",
    "\n",
    "#drop unnamed 0 column\n",
    "for i in course_programs:\n",
    "        \n",
    "    #merge with the targets we calculated on the other \n",
    "    course_programs[i] = course_programs[i].merge(student_list, on = ['course_encoding', 'userid'], how = 'inner')\n",
    "    course_programs[i].drop(['Unnamed: 0', 'exam_mark', 'final_mark'], axis = 1, inplace = True)\n",
    "    \n",
    "    #convert results to object\n",
    "    course_programs[i]['course_encoding'], course_programs[i]['userid'] = course_programs[i]['course_encoding'].astype(object), course_programs[i]['userid'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc3c84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9296 entries, 0 to 9295\n",
      "Data columns (total 31 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   course_encoding  9296 non-null   object\n",
      " 1   userid           9296 non-null   object\n",
      " 2   0 to 4%          9296 non-null   int64 \n",
      " 3   4 to 8%          9296 non-null   int64 \n",
      " 4   8 to 12%         9296 non-null   int64 \n",
      " 5   12 to 16%        9296 non-null   int64 \n",
      " 6   16 to 20%        9296 non-null   int64 \n",
      " 7   20 to 24%        9296 non-null   int64 \n",
      " 8   24 to 28%        9296 non-null   int64 \n",
      " 9   28 to 32%        9296 non-null   int64 \n",
      " 10  32 to 36%        9296 non-null   int64 \n",
      " 11  36 to 40%        9296 non-null   int64 \n",
      " 12  40 to 44%        9296 non-null   int64 \n",
      " 13  44 to 48%        9296 non-null   int64 \n",
      " 14  48 to 52%        9296 non-null   int64 \n",
      " 15  52 to 56%        9296 non-null   int64 \n",
      " 16  56 to 60%        9296 non-null   int64 \n",
      " 17  60 to 64%        9296 non-null   int64 \n",
      " 18  64 to 68%        9296 non-null   int64 \n",
      " 19  68 to 72%        9296 non-null   int64 \n",
      " 20  72 to 76%        9296 non-null   int64 \n",
      " 21  76 to 80%        9296 non-null   int64 \n",
      " 22  80 to 84%        9296 non-null   int64 \n",
      " 23  84 to 88%        9296 non-null   int64 \n",
      " 24  88 to 92%        9296 non-null   int64 \n",
      " 25  92 to 96%        9296 non-null   int64 \n",
      " 26  96 to 100%       9296 non-null   int64 \n",
      " 27  exam_fail        9296 non-null   int64 \n",
      " 28  final_fail       9296 non-null   int64 \n",
      " 29  exam_gifted      9296 non-null   int64 \n",
      " 30  final_gifted     9296 non-null   int64 \n",
      "dtypes: int64(29), object(2)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "course_programs['Date_threshold_100'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a751ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_encoding</th>\n",
       "      <th>userid</th>\n",
       "      <th>0 to 4%</th>\n",
       "      <th>4 to 8%</th>\n",
       "      <th>8 to 12%</th>\n",
       "      <th>12 to 16%</th>\n",
       "      <th>16 to 20%</th>\n",
       "      <th>20 to 24%</th>\n",
       "      <th>24 to 28%</th>\n",
       "      <th>28 to 32%</th>\n",
       "      <th>...</th>\n",
       "      <th>76 to 80%</th>\n",
       "      <th>80 to 84%</th>\n",
       "      <th>84 to 88%</th>\n",
       "      <th>88 to 92%</th>\n",
       "      <th>92 to 96%</th>\n",
       "      <th>96 to 100%</th>\n",
       "      <th>exam_fail</th>\n",
       "      <th>final_fail</th>\n",
       "      <th>exam_gifted</th>\n",
       "      <th>final_gifted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9296.0</td>\n",
       "      <td>9296.0</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.0</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>138.0</td>\n",
       "      <td>1590.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>150.0</td>\n",
       "      <td>3178.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>178.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.081863</td>\n",
       "      <td>8.307874</td>\n",
       "      <td>10.752797</td>\n",
       "      <td>11.193739</td>\n",
       "      <td>10.127797</td>\n",
       "      <td>8.966652</td>\n",
       "      <td>10.545396</td>\n",
       "      <td>11.445245</td>\n",
       "      <td>...</td>\n",
       "      <td>11.718051</td>\n",
       "      <td>13.136403</td>\n",
       "      <td>22.827883</td>\n",
       "      <td>27.341007</td>\n",
       "      <td>12.599613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.201377</td>\n",
       "      <td>0.149957</td>\n",
       "      <td>0.276893</td>\n",
       "      <td>0.308090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.526351</td>\n",
       "      <td>13.580025</td>\n",
       "      <td>13.626754</td>\n",
       "      <td>16.400023</td>\n",
       "      <td>14.291254</td>\n",
       "      <td>12.180177</td>\n",
       "      <td>13.507892</td>\n",
       "      <td>15.932226</td>\n",
       "      <td>...</td>\n",
       "      <td>28.186874</td>\n",
       "      <td>36.690068</td>\n",
       "      <td>47.158607</td>\n",
       "      <td>54.963959</td>\n",
       "      <td>35.194597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.401051</td>\n",
       "      <td>0.357048</td>\n",
       "      <td>0.447487</td>\n",
       "      <td>0.461729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>619.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>1091.000000</td>\n",
       "      <td>604.000000</td>\n",
       "      <td>747.000000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        course_encoding  userid      0 to 4%      4 to 8%     8 to 12%  \\\n",
       "count            9296.0  9296.0  9296.000000  9296.000000  9296.000000   \n",
       "unique            138.0  1590.0          NaN          NaN          NaN   \n",
       "top               150.0  3178.0          NaN          NaN          NaN   \n",
       "freq              178.0    14.0          NaN          NaN          NaN   \n",
       "mean                NaN     NaN     1.081863     8.307874    10.752797   \n",
       "std                 NaN     NaN     3.526351    13.580025    13.626754   \n",
       "min                 NaN     NaN     0.000000     0.000000     0.000000   \n",
       "25%                 NaN     NaN     0.000000     0.000000     1.000000   \n",
       "50%                 NaN     NaN     0.000000     2.000000     7.000000   \n",
       "75%                 NaN     NaN     1.000000    12.000000    15.000000   \n",
       "max                 NaN     NaN    66.000000   269.000000   360.000000   \n",
       "\n",
       "          12 to 16%    16 to 20%    20 to 24%    24 to 28%    28 to 32%  ...  \\\n",
       "count   9296.000000  9296.000000  9296.000000  9296.000000  9296.000000  ...   \n",
       "unique          NaN          NaN          NaN          NaN          NaN  ...   \n",
       "top             NaN          NaN          NaN          NaN          NaN  ...   \n",
       "freq            NaN          NaN          NaN          NaN          NaN  ...   \n",
       "mean      11.193739    10.127797     8.966652    10.545396    11.445245  ...   \n",
       "std       16.400023    14.291254    12.180177    13.507892    15.932226  ...   \n",
       "min        0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%        2.000000     2.000000     1.000000     2.000000     3.000000  ...   \n",
       "50%        7.000000     6.000000     5.000000     7.000000     7.000000  ...   \n",
       "75%       15.000000    13.000000    13.000000    14.000000    14.000000  ...   \n",
       "max      619.000000   315.000000   248.000000   268.000000   237.000000  ...   \n",
       "\n",
       "          76 to 80%    80 to 84%    84 to 88%    88 to 92%    92 to 96%  \\\n",
       "count   9296.000000  9296.000000  9296.000000  9296.000000  9296.000000   \n",
       "unique          NaN          NaN          NaN          NaN          NaN   \n",
       "top             NaN          NaN          NaN          NaN          NaN   \n",
       "freq            NaN          NaN          NaN          NaN          NaN   \n",
       "mean      11.718051    13.136403    22.827883    27.341007    12.599613   \n",
       "std       28.186874    36.690068    47.158607    54.963959    35.194597   \n",
       "min        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%        2.000000     2.000000     4.000000     2.000000     0.000000   \n",
       "75%       10.000000    10.000000    23.000000    27.000000     5.000000   \n",
       "max      614.000000  1091.000000   604.000000   747.000000   407.000000   \n",
       "\n",
       "        96 to 100%    exam_fail   final_fail  exam_gifted  final_gifted  \n",
       "count       9296.0  9296.000000  9296.000000  9296.000000   9296.000000  \n",
       "unique         NaN          NaN          NaN          NaN           NaN  \n",
       "top            NaN          NaN          NaN          NaN           NaN  \n",
       "freq           NaN          NaN          NaN          NaN           NaN  \n",
       "mean           0.0     0.201377     0.149957     0.276893      0.308090  \n",
       "std            0.0     0.401051     0.357048     0.447487      0.461729  \n",
       "min            0.0     0.000000     0.000000     0.000000      0.000000  \n",
       "25%            0.0     0.000000     0.000000     0.000000      0.000000  \n",
       "50%            0.0     0.000000     0.000000     0.000000      0.000000  \n",
       "75%            0.0     0.000000     0.000000     1.000000      1.000000  \n",
       "max            0.0     1.000000     1.000000     1.000000      1.000000  \n",
       "\n",
       "[11 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_programs['Date_threshold_100'].describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3291817",
   "metadata": {},
   "source": [
    "In our second attempt, we are looking to obtain a different result. Instead of using the absolute number of clicks used in each instance, we are instead looking to use the percent number of clicks made by each student relative to the the total number of clicks performed in the curricular unit.\n",
    "\n",
    "For that we will use transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5b9e350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98455f6b446741cd93226dc0f55e09a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667e21f8c4584a56ae1c2c5541c0e6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06c70a083af4416901b7edba1c567f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f223af5960462da5a1b47f4843c642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4459cc6c5ef046eca821abc5b05f58b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a8d14fa4524de084220f0977177b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(course_programs.keys()):\n",
    "    \n",
    "    for j in tqdm(temporal_columns):\n",
    "            course_programs[i][j] = np.where(course_programs[i].fillna(0).groupby('course_encoding')[j].transform('sum') != 0, #where valid operations occur\n",
    "                                             course_programs[i][j].fillna(0) / course_programs[i].fillna(0).groupby('course_encoding')[j].transform('sum') * 100, #calculate percentage\n",
    "                                             0) #otherwise, its 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be722ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for scalers\n",
    "def normalize(dataset,scaler):\n",
    "    \n",
    "    if scaler == 'MinMax':\n",
    "        pt = MinMaxScaler()\n",
    "    elif scaler == 'Standard':\n",
    "        pt = StandardScaler()\n",
    "    elif scaler == 'Robust':\n",
    "        pt = RobustScaler()\n",
    "    elif scaler == 'Quantile':\n",
    "        pt = QuantileTransformer()\n",
    "    else:\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "    \n",
    "    data = pt.fit_transform(dataset)\n",
    "    \n",
    "    # convert the array back to a dataframe\n",
    "    normalized_df = pd.DataFrame(data,columns=dataset.columns)\n",
    "    return normalized_df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ea7510e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1debccd9965341c7b3f0c9d8168b2eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create backup\n",
    "normalized_data = deepcopy(course_programs)\n",
    "\n",
    "#convert index\n",
    "for i in tqdm(normalized_data):\n",
    "    normalized_data[i].set_index(['course_encoding', 'userid'], drop = True, inplace = True)\n",
    "    normalized_data[i].fillna(0, inplace = True)\n",
    "    #Then, apply normalize function to rescale the train columns\n",
    "    normalized_data[i] = normalize(normalized_data[i].filter(temporal_columns),'Standard')\n",
    "    \n",
    "    #and remerge target columns\n",
    "    normalized_data[i][targets] =  deepcopy(course_programs[i][targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4d5475",
   "metadata": {},
   "source": [
    "#### Implementing Cross-Validation with Deep Learning Model\n",
    "\n",
    "**1. Create the Deep Learning Model**\n",
    "\n",
    "In this instance, we will follow-up with on the approach used in Chen & Cui - CrossEntropyLoss with applied over a softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a16bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Uni(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM_Uni, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first = True) #lstm\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "    \n",
    "        self.fc = nn.Linear(self.hidden_size, num_classes) #fully connected last layer\n",
    "\n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        \n",
    "        #Xavier_init for both H_0 and C_0\n",
    "        torch.nn.init.xavier_normal_(h_0)\n",
    "        torch.nn.init.xavier_normal_(c_0)\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        lstm_out, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        last_output = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        \n",
    "        #we are interested in only keeping the last output\n",
    "        drop_out = self.dropout(last_output)\n",
    "        pre_softmax = self.fc(drop_out) #Final Output - dense\n",
    "        return pre_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c356bd",
   "metadata": {},
   "source": [
    "**2. Define the train and validation Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25b29a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,dataloader,loss_fn,optimizer):\n",
    "    \n",
    "    train_loss,train_correct=0.0,0 \n",
    "    model.train()\n",
    "    for X, labels in dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = loss_fn(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * X.size(0)\n",
    "        scores, predictions = torch.max(F.log_softmax(output.data), 1)\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "    return train_loss,train_correct\n",
    "  \n",
    "def valid_epoch(model,dataloader,loss_fn):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    targets = []\n",
    "    y_pred = []\n",
    "    probability_1 = []\n",
    "    \n",
    "    model.eval()\n",
    "    for X, labels in dataloader:\n",
    "\n",
    "        output = model(X)\n",
    "        loss=loss_fn(output,labels)\n",
    "        valid_loss+=loss.item()*X.size(0)\n",
    "        probability_1.append(F.softmax(output.data)[:,1])\n",
    "        predictions = torch.argmax(output, dim=1)\n",
    "        val_correct+=(predictions == labels).sum().item()\n",
    "        targets.append(labels)\n",
    "        y_pred.append(predictions)\n",
    "    \n",
    "    #concat all results\n",
    "    targets = torch.cat(targets).data.cpu().numpy()\n",
    "    y_pred = torch.cat(y_pred).data.cpu().numpy()\n",
    "    probability_1 = torch.cat(probability_1).data.cpu().numpy()\n",
    "    \n",
    "    #calculate precision, recall and AUC score\n",
    "    \n",
    "    precision = precision_score(targets, y_pred)\n",
    "    recall = recall_score(targets, y_pred)\n",
    "    auroc = roc_auc_score(targets, probability_1)\n",
    "    \n",
    "    #return all\n",
    "    return valid_loss,val_correct, precision, recall, auroc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4543fb3",
   "metadata": {},
   "source": [
    "**3. Define main hyperparameters of the model, including splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcbbef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "num_epochs = 100 #50 epochs\n",
    "learning_rate = 0.01 #0.01 lr\n",
    "input_size = 1 #number of features\n",
    "hidden_size = 40 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "#Shape of Output as required for SoftMax Classifier\n",
    "num_classes = 2 #output shape\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "k=10\n",
    "splits= RepeatedKFold(n_splits=k, n_repeats=replicas, random_state=15) #kfold of 10 with 30 replicas\n",
    "criterion = nn.CrossEntropyLoss()    # cross-entropy for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20713d9",
   "metadata": {},
   "source": [
    "**4. Make the splits and Start Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45544589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d6db64bd3448d78131155875eb7549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date_threshold_10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e06fa0a94cd44408bdc3fdf924212bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exam_fail\n",
      "Before reshaping:\n",
      "Training Shape torch.Size([7436, 25]) torch.Size([7436])\n",
      "Testing Shape torch.Size([1860, 25]) torch.Size([1860])\n",
      "\n",
      "After reshaping:\n",
      "Training Shape torch.Size([7436, 25, 1]) torch.Size([7436])\n",
      "Testing Shape torch.Size([1860, 25, 1]) torch.Size([1860])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0385b91a35cc4b3e8c23fd4f58b32252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4eac65beb74a0b8aa68c863b179d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best Accuracy found: 81.45%\n",
      "Epoch: 1\n",
      "Epoch:10/100 AVG Training Loss:0.496 AVG Validation Loss:0.470 AVG Training Acc 79.71 % AVG Validation Acc 81.45 %\n",
      "New Best Accuracy found: 81.59%\n",
      "Epoch: 12\n",
      "Epoch:20/100 AVG Training Loss:0.490 AVG Validation Loss:0.470 AVG Training Acc 79.75 % AVG Validation Acc 81.59 %\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/100 AVG Training Loss:0.482 AVG Validation Loss:0.473 AVG Training Acc 79.81 % AVG Validation Acc 81.18 %\n",
      "Epoch:40/100 AVG Training Loss:0.478 AVG Validation Loss:0.475 AVG Training Acc 79.93 % AVG Validation Acc 80.91 %\n",
      "Epoch:50/100 AVG Training Loss:0.472 AVG Validation Loss:0.477 AVG Training Acc 80.17 % AVG Validation Acc 81.05 %\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/100 AVG Training Loss:0.462 AVG Validation Loss:0.483 AVG Training Acc 80.38 % AVG Validation Acc 80.65 %\n",
      "Epoch:70/100 AVG Training Loss:0.463 AVG Validation Loss:0.486 AVG Training Acc 80.22 % AVG Validation Acc 80.51 %\n",
      "Epoch:80/100 AVG Training Loss:0.459 AVG Validation Loss:0.488 AVG Training Acc 80.35 % AVG Validation Acc 80.51 %\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/100 AVG Training Loss:0.460 AVG Validation Loss:0.488 AVG Training Acc 80.27 % AVG Validation Acc 80.65 %\n",
      "Epoch:100/100 AVG Training Loss:0.460 AVG Validation Loss:0.488 AVG Training Acc 80.26 % AVG Validation Acc 80.65 %\n",
      "Split 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897d1794de834dae9bd65822d1cee49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.497 AVG Validation Loss:0.485 AVG Training Acc 79.80 % AVG Validation Acc 80.78 %\n",
      "Epoch:20/100 AVG Training Loss:0.490 AVG Validation Loss:0.483 AVG Training Acc 79.84 % AVG Validation Acc 80.51 %\n",
      "Epoch:30/100 AVG Training Loss:0.489 AVG Validation Loss:0.479 AVG Training Acc 79.75 % AVG Validation Acc 80.65 %\n",
      "Epoch:40/100 AVG Training Loss:0.481 AVG Validation Loss:0.488 AVG Training Acc 79.90 % AVG Validation Acc 79.84 %\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:50/100 AVG Training Loss:0.468 AVG Validation Loss:0.482 AVG Training Acc 80.25 % AVG Validation Acc 80.38 %\n",
      "Epoch:60/100 AVG Training Loss:0.457 AVG Validation Loss:0.486 AVG Training Acc 80.60 % AVG Validation Acc 80.11 %\n",
      "Epoch:70/100 AVG Training Loss:0.454 AVG Validation Loss:0.501 AVG Training Acc 80.72 % AVG Validation Acc 80.38 %\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/100 AVG Training Loss:0.451 AVG Validation Loss:0.490 AVG Training Acc 80.87 % AVG Validation Acc 80.65 %\n",
      "Epoch:90/100 AVG Training Loss:0.449 AVG Validation Loss:0.497 AVG Training Acc 80.71 % AVG Validation Acc 80.38 %\n",
      "Epoch:100/100 AVG Training Loss:0.447 AVG Validation Loss:0.499 AVG Training Acc 80.89 % AVG Validation Acc 80.51 %\n",
      "Split 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a824ae01be14582a219e14d877fa538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.497 AVG Validation Loss:0.508 AVG Training Acc 79.92 % AVG Validation Acc 78.90 %\n",
      "Epoch:20/100 AVG Training Loss:0.487 AVG Validation Loss:0.508 AVG Training Acc 80.01 % AVG Validation Acc 78.90 %\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/100 AVG Training Loss:0.482 AVG Validation Loss:0.509 AVG Training Acc 80.07 % AVG Validation Acc 78.90 %\n",
      "Epoch:40/100 AVG Training Loss:0.476 AVG Validation Loss:0.514 AVG Training Acc 80.19 % AVG Validation Acc 78.76 %\n",
      "Epoch:50/100 AVG Training Loss:0.470 AVG Validation Loss:0.517 AVG Training Acc 80.36 % AVG Validation Acc 78.36 %\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/100 AVG Training Loss:0.466 AVG Validation Loss:0.524 AVG Training Acc 80.63 % AVG Validation Acc 78.23 %\n",
      "Epoch:70/100 AVG Training Loss:0.464 AVG Validation Loss:0.528 AVG Training Acc 80.69 % AVG Validation Acc 78.09 %\n",
      "Epoch:80/100 AVG Training Loss:0.462 AVG Validation Loss:0.528 AVG Training Acc 80.59 % AVG Validation Acc 78.09 %\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/100 AVG Training Loss:0.463 AVG Validation Loss:0.530 AVG Training Acc 80.66 % AVG Validation Acc 77.96 %\n",
      "Epoch:100/100 AVG Training Loss:0.462 AVG Validation Loss:0.529 AVG Training Acc 80.68 % AVG Validation Acc 77.82 %\n",
      "Split 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f78d1ae80cb4f8992d866e54fcfafe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.492 AVG Validation Loss:0.509 AVG Training Acc 80.11 % AVG Validation Acc 77.82 %\n",
      "Epoch:20/100 AVG Training Loss:0.489 AVG Validation Loss:0.520 AVG Training Acc 80.11 % AVG Validation Acc 77.96 %\n",
      "Epoch:30/100 AVG Training Loss:0.484 AVG Validation Loss:0.517 AVG Training Acc 80.22 % AVG Validation Acc 77.82 %\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/100 AVG Training Loss:0.471 AVG Validation Loss:0.525 AVG Training Acc 80.62 % AVG Validation Acc 77.02 %\n",
      "Epoch:50/100 AVG Training Loss:0.467 AVG Validation Loss:0.527 AVG Training Acc 80.68 % AVG Validation Acc 77.15 %\n",
      "Epoch:60/100 AVG Training Loss:0.459 AVG Validation Loss:0.528 AVG Training Acc 80.84 % AVG Validation Acc 76.88 %\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/100 AVG Training Loss:0.458 AVG Validation Loss:0.530 AVG Training Acc 81.01 % AVG Validation Acc 77.15 %\n",
      "Epoch:80/100 AVG Training Loss:0.457 AVG Validation Loss:0.529 AVG Training Acc 80.99 % AVG Validation Acc 77.02 %\n",
      "Epoch:90/100 AVG Training Loss:0.455 AVG Validation Loss:0.530 AVG Training Acc 81.17 % AVG Validation Acc 76.88 %\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/100 AVG Training Loss:0.455 AVG Validation Loss:0.531 AVG Training Acc 80.98 % AVG Validation Acc 76.88 %\n",
      "Split 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac663c7d4454ceda387ce4419e73595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.494 AVG Validation Loss:0.507 AVG Training Acc 79.96 % AVG Validation Acc 79.30 %\n",
      "Epoch:20/100 AVG Training Loss:0.489 AVG Validation Loss:0.496 AVG Training Acc 79.93 % AVG Validation Acc 79.30 %\n",
      "Epoch:30/100 AVG Training Loss:0.481 AVG Validation Loss:0.495 AVG Training Acc 80.04 % AVG Validation Acc 79.17 %\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/100 AVG Training Loss:0.465 AVG Validation Loss:0.508 AVG Training Acc 80.33 % AVG Validation Acc 78.90 %\n",
      "Epoch:50/100 AVG Training Loss:0.457 AVG Validation Loss:0.520 AVG Training Acc 80.78 % AVG Validation Acc 78.76 %\n",
      "Epoch:60/100 AVG Training Loss:0.448 AVG Validation Loss:0.527 AVG Training Acc 80.98 % AVG Validation Acc 78.09 %\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/100 AVG Training Loss:0.444 AVG Validation Loss:0.529 AVG Training Acc 81.10 % AVG Validation Acc 78.23 %\n",
      "Epoch:80/100 AVG Training Loss:0.441 AVG Validation Loss:0.536 AVG Training Acc 81.17 % AVG Validation Acc 78.23 %\n",
      "Epoch:90/100 AVG Training Loss:0.439 AVG Validation Loss:0.538 AVG Training Acc 81.23 % AVG Validation Acc 78.09 %\n",
      "Epoch    96: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/100 AVG Training Loss:0.437 AVG Validation Loss:0.541 AVG Training Acc 81.38 % AVG Validation Acc 77.55 %\n",
      "Split 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a343d8003703443c93afdf4c62d49468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best Accuracy found: 82.26%\n",
      "Epoch: 1\n",
      "Epoch:10/100 AVG Training Loss:0.502 AVG Validation Loss:0.453 AVG Training Acc 79.60 % AVG Validation Acc 82.26 %\n",
      "New Best Accuracy found: 82.39%\n",
      "Epoch: 15\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/100 AVG Training Loss:0.490 AVG Validation Loss:0.448 AVG Training Acc 79.65 % AVG Validation Acc 82.53 %\n",
      "New Best Accuracy found: 82.53%\n",
      "Epoch: 20\n",
      "Epoch:30/100 AVG Training Loss:0.488 AVG Validation Loss:0.447 AVG Training Acc 79.74 % AVG Validation Acc 82.39 %\n",
      "Epoch:40/100 AVG Training Loss:0.486 AVG Validation Loss:0.447 AVG Training Acc 79.71 % AVG Validation Acc 82.26 %\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/100 AVG Training Loss:0.480 AVG Validation Loss:0.448 AVG Training Acc 79.81 % AVG Validation Acc 81.99 %\n",
      "Epoch:60/100 AVG Training Loss:0.480 AVG Validation Loss:0.449 AVG Training Acc 79.86 % AVG Validation Acc 82.12 %\n",
      "Epoch:70/100 AVG Training Loss:0.478 AVG Validation Loss:0.448 AVG Training Acc 79.95 % AVG Validation Acc 82.12 %\n",
      "Epoch:80/100 AVG Training Loss:0.477 AVG Validation Loss:0.451 AVG Training Acc 80.05 % AVG Validation Acc 82.12 %\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/100 AVG Training Loss:0.477 AVG Validation Loss:0.450 AVG Training Acc 79.99 % AVG Validation Acc 82.12 %\n",
      "Epoch:100/100 AVG Training Loss:0.477 AVG Validation Loss:0.449 AVG Training Acc 80.10 % AVG Validation Acc 82.12 %\n",
      "Split 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2234875f82104b52a4e2ad477d85468e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.494 AVG Validation Loss:0.487 AVG Training Acc 79.74 % AVG Validation Acc 81.16 %\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/100 AVG Training Loss:0.485 AVG Validation Loss:0.490 AVG Training Acc 79.90 % AVG Validation Acc 81.02 %\n",
      "Epoch:30/100 AVG Training Loss:0.484 AVG Validation Loss:0.491 AVG Training Acc 79.93 % AVG Validation Acc 80.89 %\n",
      "Epoch:40/100 AVG Training Loss:0.483 AVG Validation Loss:0.489 AVG Training Acc 79.98 % AVG Validation Acc 80.75 %\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/100 AVG Training Loss:0.479 AVG Validation Loss:0.491 AVG Training Acc 79.89 % AVG Validation Acc 80.75 %\n",
      "Epoch:60/100 AVG Training Loss:0.478 AVG Validation Loss:0.492 AVG Training Acc 79.98 % AVG Validation Acc 80.62 %\n",
      "Epoch:70/100 AVG Training Loss:0.479 AVG Validation Loss:0.492 AVG Training Acc 80.08 % AVG Validation Acc 80.62 %\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:80/100 AVG Training Loss:0.479 AVG Validation Loss:0.493 AVG Training Acc 80.07 % AVG Validation Acc 80.62 %\n",
      "Epoch:90/100 AVG Training Loss:0.477 AVG Validation Loss:0.492 AVG Training Acc 80.11 % AVG Validation Acc 80.62 %\n",
      "Epoch:100/100 AVG Training Loss:0.477 AVG Validation Loss:0.492 AVG Training Acc 80.07 % AVG Validation Acc 80.62 %\n",
      "Split 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c92f40c8189d4a4f975f6a65173ce17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.504 AVG Validation Loss:0.477 AVG Training Acc 79.68 % AVG Validation Acc 81.16 %\n",
      "Epoch:20/100 AVG Training Loss:0.492 AVG Validation Loss:0.462 AVG Training Acc 79.74 % AVG Validation Acc 81.16 %\n",
      "Epoch:30/100 AVG Training Loss:0.485 AVG Validation Loss:0.468 AVG Training Acc 79.75 % AVG Validation Acc 81.02 %\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/100 AVG Training Loss:0.462 AVG Validation Loss:0.478 AVG Training Acc 80.59 % AVG Validation Acc 80.75 %\n",
      "Epoch:50/100 AVG Training Loss:0.455 AVG Validation Loss:0.483 AVG Training Acc 80.70 % AVG Validation Acc 80.48 %\n",
      "Epoch:60/100 AVG Training Loss:0.449 AVG Validation Loss:0.487 AVG Training Acc 81.07 % AVG Validation Acc 80.08 %\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/100 AVG Training Loss:0.438 AVG Validation Loss:0.494 AVG Training Acc 81.46 % AVG Validation Acc 79.95 %\n",
      "Epoch:80/100 AVG Training Loss:0.437 AVG Validation Loss:0.485 AVG Training Acc 81.62 % AVG Validation Acc 80.75 %\n",
      "Epoch:90/100 AVG Training Loss:0.436 AVG Validation Loss:0.492 AVG Training Acc 81.41 % AVG Validation Acc 80.22 %\n",
      "Epoch    95: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/100 AVG Training Loss:0.434 AVG Validation Loss:0.496 AVG Training Acc 81.76 % AVG Validation Acc 79.81 %\n",
      "Split 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d52db39fa8548b69aa547270f59a036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.494 AVG Validation Loss:0.503 AVG Training Acc 79.93 % AVG Validation Acc 79.00 %\n",
      "Epoch:20/100 AVG Training Loss:0.493 AVG Validation Loss:0.501 AVG Training Acc 80.07 % AVG Validation Acc 79.00 %\n",
      "Epoch:30/100 AVG Training Loss:0.481 AVG Validation Loss:0.488 AVG Training Acc 79.95 % AVG Validation Acc 79.14 %\n",
      "Epoch:40/100 AVG Training Loss:0.477 AVG Validation Loss:0.500 AVG Training Acc 80.23 % AVG Validation Acc 79.14 %\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:50/100 AVG Training Loss:0.460 AVG Validation Loss:0.499 AVG Training Acc 80.62 % AVG Validation Acc 79.27 %\n",
      "Epoch:60/100 AVG Training Loss:0.453 AVG Validation Loss:0.500 AVG Training Acc 80.80 % AVG Validation Acc 79.27 %\n",
      "Epoch:70/100 AVG Training Loss:0.444 AVG Validation Loss:0.504 AVG Training Acc 80.98 % AVG Validation Acc 79.27 %\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/100 AVG Training Loss:0.444 AVG Validation Loss:0.507 AVG Training Acc 81.05 % AVG Validation Acc 79.41 %\n",
      "Epoch:90/100 AVG Training Loss:0.441 AVG Validation Loss:0.509 AVG Training Acc 81.10 % AVG Validation Acc 79.14 %\n",
      "Epoch:100/100 AVG Training Loss:0.440 AVG Validation Loss:0.515 AVG Training Acc 81.11 % AVG Validation Acc 79.27 %\n",
      "Split 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a498c55388e4855a52d2f36d6fe544c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.488 AVG Validation Loss:0.538 AVG Training Acc 80.17 % AVG Validation Acc 76.99 %\n",
      "Epoch:20/100 AVG Training Loss:0.484 AVG Validation Loss:0.544 AVG Training Acc 80.23 % AVG Validation Acc 76.85 %\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/100 AVG Training Loss:0.472 AVG Validation Loss:0.551 AVG Training Acc 80.50 % AVG Validation Acc 76.58 %\n",
      "Epoch:40/100 AVG Training Loss:0.463 AVG Validation Loss:0.561 AVG Training Acc 80.83 % AVG Validation Acc 76.58 %\n",
      "Epoch:50/100 AVG Training Loss:0.454 AVG Validation Loss:0.569 AVG Training Acc 81.07 % AVG Validation Acc 76.04 %\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/100 AVG Training Loss:0.450 AVG Validation Loss:0.576 AVG Training Acc 81.20 % AVG Validation Acc 76.31 %\n",
      "Epoch:70/100 AVG Training Loss:0.446 AVG Validation Loss:0.578 AVG Training Acc 81.25 % AVG Validation Acc 76.18 %\n",
      "Epoch:80/100 AVG Training Loss:0.448 AVG Validation Loss:0.577 AVG Training Acc 81.26 % AVG Validation Acc 76.31 %\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/100 AVG Training Loss:0.446 AVG Validation Loss:0.579 AVG Training Acc 81.41 % AVG Validation Acc 76.18 %\n",
      "Epoch:100/100 AVG Training Loss:0.445 AVG Validation Loss:0.581 AVG Training Acc 81.38 % AVG Validation Acc 76.18 %\n",
      "Split 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb410360756c4fbe957e69d80765ddba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best Accuracy found: 82.93%\n",
      "Epoch: 1\n",
      "Epoch:10/100 AVG Training Loss:0.500 AVG Validation Loss:0.456 AVG Training Acc 79.45 % AVG Validation Acc 82.93 %\n",
      "Epoch:20/100 AVG Training Loss:0.491 AVG Validation Loss:0.454 AVG Training Acc 79.62 % AVG Validation Acc 82.39 %\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/100 AVG Training Loss:0.481 AVG Validation Loss:0.453 AVG Training Acc 79.75 % AVG Validation Acc 82.12 %\n",
      "Epoch:40/100 AVG Training Loss:0.471 AVG Validation Loss:0.458 AVG Training Acc 79.92 % AVG Validation Acc 81.59 %\n",
      "Epoch:50/100 AVG Training Loss:0.462 AVG Validation Loss:0.458 AVG Training Acc 80.25 % AVG Validation Acc 81.99 %\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/100 AVG Training Loss:0.456 AVG Validation Loss:0.460 AVG Training Acc 80.65 % AVG Validation Acc 81.45 %\n",
      "Epoch:70/100 AVG Training Loss:0.451 AVG Validation Loss:0.461 AVG Training Acc 80.93 % AVG Validation Acc 81.45 %\n",
      "Epoch:80/100 AVG Training Loss:0.450 AVG Validation Loss:0.466 AVG Training Acc 80.66 % AVG Validation Acc 81.45 %\n",
      "Epoch:90/100 AVG Training Loss:0.450 AVG Validation Loss:0.467 AVG Training Acc 80.75 % AVG Validation Acc 80.91 %\n",
      "Epoch    90: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/100 AVG Training Loss:0.451 AVG Validation Loss:0.468 AVG Training Acc 80.68 % AVG Validation Acc 81.18 %\n",
      "Split 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d03d3e481342ef9f2af574f30fc69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.496 AVG Validation Loss:0.492 AVG Training Acc 79.83 % AVG Validation Acc 79.44 %\n",
      "Epoch:20/100 AVG Training Loss:0.490 AVG Validation Loss:0.489 AVG Training Acc 79.89 % AVG Validation Acc 79.57 %\n",
      "Epoch:30/100 AVG Training Loss:0.489 AVG Validation Loss:0.484 AVG Training Acc 79.83 % AVG Validation Acc 79.57 %\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/100 AVG Training Loss:0.476 AVG Validation Loss:0.484 AVG Training Acc 80.11 % AVG Validation Acc 79.57 %\n",
      "Epoch:50/100 AVG Training Loss:0.470 AVG Validation Loss:0.490 AVG Training Acc 80.25 % AVG Validation Acc 79.30 %\n",
      "Epoch:60/100 AVG Training Loss:0.464 AVG Validation Loss:0.496 AVG Training Acc 80.42 % AVG Validation Acc 78.90 %\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/100 AVG Training Loss:0.460 AVG Validation Loss:0.496 AVG Training Acc 80.60 % AVG Validation Acc 78.36 %\n",
      "Epoch:80/100 AVG Training Loss:0.460 AVG Validation Loss:0.497 AVG Training Acc 80.80 % AVG Validation Acc 78.49 %\n",
      "Epoch:90/100 AVG Training Loss:0.460 AVG Validation Loss:0.500 AVG Training Acc 80.56 % AVG Validation Acc 78.49 %\n",
      "Epoch    96: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/100 AVG Training Loss:0.458 AVG Validation Loss:0.500 AVG Training Acc 80.68 % AVG Validation Acc 78.76 %\n",
      "Split 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56925d89374a44a6aaae512cd4aadc81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.498 AVG Validation Loss:0.476 AVG Training Acc 79.62 % AVG Validation Acc 81.59 %\n",
      "Epoch:20/100 AVG Training Loss:0.493 AVG Validation Loss:0.478 AVG Training Acc 79.69 % AVG Validation Acc 81.45 %\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/100 AVG Training Loss:0.480 AVG Validation Loss:0.482 AVG Training Acc 79.92 % AVG Validation Acc 81.59 %\n",
      "Epoch:40/100 AVG Training Loss:0.472 AVG Validation Loss:0.490 AVG Training Acc 80.05 % AVG Validation Acc 81.45 %\n",
      "Epoch:50/100 AVG Training Loss:0.467 AVG Validation Loss:0.492 AVG Training Acc 80.30 % AVG Validation Acc 81.05 %\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/100 AVG Training Loss:0.463 AVG Validation Loss:0.493 AVG Training Acc 80.39 % AVG Validation Acc 81.18 %\n",
      "Epoch:70/100 AVG Training Loss:0.461 AVG Validation Loss:0.494 AVG Training Acc 80.45 % AVG Validation Acc 81.05 %\n",
      "Epoch:80/100 AVG Training Loss:0.459 AVG Validation Loss:0.497 AVG Training Acc 80.62 % AVG Validation Acc 80.78 %\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/100 AVG Training Loss:0.459 AVG Validation Loss:0.497 AVG Training Acc 80.56 % AVG Validation Acc 80.91 %\n",
      "Epoch:100/100 AVG Training Loss:0.458 AVG Validation Loss:0.496 AVG Training Acc 80.50 % AVG Validation Acc 80.91 %\n",
      "Split 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed073dce363a41b39a966368f01831a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.496 AVG Validation Loss:0.487 AVG Training Acc 79.78 % AVG Validation Acc 80.65 %\n",
      "Epoch:20/100 AVG Training Loss:0.490 AVG Validation Loss:0.485 AVG Training Acc 79.78 % AVG Validation Acc 80.65 %\n",
      "Epoch:30/100 AVG Training Loss:0.487 AVG Validation Loss:0.495 AVG Training Acc 79.90 % AVG Validation Acc 80.38 %\n",
      "Epoch:40/100 AVG Training Loss:0.485 AVG Validation Loss:0.485 AVG Training Acc 79.89 % AVG Validation Acc 80.78 %\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:50/100 AVG Training Loss:0.472 AVG Validation Loss:0.500 AVG Training Acc 80.02 % AVG Validation Acc 80.11 %\n",
      "Epoch:60/100 AVG Training Loss:0.462 AVG Validation Loss:0.512 AVG Training Acc 80.30 % AVG Validation Acc 79.97 %\n",
      "Epoch:70/100 AVG Training Loss:0.458 AVG Validation Loss:0.523 AVG Training Acc 80.19 % AVG Validation Acc 79.84 %\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/100 AVG Training Loss:0.448 AVG Validation Loss:0.534 AVG Training Acc 80.59 % AVG Validation Acc 79.70 %\n",
      "Epoch:90/100 AVG Training Loss:0.447 AVG Validation Loss:0.530 AVG Training Acc 80.53 % AVG Validation Acc 79.57 %\n",
      "Epoch:100/100 AVG Training Loss:0.447 AVG Validation Loss:0.532 AVG Training Acc 80.68 % AVG Validation Acc 79.84 %\n",
      "Split 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557034078c7a456890db3e20b1168a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.491 AVG Validation Loss:0.512 AVG Training Acc 79.93 % AVG Validation Acc 79.03 %\n",
      "Epoch:20/100 AVG Training Loss:0.487 AVG Validation Loss:0.508 AVG Training Acc 80.04 % AVG Validation Acc 79.03 %\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/100 AVG Training Loss:0.471 AVG Validation Loss:0.518 AVG Training Acc 80.32 % AVG Validation Acc 78.63 %\n",
      "Epoch:40/100 AVG Training Loss:0.466 AVG Validation Loss:0.521 AVG Training Acc 80.56 % AVG Validation Acc 78.36 %\n",
      "Epoch:50/100 AVG Training Loss:0.457 AVG Validation Loss:0.528 AVG Training Acc 80.84 % AVG Validation Acc 78.36 %\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/100 AVG Training Loss:0.453 AVG Validation Loss:0.529 AVG Training Acc 80.90 % AVG Validation Acc 78.23 %\n",
      "Epoch:70/100 AVG Training Loss:0.451 AVG Validation Loss:0.533 AVG Training Acc 81.19 % AVG Validation Acc 78.09 %\n",
      "Epoch:80/100 AVG Training Loss:0.450 AVG Validation Loss:0.537 AVG Training Acc 80.95 % AVG Validation Acc 78.23 %\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/100 AVG Training Loss:0.452 AVG Validation Loss:0.530 AVG Training Acc 81.10 % AVG Validation Acc 78.23 %\n",
      "Epoch:100/100 AVG Training Loss:0.450 AVG Validation Loss:0.537 AVG Training Acc 81.20 % AVG Validation Acc 78.09 %\n",
      "Split 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1d0776512c495991f11efb700bafaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.494 AVG Validation Loss:0.539 AVG Training Acc 80.17 % AVG Validation Acc 77.02 %\n",
      "Epoch:20/100 AVG Training Loss:0.488 AVG Validation Loss:0.522 AVG Training Acc 80.25 % AVG Validation Acc 77.15 %\n",
      "Epoch:30/100 AVG Training Loss:0.483 AVG Validation Loss:0.525 AVG Training Acc 80.17 % AVG Validation Acc 77.02 %\n",
      "Epoch:40/100 AVG Training Loss:0.476 AVG Validation Loss:0.532 AVG Training Acc 80.26 % AVG Validation Acc 76.88 %\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:50/100 AVG Training Loss:0.466 AVG Validation Loss:0.529 AVG Training Acc 80.45 % AVG Validation Acc 76.61 %\n",
      "Epoch:60/100 AVG Training Loss:0.455 AVG Validation Loss:0.541 AVG Training Acc 80.78 % AVG Validation Acc 76.21 %\n",
      "Epoch:70/100 AVG Training Loss:0.450 AVG Validation Loss:0.542 AVG Training Acc 80.90 % AVG Validation Acc 76.34 %\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/100 AVG Training Loss:0.444 AVG Validation Loss:0.546 AVG Training Acc 81.04 % AVG Validation Acc 76.08 %\n",
      "Epoch:90/100 AVG Training Loss:0.441 AVG Validation Loss:0.549 AVG Training Acc 81.43 % AVG Validation Acc 76.21 %\n",
      "Epoch:100/100 AVG Training Loss:0.442 AVG Validation Loss:0.552 AVG Training Acc 80.92 % AVG Validation Acc 76.21 %\n",
      "Split 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c477d8faf994e6cbdaacf666baeb79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.492 AVG Validation Loss:0.516 AVG Training Acc 80.10 % AVG Validation Acc 77.66 %\n",
      "Epoch:20/100 AVG Training Loss:0.488 AVG Validation Loss:0.514 AVG Training Acc 80.13 % AVG Validation Acc 77.79 %\n",
      "Epoch:30/100 AVG Training Loss:0.482 AVG Validation Loss:0.518 AVG Training Acc 80.35 % AVG Validation Acc 77.39 %\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/100 AVG Training Loss:0.470 AVG Validation Loss:0.522 AVG Training Acc 80.59 % AVG Validation Acc 77.39 %\n",
      "Epoch:50/100 AVG Training Loss:0.464 AVG Validation Loss:0.532 AVG Training Acc 80.92 % AVG Validation Acc 77.25 %\n",
      "Epoch:60/100 AVG Training Loss:0.456 AVG Validation Loss:0.539 AVG Training Acc 81.05 % AVG Validation Acc 77.12 %\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/100 AVG Training Loss:0.449 AVG Validation Loss:0.544 AVG Training Acc 81.26 % AVG Validation Acc 77.25 %\n",
      "Epoch:80/100 AVG Training Loss:0.450 AVG Validation Loss:0.540 AVG Training Acc 81.22 % AVG Validation Acc 77.25 %\n",
      "Epoch:90/100 AVG Training Loss:0.450 AVG Validation Loss:0.541 AVG Training Acc 81.31 % AVG Validation Acc 77.12 %\n",
      "Epoch    95: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/100 AVG Training Loss:0.450 AVG Validation Loss:0.547 AVG Training Acc 81.11 % AVG Validation Acc 77.12 %\n",
      "Split 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1770c0eba64f45b3b147af796e2524d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.499 AVG Validation Loss:0.487 AVG Training Acc 79.75 % AVG Validation Acc 80.75 %\n",
      "Epoch:20/100 AVG Training Loss:0.490 AVG Validation Loss:0.484 AVG Training Acc 79.81 % AVG Validation Acc 80.48 %\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/100 AVG Training Loss:0.482 AVG Validation Loss:0.487 AVG Training Acc 79.84 % AVG Validation Acc 80.48 %\n",
      "Epoch:40/100 AVG Training Loss:0.475 AVG Validation Loss:0.493 AVG Training Acc 80.08 % AVG Validation Acc 80.08 %\n",
      "Epoch:50/100 AVG Training Loss:0.469 AVG Validation Loss:0.496 AVG Training Acc 80.32 % AVG Validation Acc 80.48 %\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/100 AVG Training Loss:0.464 AVG Validation Loss:0.496 AVG Training Acc 80.41 % AVG Validation Acc 80.22 %\n",
      "Epoch:70/100 AVG Training Loss:0.465 AVG Validation Loss:0.497 AVG Training Acc 80.25 % AVG Validation Acc 80.35 %\n",
      "Epoch:80/100 AVG Training Loss:0.464 AVG Validation Loss:0.499 AVG Training Acc 80.40 % AVG Validation Acc 80.35 %\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/100 AVG Training Loss:0.462 AVG Validation Loss:0.501 AVG Training Acc 80.47 % AVG Validation Acc 80.35 %\n",
      "Epoch:100/100 AVG Training Loss:0.464 AVG Validation Loss:0.502 AVG Training Acc 80.43 % AVG Validation Acc 80.35 %\n",
      "Split 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1c91eb430c4f8893af31b934ade0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.503 AVG Validation Loss:0.486 AVG Training Acc 79.78 % AVG Validation Acc 80.62 %\n",
      "Epoch:20/100 AVG Training Loss:0.496 AVG Validation Loss:0.482 AVG Training Acc 79.78 % AVG Validation Acc 80.62 %\n",
      "Epoch:30/100 AVG Training Loss:0.489 AVG Validation Loss:0.481 AVG Training Acc 79.70 % AVG Validation Acc 80.62 %\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/100 AVG Training Loss:0.483 AVG Validation Loss:0.471 AVG Training Acc 79.90 % AVG Validation Acc 80.62 %\n",
      "Epoch:50/100 AVG Training Loss:0.475 AVG Validation Loss:0.471 AVG Training Acc 80.02 % AVG Validation Acc 80.75 %\n",
      "Epoch:60/100 AVG Training Loss:0.467 AVG Validation Loss:0.474 AVG Training Acc 80.11 % AVG Validation Acc 80.35 %\n",
      "Epoch:70/100 AVG Training Loss:0.465 AVG Validation Loss:0.483 AVG Training Acc 80.35 % AVG Validation Acc 79.81 %\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/100 AVG Training Loss:0.459 AVG Validation Loss:0.486 AVG Training Acc 80.62 % AVG Validation Acc 79.81 %\n",
      "Epoch:90/100 AVG Training Loss:0.459 AVG Validation Loss:0.489 AVG Training Acc 80.61 % AVG Validation Acc 79.41 %\n",
      "Epoch:100/100 AVG Training Loss:0.456 AVG Validation Loss:0.491 AVG Training Acc 80.86 % AVG Validation Acc 79.27 %\n",
      "Split 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fee25b64774efe84f3d4c59a4bff12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.492 AVG Validation Loss:0.508 AVG Training Acc 79.99 % AVG Validation Acc 78.87 %\n",
      "Epoch:20/100 AVG Training Loss:0.488 AVG Validation Loss:0.509 AVG Training Acc 79.89 % AVG Validation Acc 79.00 %\n",
      "Epoch:30/100 AVG Training Loss:0.489 AVG Validation Loss:0.506 AVG Training Acc 79.93 % AVG Validation Acc 78.87 %\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/100 AVG Training Loss:0.471 AVG Validation Loss:0.516 AVG Training Acc 80.25 % AVG Validation Acc 78.87 %\n",
      "Epoch:50/100 AVG Training Loss:0.464 AVG Validation Loss:0.529 AVG Training Acc 80.22 % AVG Validation Acc 78.73 %\n",
      "Epoch:60/100 AVG Training Loss:0.454 AVG Validation Loss:0.538 AVG Training Acc 80.89 % AVG Validation Acc 78.06 %\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/100 AVG Training Loss:0.448 AVG Validation Loss:0.545 AVG Training Acc 80.83 % AVG Validation Acc 78.47 %\n",
      "Epoch:80/100 AVG Training Loss:0.448 AVG Validation Loss:0.546 AVG Training Acc 80.88 % AVG Validation Acc 78.33 %\n",
      "Epoch:90/100 AVG Training Loss:0.446 AVG Validation Loss:0.552 AVG Training Acc 81.07 % AVG Validation Acc 77.93 %\n",
      "Epoch    94: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/100 AVG Training Loss:0.446 AVG Validation Loss:0.551 AVG Training Acc 81.07 % AVG Validation Acc 78.06 %\n",
      "Split 21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c47df4233a24196b43e50bbc792c33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.495 AVG Validation Loss:0.492 AVG Training Acc 79.80 % AVG Validation Acc 80.11 %\n",
      "Epoch:20/100 AVG Training Loss:0.491 AVG Validation Loss:0.486 AVG Training Acc 79.84 % AVG Validation Acc 80.11 %\n",
      "Epoch:30/100 AVG Training Loss:0.488 AVG Validation Loss:0.496 AVG Training Acc 79.87 % AVG Validation Acc 79.57 %\n",
      "Epoch:40/100 AVG Training Loss:0.484 AVG Validation Loss:0.486 AVG Training Acc 79.96 % AVG Validation Acc 79.70 %\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:50/100 AVG Training Loss:0.463 AVG Validation Loss:0.496 AVG Training Acc 80.36 % AVG Validation Acc 79.17 %\n",
      "Epoch:60/100 AVG Training Loss:0.459 AVG Validation Loss:0.502 AVG Training Acc 80.56 % AVG Validation Acc 79.17 %\n",
      "Epoch:70/100 AVG Training Loss:0.456 AVG Validation Loss:0.515 AVG Training Acc 80.60 % AVG Validation Acc 78.76 %\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/100 AVG Training Loss:0.450 AVG Validation Loss:0.520 AVG Training Acc 80.93 % AVG Validation Acc 78.90 %\n",
      "Epoch:90/100 AVG Training Loss:0.445 AVG Validation Loss:0.517 AVG Training Acc 81.05 % AVG Validation Acc 79.17 %\n",
      "Epoch:100/100 AVG Training Loss:0.448 AVG Validation Loss:0.521 AVG Training Acc 80.96 % AVG Validation Acc 79.17 %\n",
      "Split 22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95731a9e8d6d41779182a44abb57474b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.495 AVG Validation Loss:0.499 AVG Training Acc 79.89 % AVG Validation Acc 79.70 %\n",
      "Epoch:20/100 AVG Training Loss:0.488 AVG Validation Loss:0.503 AVG Training Acc 79.89 % AVG Validation Acc 79.70 %\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/100 AVG Training Loss:0.476 AVG Validation Loss:0.507 AVG Training Acc 80.17 % AVG Validation Acc 79.57 %\n",
      "Epoch:40/100 AVG Training Loss:0.468 AVG Validation Loss:0.511 AVG Training Acc 80.22 % AVG Validation Acc 79.44 %\n",
      "Epoch:50/100 AVG Training Loss:0.462 AVG Validation Loss:0.520 AVG Training Acc 80.50 % AVG Validation Acc 79.30 %\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/100 AVG Training Loss:0.457 AVG Validation Loss:0.528 AVG Training Acc 80.53 % AVG Validation Acc 79.30 %\n",
      "Epoch:70/100 AVG Training Loss:0.456 AVG Validation Loss:0.528 AVG Training Acc 80.71 % AVG Validation Acc 79.30 %\n",
      "Epoch:80/100 AVG Training Loss:0.453 AVG Validation Loss:0.530 AVG Training Acc 80.74 % AVG Validation Acc 78.76 %\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/100 AVG Training Loss:0.451 AVG Validation Loss:0.532 AVG Training Acc 80.68 % AVG Validation Acc 79.17 %\n",
      "Epoch:100/100 AVG Training Loss:0.453 AVG Validation Loss:0.529 AVG Training Acc 80.72 % AVG Validation Acc 79.03 %\n",
      "Split 23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bae0142e60642b39aee337df23049ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.493 AVG Validation Loss:0.496 AVG Training Acc 79.80 % AVG Validation Acc 80.11 %\n",
      "Epoch:20/100 AVG Training Loss:0.491 AVG Validation Loss:0.488 AVG Training Acc 79.68 % AVG Validation Acc 80.11 %\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/100 AVG Training Loss:0.481 AVG Validation Loss:0.488 AVG Training Acc 80.05 % AVG Validation Acc 79.84 %\n",
      "Epoch:40/100 AVG Training Loss:0.476 AVG Validation Loss:0.494 AVG Training Acc 80.11 % AVG Validation Acc 79.30 %\n",
      "Epoch:50/100 AVG Training Loss:0.470 AVG Validation Loss:0.499 AVG Training Acc 80.26 % AVG Validation Acc 79.44 %\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/100 AVG Training Loss:0.467 AVG Validation Loss:0.503 AVG Training Acc 80.27 % AVG Validation Acc 79.17 %\n",
      "Epoch:70/100 AVG Training Loss:0.464 AVG Validation Loss:0.505 AVG Training Acc 80.47 % AVG Validation Acc 79.17 %\n",
      "Epoch:80/100 AVG Training Loss:0.463 AVG Validation Loss:0.504 AVG Training Acc 80.54 % AVG Validation Acc 79.30 %\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/100 AVG Training Loss:0.462 AVG Validation Loss:0.505 AVG Training Acc 80.42 % AVG Validation Acc 79.17 %\n",
      "Epoch:100/100 AVG Training Loss:0.462 AVG Validation Loss:0.506 AVG Training Acc 80.41 % AVG Validation Acc 79.30 %\n",
      "Split 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b7bda0dbe4413aab63eec2feb0daba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.494 AVG Validation Loss:0.499 AVG Training Acc 79.93 % AVG Validation Acc 79.30 %\n",
      "Epoch:20/100 AVG Training Loss:0.488 AVG Validation Loss:0.495 AVG Training Acc 79.95 % AVG Validation Acc 79.17 %\n",
      "Epoch:30/100 AVG Training Loss:0.486 AVG Validation Loss:0.497 AVG Training Acc 80.04 % AVG Validation Acc 79.57 %\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/100 AVG Training Loss:0.475 AVG Validation Loss:0.505 AVG Training Acc 80.30 % AVG Validation Acc 79.17 %\n",
      "Epoch:50/100 AVG Training Loss:0.468 AVG Validation Loss:0.505 AVG Training Acc 80.26 % AVG Validation Acc 78.63 %\n",
      "Epoch:60/100 AVG Training Loss:0.464 AVG Validation Loss:0.509 AVG Training Acc 80.60 % AVG Validation Acc 78.49 %\n",
      "Epoch:70/100 AVG Training Loss:0.460 AVG Validation Loss:0.508 AVG Training Acc 80.57 % AVG Validation Acc 78.76 %\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/100 AVG Training Loss:0.456 AVG Validation Loss:0.513 AVG Training Acc 80.92 % AVG Validation Acc 78.09 %\n",
      "Epoch:90/100 AVG Training Loss:0.456 AVG Validation Loss:0.511 AVG Training Acc 80.93 % AVG Validation Acc 78.36 %\n",
      "Epoch:100/100 AVG Training Loss:0.454 AVG Validation Loss:0.512 AVG Training Acc 80.86 % AVG Validation Acc 78.36 %\n",
      "Split 25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5768e2d5a2754554af76bf5e53ea401f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.491 AVG Validation Loss:0.508 AVG Training Acc 79.89 % AVG Validation Acc 78.90 %\n",
      "Epoch:20/100 AVG Training Loss:0.487 AVG Validation Loss:0.506 AVG Training Acc 79.95 % AVG Validation Acc 78.76 %\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/100 AVG Training Loss:0.478 AVG Validation Loss:0.506 AVG Training Acc 80.29 % AVG Validation Acc 78.76 %\n",
      "Epoch:40/100 AVG Training Loss:0.474 AVG Validation Loss:0.510 AVG Training Acc 80.51 % AVG Validation Acc 78.76 %\n",
      "Epoch:50/100 AVG Training Loss:0.472 AVG Validation Loss:0.513 AVG Training Acc 80.60 % AVG Validation Acc 78.90 %\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/100 AVG Training Loss:0.467 AVG Validation Loss:0.518 AVG Training Acc 80.69 % AVG Validation Acc 78.49 %\n",
      "Epoch:70/100 AVG Training Loss:0.466 AVG Validation Loss:0.519 AVG Training Acc 80.63 % AVG Validation Acc 78.49 %\n",
      "Epoch:80/100 AVG Training Loss:0.466 AVG Validation Loss:0.521 AVG Training Acc 80.57 % AVG Validation Acc 78.49 %\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/100 AVG Training Loss:0.465 AVG Validation Loss:0.522 AVG Training Acc 80.69 % AVG Validation Acc 78.49 %\n",
      "Epoch:100/100 AVG Training Loss:0.465 AVG Validation Loss:0.523 AVG Training Acc 80.60 % AVG Validation Acc 78.49 %\n",
      "Split 26\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3456bf440f4d46779101860b4f1deff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.497 AVG Validation Loss:0.473 AVG Training Acc 79.68 % AVG Validation Acc 81.05 %\n",
      "Epoch:20/100 AVG Training Loss:0.494 AVG Validation Loss:0.472 AVG Training Acc 79.74 % AVG Validation Acc 81.18 %\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/100 AVG Training Loss:0.486 AVG Validation Loss:0.472 AVG Training Acc 79.80 % AVG Validation Acc 81.05 %\n",
      "Epoch:40/100 AVG Training Loss:0.477 AVG Validation Loss:0.479 AVG Training Acc 80.05 % AVG Validation Acc 80.91 %\n",
      "Epoch:50/100 AVG Training Loss:0.472 AVG Validation Loss:0.481 AVG Training Acc 80.27 % AVG Validation Acc 80.51 %\n",
      "Epoch:60/100 AVG Training Loss:0.469 AVG Validation Loss:0.482 AVG Training Acc 80.35 % AVG Validation Acc 80.51 %\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/100 AVG Training Loss:0.465 AVG Validation Loss:0.481 AVG Training Acc 80.51 % AVG Validation Acc 80.91 %\n",
      "Epoch:80/100 AVG Training Loss:0.464 AVG Validation Loss:0.482 AVG Training Acc 80.47 % AVG Validation Acc 80.51 %\n",
      "Epoch:90/100 AVG Training Loss:0.462 AVG Validation Loss:0.485 AVG Training Acc 80.50 % AVG Validation Acc 80.51 %\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/100 AVG Training Loss:0.463 AVG Validation Loss:0.484 AVG Training Acc 80.53 % AVG Validation Acc 80.38 %\n",
      "Split 27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515ffd487c154db19adc2b3633fcfb82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.492 AVG Validation Loss:0.519 AVG Training Acc 80.05 % AVG Validation Acc 78.06 %\n",
      "Epoch:20/100 AVG Training Loss:0.492 AVG Validation Loss:0.514 AVG Training Acc 80.02 % AVG Validation Acc 78.20 %\n",
      "Epoch:30/100 AVG Training Loss:0.484 AVG Validation Loss:0.517 AVG Training Acc 80.04 % AVG Validation Acc 78.20 %\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/100 AVG Training Loss:0.477 AVG Validation Loss:0.520 AVG Training Acc 80.37 % AVG Validation Acc 78.06 %\n",
      "Epoch:50/100 AVG Training Loss:0.473 AVG Validation Loss:0.523 AVG Training Acc 80.46 % AVG Validation Acc 78.06 %\n",
      "Epoch:60/100 AVG Training Loss:0.467 AVG Validation Loss:0.524 AVG Training Acc 80.68 % AVG Validation Acc 78.33 %\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/100 AVG Training Loss:0.464 AVG Validation Loss:0.524 AVG Training Acc 80.59 % AVG Validation Acc 78.33 %\n",
      "Epoch:80/100 AVG Training Loss:0.464 AVG Validation Loss:0.527 AVG Training Acc 80.70 % AVG Validation Acc 78.06 %\n",
      "Epoch:90/100 AVG Training Loss:0.463 AVG Validation Loss:0.528 AVG Training Acc 80.79 % AVG Validation Acc 78.06 %\n",
      "Epoch    96: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/100 AVG Training Loss:0.463 AVG Validation Loss:0.532 AVG Training Acc 80.77 % AVG Validation Acc 78.06 %\n",
      "Split 28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f6ee3701e74bf39965e7c8b424420d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.494 AVG Validation Loss:0.499 AVG Training Acc 79.96 % AVG Validation Acc 79.27 %\n",
      "Epoch:20/100 AVG Training Loss:0.490 AVG Validation Loss:0.493 AVG Training Acc 79.84 % AVG Validation Acc 79.14 %\n",
      "Epoch:30/100 AVG Training Loss:0.485 AVG Validation Loss:0.499 AVG Training Acc 79.92 % AVG Validation Acc 79.14 %\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/100 AVG Training Loss:0.478 AVG Validation Loss:0.496 AVG Training Acc 79.98 % AVG Validation Acc 79.41 %\n",
      "Epoch:50/100 AVG Training Loss:0.472 AVG Validation Loss:0.505 AVG Training Acc 80.25 % AVG Validation Acc 79.27 %\n",
      "Epoch:60/100 AVG Training Loss:0.467 AVG Validation Loss:0.511 AVG Training Acc 80.22 % AVG Validation Acc 79.00 %\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/100 AVG Training Loss:0.465 AVG Validation Loss:0.503 AVG Training Acc 80.40 % AVG Validation Acc 79.00 %\n",
      "Epoch:80/100 AVG Training Loss:0.464 AVG Validation Loss:0.508 AVG Training Acc 80.44 % AVG Validation Acc 78.87 %\n",
      "Epoch:90/100 AVG Training Loss:0.463 AVG Validation Loss:0.508 AVG Training Acc 80.50 % AVG Validation Acc 78.87 %\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/100 AVG Training Loss:0.461 AVG Validation Loss:0.510 AVG Training Acc 80.46 % AVG Validation Acc 79.14 %\n",
      "Split 29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577b9c303bc24c71920212398e3dd582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.495 AVG Validation Loss:0.477 AVG Training Acc 79.65 % AVG Validation Acc 81.43 %\n",
      "Epoch:20/100 AVG Training Loss:0.487 AVG Validation Loss:0.482 AVG Training Acc 79.73 % AVG Validation Acc 81.70 %\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/100 AVG Training Loss:0.475 AVG Validation Loss:0.490 AVG Training Acc 80.26 % AVG Validation Acc 81.16 %\n",
      "Epoch:40/100 AVG Training Loss:0.464 AVG Validation Loss:0.497 AVG Training Acc 80.41 % AVG Validation Acc 81.43 %\n",
      "Epoch:50/100 AVG Training Loss:0.459 AVG Validation Loss:0.501 AVG Training Acc 80.52 % AVG Validation Acc 81.16 %\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/100 AVG Training Loss:0.450 AVG Validation Loss:0.510 AVG Training Acc 81.02 % AVG Validation Acc 81.16 %\n",
      "Epoch:70/100 AVG Training Loss:0.445 AVG Validation Loss:0.511 AVG Training Acc 80.89 % AVG Validation Acc 80.89 %\n",
      "Epoch:80/100 AVG Training Loss:0.445 AVG Validation Loss:0.510 AVG Training Acc 81.05 % AVG Validation Acc 80.89 %\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/100 AVG Training Loss:0.446 AVG Validation Loss:0.511 AVG Training Acc 81.10 % AVG Validation Acc 80.89 %\n",
      "Epoch:100/100 AVG Training Loss:0.445 AVG Validation Loss:0.513 AVG Training Acc 80.97 % AVG Validation Acc 80.75 %\n",
      "Split 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff57f62f86e4ae48693b7c9689c09ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.498 AVG Validation Loss:0.486 AVG Training Acc 79.77 % AVG Validation Acc 80.48 %\n",
      "Epoch:20/100 AVG Training Loss:0.491 AVG Validation Loss:0.480 AVG Training Acc 79.80 % AVG Validation Acc 80.62 %\n",
      "Epoch:30/100 AVG Training Loss:0.484 AVG Validation Loss:0.482 AVG Training Acc 79.75 % AVG Validation Acc 80.75 %\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/100 AVG Training Loss:0.468 AVG Validation Loss:0.481 AVG Training Acc 79.90 % AVG Validation Acc 81.02 %\n",
      "Epoch:50/100 AVG Training Loss:0.458 AVG Validation Loss:0.490 AVG Training Acc 80.14 % AVG Validation Acc 80.75 %\n",
      "Epoch:60/100 AVG Training Loss:0.450 AVG Validation Loss:0.497 AVG Training Acc 80.49 % AVG Validation Acc 80.89 %\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/100 AVG Training Loss:0.446 AVG Validation Loss:0.496 AVG Training Acc 80.71 % AVG Validation Acc 80.75 %\n",
      "Epoch:80/100 AVG Training Loss:0.444 AVG Validation Loss:0.497 AVG Training Acc 80.55 % AVG Validation Acc 80.89 %\n",
      "Epoch:90/100 AVG Training Loss:0.443 AVG Validation Loss:0.499 AVG Training Acc 80.71 % AVG Validation Acc 80.89 %\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/100 AVG Training Loss:0.441 AVG Validation Loss:0.501 AVG Training Acc 80.85 % AVG Validation Acc 80.75 %\n",
      "Split 31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cf5e70557c449cbb2ceaea459c8213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.498 AVG Validation Loss:0.491 AVG Training Acc 79.80 % AVG Validation Acc 79.97 %\n",
      "Epoch:20/100 AVG Training Loss:0.490 AVG Validation Loss:0.486 AVG Training Acc 79.86 % AVG Validation Acc 80.11 %\n",
      "Epoch:30/100 AVG Training Loss:0.489 AVG Validation Loss:0.486 AVG Training Acc 79.83 % AVG Validation Acc 79.97 %\n",
      "Epoch:40/100 AVG Training Loss:0.481 AVG Validation Loss:0.491 AVG Training Acc 79.93 % AVG Validation Acc 80.11 %\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:50/100 AVG Training Loss:0.469 AVG Validation Loss:0.495 AVG Training Acc 80.57 % AVG Validation Acc 79.97 %\n",
      "Epoch:60/100 AVG Training Loss:0.464 AVG Validation Loss:0.502 AVG Training Acc 80.60 % AVG Validation Acc 79.17 %\n",
      "Epoch:70/100 AVG Training Loss:0.462 AVG Validation Loss:0.508 AVG Training Acc 80.66 % AVG Validation Acc 79.70 %\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/100 AVG Training Loss:0.456 AVG Validation Loss:0.514 AVG Training Acc 80.98 % AVG Validation Acc 79.70 %\n",
      "Epoch:90/100 AVG Training Loss:0.454 AVG Validation Loss:0.514 AVG Training Acc 80.90 % AVG Validation Acc 79.84 %\n",
      "Epoch:100/100 AVG Training Loss:0.455 AVG Validation Loss:0.513 AVG Training Acc 80.87 % AVG Validation Acc 79.97 %\n",
      "Split 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0ec82ff37342378818bcc0b6a1f9ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.493 AVG Validation Loss:0.499 AVG Training Acc 79.95 % AVG Validation Acc 79.17 %\n",
      "Epoch:20/100 AVG Training Loss:0.487 AVG Validation Loss:0.498 AVG Training Acc 79.96 % AVG Validation Acc 79.03 %\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/100 AVG Training Loss:0.471 AVG Validation Loss:0.498 AVG Training Acc 80.22 % AVG Validation Acc 79.17 %\n",
      "Epoch:40/100 AVG Training Loss:0.466 AVG Validation Loss:0.502 AVG Training Acc 80.45 % AVG Validation Acc 78.76 %\n",
      "Epoch:50/100 AVG Training Loss:0.458 AVG Validation Loss:0.502 AVG Training Acc 80.77 % AVG Validation Acc 78.63 %\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/100 AVG Training Loss:0.451 AVG Validation Loss:0.511 AVG Training Acc 80.95 % AVG Validation Acc 78.63 %\n",
      "Epoch:70/100 AVG Training Loss:0.450 AVG Validation Loss:0.511 AVG Training Acc 81.20 % AVG Validation Acc 78.49 %\n",
      "Epoch:80/100 AVG Training Loss:0.447 AVG Validation Loss:0.514 AVG Training Acc 81.22 % AVG Validation Acc 78.63 %\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/100 AVG Training Loss:0.449 AVG Validation Loss:0.515 AVG Training Acc 81.20 % AVG Validation Acc 78.36 %\n",
      "Epoch:100/100 AVG Training Loss:0.448 AVG Validation Loss:0.515 AVG Training Acc 81.25 % AVG Validation Acc 78.63 %\n",
      "Split 33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40cc8a12b8c4dc3b88ea3daf3f9ab12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.496 AVG Validation Loss:0.473 AVG Training Acc 79.68 % AVG Validation Acc 81.32 %\n",
      "Epoch:20/100 AVG Training Loss:0.493 AVG Validation Loss:0.472 AVG Training Acc 79.77 % AVG Validation Acc 81.32 %\n",
      "Epoch:30/100 AVG Training Loss:0.487 AVG Validation Loss:0.479 AVG Training Acc 79.69 % AVG Validation Acc 81.32 %\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/100 AVG Training Loss:0.473 AVG Validation Loss:0.481 AVG Training Acc 80.19 % AVG Validation Acc 80.78 %\n",
      "Epoch:50/100 AVG Training Loss:0.468 AVG Validation Loss:0.486 AVG Training Acc 80.33 % AVG Validation Acc 80.51 %\n",
      "Epoch:60/100 AVG Training Loss:0.461 AVG Validation Loss:0.492 AVG Training Acc 80.56 % AVG Validation Acc 80.24 %\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/100 AVG Training Loss:0.459 AVG Validation Loss:0.496 AVG Training Acc 80.51 % AVG Validation Acc 80.38 %\n",
      "Epoch:80/100 AVG Training Loss:0.457 AVG Validation Loss:0.497 AVG Training Acc 80.89 % AVG Validation Acc 80.24 %\n",
      "Epoch:90/100 AVG Training Loss:0.455 AVG Validation Loss:0.497 AVG Training Acc 80.69 % AVG Validation Acc 80.51 %\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/100 AVG Training Loss:0.454 AVG Validation Loss:0.497 AVG Training Acc 80.95 % AVG Validation Acc 80.51 %\n",
      "Split 34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a57f6c369e4524b59a826487fb768f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.494 AVG Validation Loss:0.509 AVG Training Acc 79.95 % AVG Validation Acc 79.17 %\n",
      "Epoch:20/100 AVG Training Loss:0.488 AVG Validation Loss:0.493 AVG Training Acc 79.92 % AVG Validation Acc 79.17 %\n",
      "Epoch:30/100 AVG Training Loss:0.482 AVG Validation Loss:0.495 AVG Training Acc 80.11 % AVG Validation Acc 79.17 %\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/100 AVG Training Loss:0.474 AVG Validation Loss:0.495 AVG Training Acc 80.26 % AVG Validation Acc 79.30 %\n",
      "Epoch:50/100 AVG Training Loss:0.463 AVG Validation Loss:0.502 AVG Training Acc 80.63 % AVG Validation Acc 78.90 %\n",
      "Epoch:60/100 AVG Training Loss:0.458 AVG Validation Loss:0.504 AVG Training Acc 80.87 % AVG Validation Acc 78.49 %\n",
      "Epoch:70/100 AVG Training Loss:0.454 AVG Validation Loss:0.507 AVG Training Acc 80.98 % AVG Validation Acc 78.63 %\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/100 AVG Training Loss:0.451 AVG Validation Loss:0.509 AVG Training Acc 81.14 % AVG Validation Acc 78.76 %\n",
      "Epoch:90/100 AVG Training Loss:0.447 AVG Validation Loss:0.514 AVG Training Acc 81.10 % AVG Validation Acc 78.36 %\n",
      "Epoch:100/100 AVG Training Loss:0.447 AVG Validation Loss:0.511 AVG Training Acc 81.13 % AVG Validation Acc 78.63 %\n",
      "Split 35\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7396464a09114e798fcb59d06997f3d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.495 AVG Validation Loss:0.492 AVG Training Acc 79.83 % AVG Validation Acc 80.24 %\n",
      "Epoch:20/100 AVG Training Loss:0.488 AVG Validation Loss:0.485 AVG Training Acc 79.93 % AVG Validation Acc 80.24 %\n",
      "Epoch:30/100 AVG Training Loss:0.484 AVG Validation Loss:0.495 AVG Training Acc 79.83 % AVG Validation Acc 79.70 %\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/100 AVG Training Loss:0.468 AVG Validation Loss:0.503 AVG Training Acc 80.35 % AVG Validation Acc 79.57 %\n",
      "Epoch:50/100 AVG Training Loss:0.461 AVG Validation Loss:0.505 AVG Training Acc 80.53 % AVG Validation Acc 79.70 %\n",
      "Epoch:60/100 AVG Training Loss:0.453 AVG Validation Loss:0.507 AVG Training Acc 80.77 % AVG Validation Acc 79.84 %\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/100 AVG Training Loss:0.449 AVG Validation Loss:0.509 AVG Training Acc 80.95 % AVG Validation Acc 79.84 %\n",
      "Epoch:80/100 AVG Training Loss:0.448 AVG Validation Loss:0.513 AVG Training Acc 80.86 % AVG Validation Acc 79.70 %\n",
      "Epoch:90/100 AVG Training Loss:0.448 AVG Validation Loss:0.512 AVG Training Acc 80.86 % AVG Validation Acc 79.70 %\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/100 AVG Training Loss:0.445 AVG Validation Loss:0.513 AVG Training Acc 81.02 % AVG Validation Acc 79.84 %\n",
      "Split 36\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82db8b518f1344c7825eda4b8796872f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.495 AVG Validation Loss:0.483 AVG Training Acc 79.80 % AVG Validation Acc 80.38 %\n",
      "Epoch:20/100 AVG Training Loss:0.490 AVG Validation Loss:0.483 AVG Training Acc 79.69 % AVG Validation Acc 80.65 %\n",
      "Epoch:30/100 AVG Training Loss:0.485 AVG Validation Loss:0.486 AVG Training Acc 79.83 % AVG Validation Acc 80.65 %\n",
      "Epoch:40/100 AVG Training Loss:0.480 AVG Validation Loss:0.486 AVG Training Acc 79.92 % AVG Validation Acc 80.65 %\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:50/100 AVG Training Loss:0.457 AVG Validation Loss:0.515 AVG Training Acc 80.60 % AVG Validation Acc 80.11 %\n",
      "Epoch:60/100 AVG Training Loss:0.446 AVG Validation Loss:0.553 AVG Training Acc 80.93 % AVG Validation Acc 79.44 %\n",
      "Epoch:70/100 AVG Training Loss:0.439 AVG Validation Loss:0.560 AVG Training Acc 81.29 % AVG Validation Acc 80.11 %\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/100 AVG Training Loss:0.431 AVG Validation Loss:0.571 AVG Training Acc 81.81 % AVG Validation Acc 79.97 %\n",
      "Epoch:90/100 AVG Training Loss:0.430 AVG Validation Loss:0.575 AVG Training Acc 81.60 % AVG Validation Acc 79.30 %\n",
      "Epoch:100/100 AVG Training Loss:0.428 AVG Validation Loss:0.572 AVG Training Acc 81.83 % AVG Validation Acc 79.57 %\n",
      "Split 37\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569e4126e76b477aae945027171e973d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.501 AVG Validation Loss:0.512 AVG Training Acc 79.98 % AVG Validation Acc 78.87 %\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/100 AVG Training Loss:0.490 AVG Validation Loss:0.488 AVG Training Acc 79.96 % AVG Validation Acc 78.87 %\n",
      "Epoch:30/100 AVG Training Loss:0.486 AVG Validation Loss:0.485 AVG Training Acc 80.10 % AVG Validation Acc 79.00 %\n",
      "Epoch:40/100 AVG Training Loss:0.486 AVG Validation Loss:0.487 AVG Training Acc 80.13 % AVG Validation Acc 78.73 %\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/100 AVG Training Loss:0.483 AVG Validation Loss:0.489 AVG Training Acc 80.16 % AVG Validation Acc 78.73 %\n",
      "Epoch:60/100 AVG Training Loss:0.482 AVG Validation Loss:0.489 AVG Training Acc 80.28 % AVG Validation Acc 78.87 %\n",
      "Epoch:70/100 AVG Training Loss:0.482 AVG Validation Loss:0.489 AVG Training Acc 80.31 % AVG Validation Acc 78.87 %\n",
      "Epoch:80/100 AVG Training Loss:0.482 AVG Validation Loss:0.489 AVG Training Acc 80.31 % AVG Validation Acc 78.87 %\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/100 AVG Training Loss:0.482 AVG Validation Loss:0.489 AVG Training Acc 80.35 % AVG Validation Acc 78.87 %\n",
      "Epoch:100/100 AVG Training Loss:0.482 AVG Validation Loss:0.489 AVG Training Acc 80.37 % AVG Validation Acc 78.87 %\n",
      "Split 38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4454732db2b3418d919958332333d715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.492 AVG Validation Loss:0.515 AVG Training Acc 79.96 % AVG Validation Acc 78.60 %\n",
      "Epoch:20/100 AVG Training Loss:0.486 AVG Validation Loss:0.517 AVG Training Acc 80.02 % AVG Validation Acc 78.60 %\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/100 AVG Training Loss:0.475 AVG Validation Loss:0.527 AVG Training Acc 80.23 % AVG Validation Acc 78.47 %\n",
      "Epoch:40/100 AVG Training Loss:0.471 AVG Validation Loss:0.531 AVG Training Acc 80.38 % AVG Validation Acc 78.47 %\n",
      "Epoch:50/100 AVG Training Loss:0.469 AVG Validation Loss:0.532 AVG Training Acc 80.38 % AVG Validation Acc 78.33 %\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:60/100 AVG Training Loss:0.466 AVG Validation Loss:0.533 AVG Training Acc 80.61 % AVG Validation Acc 78.47 %\n",
      "Epoch:70/100 AVG Training Loss:0.467 AVG Validation Loss:0.534 AVG Training Acc 80.50 % AVG Validation Acc 78.47 %\n",
      "Epoch:80/100 AVG Training Loss:0.463 AVG Validation Loss:0.536 AVG Training Acc 80.53 % AVG Validation Acc 78.47 %\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/100 AVG Training Loss:0.464 AVG Validation Loss:0.536 AVG Training Acc 80.67 % AVG Validation Acc 78.47 %\n",
      "Epoch:100/100 AVG Training Loss:0.465 AVG Validation Loss:0.536 AVG Training Acc 80.50 % AVG Validation Acc 78.47 %\n",
      "Split 39\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b6c8e989454d5ead2fb32dc9c35808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.497 AVG Validation Loss:0.483 AVG Training Acc 79.73 % AVG Validation Acc 81.16 %\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/100 AVG Training Loss:0.486 AVG Validation Loss:0.481 AVG Training Acc 79.75 % AVG Validation Acc 81.16 %\n",
      "Epoch:30/100 AVG Training Loss:0.481 AVG Validation Loss:0.485 AVG Training Acc 79.84 % AVG Validation Acc 80.75 %\n",
      "Epoch:40/100 AVG Training Loss:0.476 AVG Validation Loss:0.490 AVG Training Acc 79.96 % AVG Validation Acc 80.89 %\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:50/100 AVG Training Loss:0.466 AVG Validation Loss:0.507 AVG Training Acc 80.34 % AVG Validation Acc 80.48 %\n",
      "Epoch:60/100 AVG Training Loss:0.462 AVG Validation Loss:0.507 AVG Training Acc 80.29 % AVG Validation Acc 80.48 %\n",
      "Epoch:70/100 AVG Training Loss:0.461 AVG Validation Loss:0.510 AVG Training Acc 80.46 % AVG Validation Acc 80.35 %\n",
      "Epoch:80/100 AVG Training Loss:0.461 AVG Validation Loss:0.508 AVG Training Acc 80.49 % AVG Validation Acc 80.35 %\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:90/100 AVG Training Loss:0.459 AVG Validation Loss:0.510 AVG Training Acc 80.55 % AVG Validation Acc 80.22 %\n",
      "Epoch:100/100 AVG Training Loss:0.459 AVG Validation Loss:0.510 AVG Training Acc 80.28 % AVG Validation Acc 80.22 %\n",
      "Split 40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005b941c36f041d7af6baf700620de2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.492 AVG Validation Loss:0.501 AVG Training Acc 79.87 % AVG Validation Acc 79.68 %\n",
      "Epoch:20/100 AVG Training Loss:0.484 AVG Validation Loss:0.505 AVG Training Acc 79.87 % AVG Validation Acc 79.41 %\n",
      "Epoch:30/100 AVG Training Loss:0.482 AVG Validation Loss:0.520 AVG Training Acc 79.98 % AVG Validation Acc 79.41 %\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/100 AVG Training Loss:0.465 AVG Validation Loss:0.513 AVG Training Acc 80.43 % AVG Validation Acc 78.33 %\n",
      "Epoch:50/100 AVG Training Loss:0.456 AVG Validation Loss:0.521 AVG Training Acc 80.64 % AVG Validation Acc 78.60 %\n",
      "Epoch:60/100 AVG Training Loss:0.450 AVG Validation Loss:0.530 AVG Training Acc 80.79 % AVG Validation Acc 78.60 %\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/100 AVG Training Loss:0.446 AVG Validation Loss:0.533 AVG Training Acc 81.22 % AVG Validation Acc 78.87 %\n",
      "Epoch:80/100 AVG Training Loss:0.443 AVG Validation Loss:0.534 AVG Training Acc 81.41 % AVG Validation Acc 79.00 %\n",
      "Epoch:90/100 AVG Training Loss:0.441 AVG Validation Loss:0.538 AVG Training Acc 81.41 % AVG Validation Acc 78.87 %\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:100/100 AVG Training Loss:0.441 AVG Validation Loss:0.534 AVG Training Acc 81.26 % AVG Validation Acc 78.73 %\n",
      "Split 41\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7552c3f695444c83baea12c2bc6e44d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.494 AVG Validation Loss:0.481 AVG Training Acc 79.78 % AVG Validation Acc 80.51 %\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/100 AVG Training Loss:0.484 AVG Validation Loss:0.481 AVG Training Acc 79.86 % AVG Validation Acc 80.51 %\n",
      "Epoch:30/100 AVG Training Loss:0.477 AVG Validation Loss:0.483 AVG Training Acc 79.90 % AVG Validation Acc 80.51 %\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(normalized_data.keys()):\n",
    "    \n",
    "    print(i)\n",
    "    threshold_dict = {} #dict to store information in for each threshold\n",
    "    data = deepcopy(normalized_data[i])\n",
    "    \n",
    "    #set X and Y columns\n",
    "    X = data[data.columns[:25]] #different timesteps\n",
    "    y = data[data.columns[-4:]] #the 4 different putative targets\n",
    "    \n",
    "    for k in tqdm(targets):\n",
    "        print(k)\n",
    "        \n",
    "        #Start with train test split\n",
    "        X_train_val, X_test, y_train_val, y_test, = train_test_split(\n",
    "                                    X,\n",
    "                                   y[k], #replace when going for multi-target \n",
    "                                   test_size = 0.20,\n",
    "                                   random_state = 15,\n",
    "                                   shuffle=True,\n",
    "                                   stratify = y[k] #replace when going for multi-target\n",
    "                                    )\n",
    "\n",
    "        # ##second, convert everything to pytorch tensor - we will convert to tensor dataset and \n",
    "        X_train_tensors = Variable(torch.Tensor(X_train_val.values))\n",
    "        X_test_tensors = Variable(torch.Tensor(X_test.values))\n",
    "\n",
    "        y_train_tensors = Variable(torch.Tensor(y_train_val.values))\n",
    "        y_test_tensors = Variable(torch.Tensor(y_test.values)) \n",
    "\n",
    "        #now, we have a dataset with features\n",
    "        print('Before reshaping:')\n",
    "        print(\"Training Shape\", X_train_tensors.shape, y_train_tensors.shape)\n",
    "        print(\"Testing Shape\", X_test_tensors.shape, y_test_tensors.shape)\n",
    "\n",
    "        #reshaping to rows, timestamps, features \n",
    "        X_train_tensors = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], X_train_tensors.shape[1], 1))\n",
    "        X_test_tensors = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], X_test_tensors.shape[1], 1))\n",
    "\n",
    "        #repeat for y\n",
    "        y_train_tensors = y_train_tensors.type(torch.cuda.LongTensor)\n",
    "        y_test_tensors = y_test_tensors.type(torch.cuda.LongTensor)\n",
    "\n",
    "        print('\\nAfter reshaping:')\n",
    "        print(\"Training Shape\", X_train_tensors.shape, y_train_tensors.shape)\n",
    "        print(\"Testing Shape\", X_test_tensors.shape, y_test_tensors.shape)\n",
    "\n",
    "        #create dataset\n",
    "        dataset = TensorDataset(X_train_tensors, y_train_tensors)\n",
    "        \n",
    "        #create dict to store fold performance\n",
    "        foldperf={}\n",
    "        \n",
    "        #reset \"best accuracy for treshold i and target k\"\n",
    "        \n",
    "        best_accuracy = 0\n",
    "\n",
    "        for fold, (train_idx,val_idx) in tqdm(enumerate(splits.split(np.arange(len(dataset))))):\n",
    "\n",
    "            print('Split {}'.format(fold + 1))\n",
    "\n",
    "            train_sampler = SubsetRandomSampler(train_idx)\n",
    "            val_sampler = SubsetRandomSampler(val_idx)\n",
    "            train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "            val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "    \n",
    "            #creates new model for each \n",
    "            model = LSTM_Uni(num_classes, input_size, hidden_size, num_layers, X_train_tensors.shape[1]).to('cuda') #our lstm class\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "            scheduler = ReduceLROnPlateau(optimizer, \n",
    "                                  'min', \n",
    "                                  patience = 10,\n",
    "                                  cooldown = 20,\n",
    "                                 verbose = True)\n",
    "    \n",
    "            history = {'train_loss': [], 'val_loss': [],'train_acc':[],'val_acc':[], 'precision': [],\n",
    "                      'recall' : [], 'auroc': []}\n",
    "\n",
    "            for epoch in tqdm(range(num_epochs)):\n",
    "                train_loss, train_correct=train_epoch(model,train_loader,criterion,optimizer)\n",
    "                val_loss, val_correct, precision, recall, auroc = valid_epoch(model,val_loader,criterion)\n",
    "\n",
    "                train_loss = train_loss / len(train_loader.sampler)\n",
    "                train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "                val_loss = val_loss / len(val_loader.sampler)\n",
    "                val_acc = val_correct / len(val_loader.sampler) * 100\n",
    "        \n",
    "        \n",
    "                if (epoch+1) % 10 == 0: \n",
    "                    print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Validation Loss:{:.3f} AVG Training Acc {:.2f} % AVG Validation Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                             num_epochs,\n",
    "                                                                                                             train_loss,\n",
    "                                                                                                             val_loss,\n",
    "                                                                                                             train_acc,\n",
    "                                                                                                             val_acc))\n",
    "                history['train_loss'].append(train_loss)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['train_acc'].append(train_acc)\n",
    "                history['val_acc'].append(val_acc)\n",
    "                history['precision'].append(precision)\n",
    "                history['recall'].append(recall)\n",
    "                history['auroc'].append(auroc)\n",
    "                scheduler.step(val_loss)\n",
    "    \n",
    "                if val_acc > best_accuracy:\n",
    "            \n",
    "                #replace best accuracy and save best model\n",
    "                    print(f'New Best Accuracy found: {val_acc:.2f}%\\nEpoch: {epoch + 1}')\n",
    "                    best_accuracy = val_acc\n",
    "                    best = deepcopy(model)\n",
    "                    curr_epoch = epoch + 1\n",
    "                    \n",
    "            #store fold performance\n",
    "            foldperf['fold{}'.format(fold+1)] = history\n",
    "        \n",
    "        #saves fold performance for target \n",
    "        threshold_dict[k] = pd.DataFrame.from_dict(foldperf, orient='index') # convert dict to dataframe\n",
    "        \n",
    "        #explode to get eacxh epoch as a row\n",
    "        threshold_dict[k] = threshold_dict[k].explode(list(threshold_dict[k].columns))\n",
    "        torch.save(best,f\"../Models/{i}/Nova_IMS_relative_clicks_best_{k}_{curr_epoch}_epochs.h\")\n",
    "        \n",
    "    # from pandas.io.parsers import ExcelWriter\n",
    "    with pd.ExcelWriter(f\"../Data/Modeling Stage/Results/IMS/Clicks_duration_relative/25_splits_{i}_{replicas}_replicas.xlsx\") as writer:  \n",
    "        for sheet in targets:\n",
    "                threshold_dict[sheet].to_excel(writer, sheet_name=str(sheet))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
