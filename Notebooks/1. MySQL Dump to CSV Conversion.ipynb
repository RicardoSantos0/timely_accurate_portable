{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21b99636",
   "metadata": {},
   "source": [
    "# Notebook 1: Extracting the relevant \n",
    "\n",
    "In this notebook, our goal is to extract datasets for supervised classification from raw moodle logs - stored as a MySQL Dump.\n",
    "\n",
    "We will start by using Python to connect to a local MySQL database restored from the original dump found in: https://storage.googleapis.com/dissertation-data/dissertation-export/mysql-export\n",
    "\n",
    "This is a computationally expensive process: the dump file size is over 6.5 Gb and we've found some issues loading such a large database into MySQL through the Workbench - only managed it through the console. \n",
    "\n",
    "In the interest of time and available space, we will use this notebook to: \n",
    "\n",
    "1. Extracted Moodle Logs and other relevant tables from the MySQL local instance,\n",
    "2. Save the extracted files as csvs - our raw datasets, \n",
    "\n",
    "Future work will be developed on these csv files, which will save us the hassle of continuously having to connect to MySQL.\n",
    "\n",
    "#### Step 1: Load relevant packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226f3214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packagest to handle MySQL dump\n",
    "from pymysql import connect\n",
    "\n",
    "# Directory Manipulation\n",
    "import os\n",
    "\n",
    "#progress bar and pandas activation\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "#pandas and np for standard preprocessing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb48c690",
   "metadata": {},
   "source": [
    "#### Step 2: Connect to Database and Open Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57857191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to db - use your appropriate credentials\n",
    "mydb = connect(host='localhost',\n",
    "               port= 3306,\n",
    "               user='root',\n",
    "               password='***********',\n",
    "               db = 'data_students')\n",
    "\n",
    "#connect cursor\n",
    "cur = mydb.cursor()\n",
    "\n",
    "#show all tables and store in list,\n",
    "cur.execute(\"SHOW TABLES\")\n",
    "sql_tables = cur.fetchall()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e2fb6f",
   "metadata": {},
   "source": [
    "### This will be an hardcoded segment\n",
    "\n",
    "After seeing the list, and knowing the database, we know which tables we want in this instance:\n",
    "\n",
    "'mdl_context', 'mdl_course', 'mdl_course_modules', 'mdl_grade_grades','mdl_grade_items', 'mdl_role_assignments', 'log'\n",
    "\n",
    "We will start by making a list of the tables we intend to keep.\n",
    "Then we will look into importing all these tables into dfs that we will store in a dict of dataframes.\n",
    "\n",
    "If you'd like to use this script in the future, even if for other purposes, make sure that you make the necessary adjustments in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff1b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating table - need to check, in the future, for a way to automate this segment\n",
    "sql_tables = ['mdl_context', 'mdl_course', 'mdl_course_modules', \n",
    "              'mdl_grade_grades','mdl_grade_items', 'mdl_role_assignments', 'log']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd637e35",
   "metadata": {},
   "source": [
    "### Back to automated section\n",
    "\n",
    "#### Step 3: Import all tables and Store them in Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec06b283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 7/7 [48:12<00:00, 413.16s/it]\n"
     ]
    }
   ],
   "source": [
    "#create import template\n",
    "select_template = 'SELECT * FROM {table_name}'\n",
    "frames_dict = {}\n",
    "\n",
    "#import all relevant tables and store them in dict\n",
    "for tname in tqdm(sql_tables):\n",
    "    query = select_template.format(table_name = tname)\n",
    "    frames_dict[tname] = pd.read_sql(query, mydb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee820694",
   "metadata": {},
   "source": [
    "#### Step 4: Save all the relevant tables as CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fbc6390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [08:14<00:00, 70.61s/it]\n"
     ]
    }
   ],
   "source": [
    "#exporting each table as csv\n",
    "for i in tqdm(frames_dict): \n",
    "    frames_dict[i].to_csv('../Data/R_Gonz_data_'+str(i)+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4df3af",
   "metadata": {},
   "source": [
    "### All done, for now.\n",
    "\n",
    "The main purpose of this notebook was to extract the relevant data from the source MySQL dump file. In the next section, we will take the data we exported and start working towards the work's main goals.\n",
    "\n",
    "Until then."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
