{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook 2.1 Data Understanding and Preprocessing of Support Tables\n",
    "\n",
    "For all intents and purposes, this should be considered as the second real notebook that is part of the thesis work. In it, we will look at the support tables that are part of the NOVA IMS original database.\n",
    "\n",
    "#### 1. We are familiarized with the general structure of the logs\n",
    "\n",
    "Before going further, we should assess the remaining tables presented in the database. \n",
    "\n",
    "Recall, **logs record interactions with the system and we are looking for ways to determine whether these interactions can assist educators identify at risk students and high performing students.**\n",
    "\n",
    "Thus, to make the best out of the logs, we will need to perform different segmentations and it is likely that we will need perform some filtering. \n",
    "\n",
    "### To do that, we will take a look at all tables\n",
    "\n",
    "We will look at all tables and all columns to make a preliminary assessment of the utility of the available elements.\n",
    "In general, these are support elements that will be used sparsely, as most of the relevant information is present in the logs.\n",
    "\n",
    "The observation of each table will resort to the same chain of commands:\n",
    "\n",
    "info -> to observe count and datatype of each column, \n",
    "describe -> a command that that returns the most notable descriptive statistics of each column.\n",
    "The obeservation of each table ends with a look at the raw data (At least the visible rows).\n",
    "\n",
    "#### 2. We'll start this notebook by importing all relevant packages and data\n",
    "\n",
    "All data is stored in an excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other tables with support information\n",
    "support_table = pd.read_excel('../Data/Nova_IMS_logs_Moodle_cursos.xlsx', sheet_name = None,\n",
    "                             dtype = {\n",
    "                                 'cd_lectivo' : object,\n",
    "                                 'cd_curso' : object,\n",
    "                                 'cd_Discip': object,\n",
    "                                 'cd_discip': object,\n",
    "                                 'userId': object,\n",
    "                                 'dataExame': pd.datetime,\n",
    "                             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this cell to write any additional piece of code that may be required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the support table dict is composed by 2 distinct tables.\n",
    "\n",
    "The first table includes \n",
    "\n",
    "**Next, we'll consider grades**\n",
    "\n",
    "Student performance is, in general, measured by the student's grade. So... how do we measure grades?\n",
    "As all we have is data from Moodle, it is important that we can either find or calculate the targets from Moodle data.\n",
    "\n",
    "So, in an immediate fashion, we'll have to identify which courses have graded assignments and slice a course list for those.\n",
    "we can deal with calculating our target at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56651 entries, 0 to 56650\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   cd_lectivo       56651 non-null  object \n",
      " 1   cd_curso         56651 non-null  object \n",
      " 2   nm_curso_pt      56651 non-null  object \n",
      " 3   nm_ramo          56651 non-null  object \n",
      " 4   cd_Discip        56651 non-null  object \n",
      " 5   ds_discip_pt     56651 non-null  object \n",
      " 6   semestre         56651 non-null  object \n",
      " 7   userId           56651 non-null  object \n",
      " 8   statusAvaliacao  56651 non-null  object \n",
      " 9   statusEpoca      56651 non-null  object \n",
      " 10  notaAvaliacao    56651 non-null  float64\n",
      " 11  cd_final         56651 non-null  object \n",
      " 12  statusFinal      56651 non-null  object \n",
      " 13  notaFinal        55486 non-null  float64\n",
      "dtypes: float64(2), object(12)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "source": [
    "support_table['logs_Moodle_cursos'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3056 entries, 0 to 3055\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   cd_lectivo      3056 non-null   object        \n",
      " 1   semestre        3056 non-null   object        \n",
      " 2   cd_discip       3056 non-null   object        \n",
      " 3   epocaAvaliacao  3056 non-null   object        \n",
      " 4   dataExame       3056 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](1), object(4)\n",
      "memory usage: 119.5+ KB\n"
     ]
    }
   ],
   "source": [
    "support_table['datasExames'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['logs_Moodle_cursos', 'datasExames'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support_table.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the informationformat feature has no null values and is a single value feature, so we remove it\n",
    "grades_table.drop('informationformat', axis = 1, inplace = True)\n",
    "\n",
    "#time created and timemodified seem to be time features, so we will appropriately make the conversion to datetime\n",
    "grades_table['timecreated'] = pd.to_datetime(grades_table['timecreated'], unit = 's', errors = 'coerce')\n",
    "grades_table['timemodified'] = pd.to_datetime(grades_table['timemodified'], unit = 's', errors = 'coerce')\n",
    "\n",
    "#experimental - to delete if need be - only keeping students - assuming we have correctly only kept students\n",
    "#grades_table = grades_table[grades_table['userid'].isin(students)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades_table.describe(include ='all', datetime_is_numeric = True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this cell to write any additional piece of code that may be required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, we have the Grade_item_table**\n",
    "\n",
    "The grade_item table stores information concerning every gradeable item present in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_item_table.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_item_table['gradetype'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the informationformat feature has no null values and is a single value feature, so we remove it\n",
    "grade_item_table.drop([\n",
    "                    'itemname',\n",
    "                    'plusfactor',\n",
    "                    'timecreated',\n",
    "                    'timemodified'\n",
    "                    ],\n",
    "                    axis = 1, inplace = True)\n",
    "\n",
    "grade_item_table.rename(columns = {'id' : 'itemid'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_item_table.describe(include ='all', datetime_is_numeric = True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_item_table['itemmodule'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this cell to write any additional piece of code that may be required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is possible to see that only a subset of courses has graded items.\n",
    "\n",
    "As performed by the authors of the Riestra-Gonzalez paper, we will only look to work with courses that have graded assignments. \n",
    "The reason for this option is straightforward - we have no access to the SIS, which means that our target will be, in some shape or form, related to the graded assignments.\n",
    "\n",
    "**After looking at the grades tables, it is important to incorporate the information presented in these tables with the tables about courses**.\n",
    "\n",
    "For that, we have access to multiple dfs related to the courses themselves.\n",
    "Behold, the course_table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_table.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_table.describe(include ='all', datetime_is_numeric = True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the course_table has multiple single value feature with no nans, as with the previous considered features, we will remove them\n",
    "\n",
    "course_table.drop(['fullname',\n",
    "                       'summary',\n",
    "                       'requested',\n",
    "                       'enablecompletion',\n",
    "                       'completionnotify'],\n",
    "                       axis = 1, inplace = True)\n",
    "\n",
    "#time created and timemodified seem to be time features, so we will appropriately make the conversion to datetime\n",
    "course_table['startdate'] = pd.to_datetime(course_table['startdate'], unit = 's', errors = 'coerce')\n",
    "course_table['timecreated'] = pd.to_datetime(course_table['timecreated'], unit = 's', errors = 'coerce')\n",
    "course_table['timemodified'] = pd.to_datetime(course_table['timemodified'], unit = 's', errors = 'coerce')\n",
    "course_table['cacherev'] = pd.to_datetime(course_table['cacherev'], unit = 's', errors = 'coerce') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this cell to write any additional piece of code that may be required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The course module table is present in other datasets \"e.g. The Open Moodle Dataset\", \n",
    "\n",
    "According to it, the course module table describes every activity performed with Moodle. In our case, it records every activity performed in every course.\n",
    "\n",
    "Here follows a brief overview of this table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_mod_table.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the course_table has multiple single value feature with no nans, as with the previous considered features, we will remove them\n",
    "\n",
    "course_mod_table.drop([\n",
    "                    'groupmembersonly',\n",
    "                    'completion',\n",
    "                    'completionview',\n",
    "                    'showdescription',\n",
    "                    'completionexpected',\n",
    "                    'score',\n",
    "                    ],\n",
    "                    axis = 1, inplace = True)\n",
    "\n",
    "#added, availablefrom and availableuntil seem to be time features, so we will appropriately make the conversion to datetime\n",
    "course_mod_table['added'] = pd.to_datetime(course_mod_table['added'], unit = 's', errors = 'coerce')\n",
    "course_mod_table['availablefrom'] = pd.to_datetime(course_mod_table['availablefrom'], unit = 's', errors = 'coerce')\n",
    "course_mod_table['availableuntil'] = pd.to_datetime(course_mod_table['availableuntil'], unit = 's', errors = 'coerce')\n",
    "\n",
    "#renaming variables that we will use later on for mergers\n",
    "course_mod_table.rename(columns = {'instance': 'iteminstance', 'course': 'courseid', 'id' : 'assign_id'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_mod_table.describe(include = 'all', datetime_is_numeric = True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_mod_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this cell to write any additional piece of code that may be required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last set of tables to check is the one that contains the context_table. The utility of these tables is rather unclear at this moment.\n",
    "\n",
    "#### Context table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_table.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_table.describe(include = 'all', datetime_is_numeric = True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this cell to write any additional piece of code that may be required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. To business\n",
    "\n",
    "The information stored in these tables is pivotal for our work with the logs. Ignoring all other noise potential insights that may arise from this data we are, for the most part, interested in 3 things:\n",
    "\n",
    "1. Identify the student population - already achieved\n",
    "2. Compute Student Performance - our target\n",
    "3. Get course duration - or find a way to compute those - the courses to that we will take forward.\n",
    "\n",
    "We've been discussing continuously that we want to, in some capacity, predict student performance. As we do not have access to the final grades, we will need to infer it from graded Moodle assignments. The first, and almost immediate observation is that we will can only use courses that use Moodle in this capacity -> which will reduce the number of courses we have to work with.\n",
    "\n",
    "We will follow the formula adopted by the authors of the Riestra-González paper:\n",
    "\n",
    "#### Student Performance and Course Duration\n",
    "\n",
    "The authors got to student performance and course duration by performing inner joins across multiple tables and filtered across different conditions:\n",
    "\n",
    "course_mod_table,\n",
    "grades_table,\n",
    "grade_item_table\n",
    "\n",
    "We will replicate their steps and hopefully, reach suport tables that return comparable results. The first step is to perform the removal of rows that will be unnecessary for us. We can only construct a solution for items that are graded and for which we have the means to estimate the course duration. \n",
    "\n",
    "Thus, in the grades_table, we will look to only keep rows that can, simultaneuously, fulfill the following pre-requisite:\n",
    "1. Have a valid final grade,\n",
    "\n",
    "The second phase will be to perform inner joins of the different tables:\n",
    "1. course_mod_table with grade_item_table on iteminstance and courseid\n",
    "2. grade_item_table.id with grades_table.itemid\n",
    "3. The merge of the previous 2 merged tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Step 1, removing all rows that have no interest to us\n",
    "\n",
    "# grades_table.dropna(subset = ['finalgrade','timecreated', 'timemodified'], inplace = True)\n",
    "\n",
    "#Step 2: Create temporary tables that associate courses and assignments\n",
    "placeholder_1 = pd.merge(course_mod_table, grade_item_table, on=['iteminstance','courseid'], how='inner')\n",
    "\n",
    "#Step 3: Create second temporary table that associates grades with assignments\n",
    "placeholder_2 = pd.merge(placeholder_1, grades_table, on ='itemid', how='inner')\n",
    "\n",
    "#step 3: merge both placeholder tables\n",
    "support_table = placeholder_2[:]\n",
    "support_table['sup_time'] = np.where(support_table['timecreated'] > support_table['timemodified'],\n",
    "                                support_table['timecreated'], support_table['timemodified'])\n",
    "\n",
    "#step 4: only keep graded items, which means nonzero max grades\n",
    "support_table = support_table[(support_table['rawgrademax'] > 0) & (support_table['sup_time'] >= '2014-08-24')]\n",
    "\n",
    "del placeholder_1, placeholder_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As a final step, we will store the start date of each course - as it will provide us with the means to, further down the line, perform the inference for course duration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep rows worth merging - this cell can only be run once\n",
    "course_table = course_table.filter(['id', 'startdate']).rename(columns = {'id': 'courseid'})\n",
    "\n",
    "#perform inner join between support table and courses with grades\n",
    "support_table = pd.merge(support_table, course_table, on = 'courseid', how = 'inner')\n",
    "\n",
    "#only keep the final result\n",
    "support_table = support_table[(support_table['startdate'] <= support_table['sup_time']) & (support_table['startdate'].dt.year >= 2014)].filter(['assign_id', 'courseid', 'startdate', 'userid', 'finalgrade', \n",
    "                                      'rawgrademax', 'sup_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "support_table.describe(include = 'all', datetime_is_numeric = True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will finish this section by filtering the features to keep and, afterward, export the support table to use with the LMS logs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2: \n",
    "\n",
    "By now, we know, generally:\n",
    "\n",
    "- all courses that had graded assignments (i.e. whose max assignment grade was not 0) - courseid,\n",
    "- all students that were registered in the curricular unit - userid,\n",
    "- if a student delivered an assignment or not and the assignment's grade - finalgrade.\n",
    "\n",
    "This information is especially useful because it will assist us in the proper filtering of the moodle activity logs and, additionally, assist us in the achievement of valuable information needed for the project: course duration and target.\n",
    "\n",
    "#### 1. Course duration: \n",
    "\n",
    "We have the start date for each course. The authors of original paper inferred course end to occur at the 95% log threshold. That is to say, 5% of the logs were registered after the end of course. We have no way to obtain a better estimate so we will accept the postulation. When we deal with the logs, we will be able to calculate end of course date.\n",
    "\n",
    "#### 2. Targets - finalgrade:\n",
    "\n",
    "The authors calculated final grade as a construct computed from the assignment grades. Again, we will accept the author's methods for this. \n",
    "\n",
    "**First**: to classify whether different assignments were mandatory or not\n",
    "\n",
    "The authors of the paper focused made a split between mandatory and optional assignments. In their view, any assignment whose submittal rate (relative to the number of students attending the course) is 40% or under would be considered an optional assignment. \n",
    "\n",
    "1. We will need to know which assignments are optional and which are mandatory. We have, from our support table, the ability to list the courses and students that attending the course. From here, we can get the number of students attending each course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get to create a pivot-table that associates students and the courses they are attending\n",
    "student_list = pd.pivot_table(support_table, index='userid', columns = 'courseid', values = 'assign_id',\n",
    "                    aggfunc='count')\n",
    "\n",
    "# we use the describe command to get the course-level aggregate statistics\n",
    "# count -> number of students attending, mean is the average number of clicks performed by each student \n",
    "student_count = student_list.describe(include = 'all').T.sort_values(by = 'count', ascending = False)['count'].reset_index()\n",
    "\n",
    "#from here, we can create a dict that associates each course to the number of students attending the course\n",
    "student_count = student_count.set_index('courseid').to_dict()['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We can, in some capacity, partially repeat the steps performed in the previous pivot-table and make the option/mandatory classification of each assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get to create a pivot-table that associates assignments and the courses are asked on\n",
    "assign_number = pd.pivot_table(support_table.dropna(), index= 'userid', columns = ['courseid', 'assign_id'], values = 'finalgrade',\n",
    "                    aggfunc='count')\n",
    "\n",
    "# we use the describe command to get the course-level aggregate statistics\n",
    "# count -> number of students delivering the assignment, mean is the average number of students delivering the assignment \n",
    "assign_number = assign_number.describe(include = 'all').T.sort_values(by = 'count', ascending = False)['count'].reset_index()\n",
    "\n",
    "#from her, we can create 2 columns: i) one with the number of students attending the course\n",
    "assign_number['registered_students'] = assign_number['courseid'].map(student_count)\n",
    "\n",
    "#then, we can calculate the percentage of assignments delivered relative to the number of attending students\n",
    "assign_number['%_submissions'] = assign_number['count'] / assign_number['registered_students']\n",
    "\n",
    "#finally, we classify each assignment as mandatory vs non-mandatory (over 40% submission rates)\n",
    "assign_number['mandatory_status'] = np.where(assign_number['%_submissions'] > 0.4, 1, 0)\n",
    "\n",
    "#from here, we can create a dict that associates each course to the number of students attending the course\n",
    "mandatory_status = assign_number.set_index('assign_id').to_dict()['mandatory_status']\n",
    "\n",
    "#from here, we can now map the mandatory vs non mandatory status of \n",
    "support_table['mandatory_status'] = support_table['assign_id'].map(mandatory_status)\n",
    "\n",
    "del assign_number, student_count, mandatory_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have assigned the mandatory status to different assignments. We will not use this knowledge immediatly, but we will need it later. What it allows us is the ability to perform new computations.\n",
    "\n",
    "**3. Now, we can clean unnecessary assignments and courses. We can now perform the following operations:**\n",
    "\n",
    "1. identify whether the students made the delivery of the assignment or not - nans vs non nans\n",
    "\n",
    "2. give every nan the classification of 0.\n",
    "\n",
    "3. verify whether any courses have average finalgrade of 0. By extension, every course that only has 0 mean finalgrades will also excluded.\n",
    "\n",
    "4. Another variant we considered was the removal of all assignments with average finalgrade = 0. Ultimately, we opted to keep these in courses where there are assignments with finalgrade > 0,\n",
    "\n",
    "5. Additionally, we considered the removal of average finalgrade equal to the rawgrademax . However, we opted to keep these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether the assignment was delivered by the student or not\n",
    "support_table['delivered'] = np.where(support_table['finalgrade'].isna(), 0, 1)\n",
    "\n",
    "#now, we fill the nas of finalgrade with 0\n",
    "support_table.fillna(0, inplace = True)\n",
    "\n",
    "#as a final note, we can now verify which courses we can exclude\n",
    "#criteria 1: if all assignments have average grade 0, the course can be excluded\n",
    "assignments_keep = support_table.groupby(['courseid']).agg({\n",
    "                                                    'userid': 'count',\n",
    "                                                    'finalgrade' : 'mean',\n",
    "                                                    'rawgrademax' : 'mean',\n",
    "                                                    },\n",
    "                                                    )\n",
    "\n",
    "#now we select assignments that fulfill the criteria avg finalgrade = 0 and store it in a list\n",
    "assignments_keep = list(assignments_keep[assignments_keep['finalgrade'] > 0].reset_index()['courseid'])\n",
    "\n",
    "#so, we keep assignments who have a positive finalgrade\n",
    "support_table = support_table[support_table['courseid'].isin(assignments_keep)]\n",
    "\n",
    "del assignments_keep\n",
    "\n",
    "#check\n",
    "support_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table.describe(include = 'all', datetime_is_numeric = True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Before finishing this notebook, there is still one thing we need to do:**\n",
    "\n",
    "So far, the students registered in each course, and their results in graded assignments.\n",
    "\n",
    "The authors of the Riestra González paper used the following equation to calculate target with several different possible values between 0 and 1 for $\\alpha$:\n",
    "\n",
    "$$\\hat{Y} = 10(\\alpha \\frac{\\sum{} mandatory\\:assignment\\:marks}{number\\:of\\:mandatory\\:assignments} + (1 - \\alpha) \\frac{\\sum{} optional\\:assignment\\:marks}{number\\:of\\:optional\\:assignments})$$\n",
    "\n",
    "We will now calculate our results for final marks using the same value used by the authors of the R. Gonzalez paper:\n",
    "\n",
    "$\\alpha = 0.5$\n",
    "\n",
    "In order to do that, we will start by normalizing all grades relative to their max possible grade to a 0 to be on a 0 to 1 scale.\n",
    "\n",
    "Then, we will compute the final mark, which will allow us to compute the targets for our classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hwe will start by defining a function that will assist us in dealing with the multiindex\n",
    "\n",
    "def flattenHierarchicalCol(col,sep = '_'):\n",
    "    '''converts multiindex columns into single index columns while retaining the hierarchical components'''\n",
    "    if not type(col) is tuple:\n",
    "        return col\n",
    "    else:\n",
    "        new_col = ''\n",
    "        for leveli,level in enumerate(col):\n",
    "            if not level == '':\n",
    "                if not leveli == 0:\n",
    "                    new_col += sep\n",
    "                new_col += level\n",
    "        return new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 1: current grade scales are varying between 0.7 and 1001. The fastest way to account for this \n",
    "support_table['assignment_mark'] = support_table['finalgrade'] / support_table['rawgrademax']\n",
    "\n",
    "#step 2: For every student and every course, we can obtain the sum and number of both mandatory and optional assignments:\n",
    "grade_estimation = support_table.groupby(['courseid', 'userid', 'mandatory_status']).agg({\n",
    "                                                                                    'assignment_mark' : ['sum', 'count'],\n",
    "                                                                                        })\n",
    "\n",
    "#applies the function that removes multiindex\n",
    "grade_estimation.columns = grade_estimation.columns.map(flattenHierarchicalCol)\n",
    "grade_estimation.reset_index(inplace = True)\n",
    "\n",
    "#now we can create an optional and a mandatory sum column for each sum \n",
    "grade_estimation['Optional'] = 0.5 * np.where(grade_estimation['mandatory_status'] == 0, #1 - alpha = 0.5,  \n",
    "                                              grade_estimation['assignment_mark_sum'] / grade_estimation['assignment_mark_count'],\n",
    "                                             np.nan)\n",
    "\n",
    "grade_estimation['Mandatory'] = 0.5 * np.where(grade_estimation['mandatory_status'] == 1, # alpha = 0.5,  \n",
    "                                              grade_estimation['assignment_mark_sum'] / grade_estimation['assignment_mark_count'],\n",
    "                                              np.nan)\n",
    "\n",
    "#for all intents and purposes, we can now remove the columns assignment_mark_sum\n",
    "grade_estimation.drop(['assignment_mark_sum', 'assignment_mark_count'], axis = 1, inplace = True)\n",
    "\n",
    "#we can now create a new pivot_table that perfectly arranges our intended result\n",
    "targets_table = grade_estimation.pivot_table(index=['courseid','userid'], \n",
    "                                         columns=['mandatory_status'],\n",
    "                                         values=['Optional', 'Mandatory'],aggfunc='sum')\n",
    "\n",
    "#next we remove the columns that we do not want to keep, final result being: for each discipline, the optional and the mandatory grades\n",
    "targets_table.columns.set_levels(['optional','mandatory'],level=1,inplace=True)\n",
    "targets_table.columns = targets_table.columns.map(flattenHierarchicalCol)\n",
    "targets_table.drop(['Mandatory_optional', 'Optional_mandatory'], axis = 1, inplace = True)\n",
    "\n",
    "#next, we sum the columns and multiply by 10:\n",
    "targets_table['final_mark'] = 10 * (targets_table['Mandatory_mandatory'].fillna(0) + targets_table['Optional_optional'].fillna(0))\n",
    "\n",
    "#if there are no optional assignments on the course, we will double the ponderation of the mandatory course\n",
    "targets_table['final_mark'] = np.where(targets_table['Optional_optional'].isna(), 2 * targets_table['final_mark'],\n",
    "                                                                                  targets_table['final_mark'])\n",
    "\n",
    "#if there are no optional assignments on the course, we will double the ponderation of the mandatory course\n",
    "targets_table['final_mark'] = np.where(targets_table['Mandatory_mandatory'].isna(), 2 * targets_table['final_mark'],\n",
    "                                                                                  targets_table['final_mark'])\n",
    "\n",
    "#before finishing this cell, we will now drop the unncecessary columns and keep the final mark\n",
    "targets_table.dropna(subset = ['Mandatory_mandatory']).reset_index(inplace = True)\n",
    "targets_table = targets_table.rename(columns = {'Mandatory_mandatory': 'Grade Mandatory', 'Optional_optional' : 'Grade Optional'})\n",
    "\n",
    "del grade_estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started the last cell with the normalization of the mark of each assignment. \n",
    "\n",
    "The cell finishes with a dataframe containing each curricular unit, each student attending it and the final mark of the student according to the formula we had placed previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_table.dropna(how = 'all', inplace = True)\n",
    "targets_table.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 2 distinct table that will be invaluable for our work in future notebooks.\n",
    "\n",
    "targets_table has stored every final mark obtain by each student attending the different courses of the university:\n",
    "\n",
    "- From final_mark, we finally are able to calculate our target variables:\n",
    "    - We can label students as at-risk or as overachievers depending on their mark,\n",
    "\n",
    "\n",
    "- From support_table, we will need more robust sets of information to be used for feature extraction and engineering:\n",
    "    - The startdate of each course,\n",
    "    - The individual mark of each assignment and at what time the assignment was delivered,\n",
    "    - The mandatory status of an assignment and whether it was or not delivered by the student in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_table.describe(include = 'all', datetime_is_numeric = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save tables \n",
    "targets_table.to_csv('../Data/Modeling Stage/R_Gonz_targets_table.csv') \n",
    "\n",
    "support_table.drop(['finalgrade', 'rawgrademax'], axis = 1).to_csv('../Data/R_Gonz_support_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Done for now\n",
    "\n",
    "In notebook 2.2. we will rely on the activity logs and our support table to perform the necessary filtering and preprocessing of the data in order to make it compliant with our necessities. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
