{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thesis notebook 4.2. - R_Gonz\n",
    "\n",
    "### Aplication of SMOTE - Keeping Outliers\n",
    "\n",
    "### Non-temporal data representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules/libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import warnings\n",
    "import time\n",
    "import plotly.express as px\n",
    "\n",
    "#tqdm to monitor progress\n",
    "from tqdm.notebook import tqdm, trange\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "#time related features\n",
    "from datetime import timedelta\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "from lightgbm import LGBMModel,LGBMClassifier\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer,PowerTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import ceil\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, RepeatedKFold, RepeatedStratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif, RFE, RFECV, SelectFromModel\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.cluster import AgglomerativeClustering , KMeans, DBSCAN\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "\n",
    "from sklearn.datasets import make_classification, load_digits\n",
    "from sklearn import svm\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, recall_score, classification_report, average_precision_score, precision_recall_curve\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables that may come in handy\n",
    "#course threshold sets the % duration that will be considered (1 = 100%)\n",
    "duration_threshold = [0.1, 0.25, 0.33, 0.5, 1]\n",
    "\n",
    "#colors for vizualizations\n",
    "nova_ims_colors = ['#BFD72F', '#5C666C']\n",
    "\n",
    "#standard color for student aggregates\n",
    "student_color = '#474838'\n",
    "\n",
    "#standard color for course aggragates\n",
    "course_color = '#1B3D2F'\n",
    "\n",
    "#standard continuous colormap\n",
    "standard_cmap = 'viridis_r'\n",
    "\n",
    "#Function designed to deal with multiindex and flatten it\n",
    "def flattenHierarchicalCol(col,sep = '_'):\n",
    "    '''converts multiindex columns into single index columns while retaining the hierarchical components'''\n",
    "    if not type(col) is tuple:\n",
    "        return col\n",
    "    else:\n",
    "        new_col = ''\n",
    "        for leveli,level in enumerate(col):\n",
    "            if not level == '':\n",
    "                if not leveli == 0:\n",
    "                    new_col += sep\n",
    "                new_col += level\n",
    "        return new_col\n",
    "    \n",
    "#number of replicas - number of repeats of stratified k fold - in this case 10\n",
    "replicas = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports dataframes\n",
    "course_programs = pd.read_excel(\"../Data/Modeling Stage/R_gonz_Non_temporal_Datasets.xlsx\", \n",
    "                                dtype = {\n",
    "                                    'course' : object,\n",
    "                                    'userid' : object},\n",
    "                               sheet_name = None)\n",
    "\n",
    "#drop unnamed 0 column\n",
    "for i in course_programs:\n",
    "    course_programs[i].drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "    \n",
    "#save tables \n",
    "class_list = pd.read_csv('../Data/Modeling Stage/R_Gonz_updated_classlist.csv', \n",
    "                         dtype = {\n",
    "                                   'course': object,\n",
    "                                   },\n",
    "                        parse_dates = ['Week before start', 'Start Date',\n",
    "                                       'End Date', 'Date_threshold_10',\n",
    "                                      'Date_threshold_25', 'Date_threshold_33', 'Date_threshold_50',\n",
    "                                      'Date_threshold_100']).drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_programs['Date_threshold_10'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Stage: Data understanding Stage\n",
    "\n",
    "#### INITIAL EXPLORATION, CLEANING & FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step1 - Take a look at the datesets\n",
    "for i in course_programs:\n",
    "    print(f'Time period: {i}\\n\\n{course_programs[i].head()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step1 - Take a look at the datesets\n",
    "for i in course_programs:\n",
    "    print(f'Time period: {i}\\n\\n{course_programs[i].info()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#describe method\n",
    "for i in tqdm(course_programs):\n",
    "    print(f'Time period: {i}\\n\\n{course_programs[i].describe(include = \"all\")}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOME DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#then we plot an histogram with all courses, we are not interested in keeping courses with a number of students inferior to 10\n",
    "sns.set_theme(context='paper', style='whitegrid', font='Calibri', rc={\"figure.figsize\":(16, 10)}, font_scale=2)\n",
    "\n",
    "#Plot the distributions of each feature \n",
    "for i in tqdm(course_programs):\n",
    "    print(i)\n",
    "    course_programs[i].hist(figsize=(16, 20), bins=100, xlabelsize=8, ylabelsize=8, color = student_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting target:\n",
    "\n",
    "As stated, we will predict the exam grade as it does not depend direcly from assignment grades.\n",
    "\n",
    "We will use a double classification problem:\n",
    "\n",
    "**Problem 1**: Identify students at risk of failure. In the portuguese system, a student passes if the final grade greater or equal to 10. Students who do not meet the threshold fail. When it comes to exams, the difference is not as clear cut. For simplicity, we will consider exam grade to be less or equal to 10 - same with final grade. \n",
    "\n",
    "**Problem 2**: Identify high performing students. The classification of high performers is not unanymous. A simple solution would be consider as high performers all students whose grade is greater or equal to a certain threshold (say 17/20). There are issues with this approach however:\n",
    "\n",
    "    - different teachers have different criteria for grading and their grading decisions. We can look at the top 20% of students in each course as identify them as the high performers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create loops for targets\n",
    "for i in tqdm(course_programs):\n",
    "    print(i)\n",
    "    \n",
    "    #deal with risk of failure\n",
    "    course_programs[i]['final_fail'] = np.where(course_programs[i]['final_mark'] > 5, 0, 1) #at risk in final grade   \n",
    "    \n",
    "    #deal with gifted classification\n",
    "    course_programs[i]['final_gifted'] = np.where(course_programs[i]['final_mark'] >= 8.5, 1, 0) #gifted students according to final grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_programs['Date_threshold_100'].describe(include = 'all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking for Group (Course) Aggregates\n",
    "\n",
    "We will look at the 100% duration logs in order to make most of our verification. After all, the 100% duration threshold will be the basis for comparison.\n",
    "\n",
    "We will, for now, use the following immediate criteria for exclusion:\n",
    "1. Median % of days with no interaction > 80%,\n",
    "2. To have 0 mean for at-risk students or gifted students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "aggregates = course_programs['Date_threshold_100'].groupby('course').agg({\n",
    "                                                                'userid' :  [('Number of users','nunique')],\n",
    "                                                                'Number of sessions' : ['min', 'mean', 'median', 'max'],\n",
    "                                                                'Clicks per day' : ['min', 'mean', 'median', 'max'],\n",
    "                                                                'Days with no interaction (%)' : ['min', 'mean', 'median', 'max'], \n",
    "                                                                'final_fail' : 'mean',\n",
    "                                                                'final_gifted' : 'mean',                                                                \n",
    "                                                                    })\n",
    "#show all\n",
    "with pd.option_context(\"display.max_rows\", None):\n",
    "    display(aggregates)\n",
    "    \n",
    "        #same for session features\n",
    "aggregates.columns = aggregates.columns.map(flattenHierarchicalCol)\n",
    "aggregates.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep rows that fulfill the criteria - median user with less than 80% days without interaction \n",
    "aggregates = aggregates[~((aggregates['Days with no interaction (%)_median'] > 85) | (aggregates['final_fail_mean'] == 0) | (aggregates['final_gifted_mean'] == 0))]\n",
    "courses_to_keep = aggregates['course'].unique()\n",
    "\n",
    "#filtering dataset for all course durations\n",
    "for i in tqdm(course_programs):\n",
    "    course_programs[i] = course_programs[i][course_programs[i]['course'].isin(courses_to_keep)].reset_index(drop = True)\n",
    "\n",
    "course_programs['Date_threshold_100'].describe(include = 'all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_programs['Date_threshold_100'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizar features\n",
    "targets = ['final_fail', 'final_gifted']\n",
    "\n",
    "numeric_feat = [ 'Number of clicks', 'Number of sessions',\n",
    "       'Largest period of inactivity (h)', 'Total time online (min)',\n",
    "       'Average session duration (min)', 'Start of Session 1 (%)',\n",
    "       'Start of Session 2 (%)', 'Start of Session 3 (%)',\n",
    "       'Start of Session 4 (%)', 'Start of Session 5 (%)',\n",
    "       'Start of Session 6 (%)', 'Start of Session 7 (%)',\n",
    "       'Start of Session 8 (%)', 'Start of Session 9 (%)',\n",
    "       'Start of Session 10 (%)', 'Days with no interaction',\n",
    "       'Clicks on forum', 'Links viewed', 'Clicks on folder',\n",
    "       'Clicks on course', 'Resources viewed', 'Discussions viewed',\n",
    "       'Quizzes started', 'Files downloaded', 'Assignments submitted',\n",
    "       'Assignments viewed', 'Forum posts', 'Number of days', 'Clicks per day',\n",
    "       'Clicks per session', 'Clicks (% of course total)',\n",
    "       'Submissions (% of course total)', 'Days with no interaction (%)',\n",
    "        'On/off campus click ratio', 'Clicks on campus']\n",
    "\n",
    "#optional features -> assignment grades\n",
    "#optional_feats =[]\n",
    "\n",
    "#binary_feat = []\n",
    "\n",
    "#categorical_feat = []\n",
    "\n",
    "#date = []\n",
    "\n",
    "drop_feat = ['final_mark', 'Average grade of assignments',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Data Quality Report for Continuous Features\n",
    "\n",
    "def DescribeContinuousFeatures(Continuous_Features, dataset):\n",
    "    Continuous_Head = ['Count', 'Missing Values (%)', 'Cardinality', 'Minimum', '1st Qrt.', 'Mean', 'Median', '3rd Qrt.', 'Maximum', 'Std. Dev.']\n",
    "    Continuous_Describe = pd.DataFrame(index=Continuous_Features, columns=Continuous_Head)\n",
    "    Continuous_Describe.index.name = 'Feature Name'\n",
    "    columns = dataset[Continuous_Features]\n",
    "\n",
    "#Total Number of Instances\n",
    "    Continuous_Describe[Continuous_Head[0]] = columns.count()\n",
    "\n",
    "#Percentage of instances that has Missing Values (sabendo à partida que nenhuma variável contínua tem missings)\n",
    "    Continuous_Describe[Continuous_Head[1]] = columns.isnull().sum() * 100 / len(dataset)\n",
    "    \n",
    "#Cardinality of each feature (cardinality measures the number of Distinct Values)\n",
    "    Continuous_Describe[Continuous_Head[2]] = columns.nunique()\n",
    "\n",
    "#Minimum Value\n",
    "    Continuous_Describe[Continuous_Head[3]] = columns.min()\n",
    "\n",
    "#1ST Quartile\n",
    "    Continuous_Describe[Continuous_Head[4]] = columns.quantile(0.25)\n",
    "\n",
    "#Mean\n",
    "    Continuous_Describe[Continuous_Head[5]] = round(columns.mean(), 2)\n",
    "\n",
    "#Median\n",
    "    Continuous_Describe[Continuous_Head[6]] = columns.median()\n",
    "\n",
    "#3rd Quartile\n",
    "    Continuous_Describe[Continuous_Head[7]] = columns.quantile(0.75)\n",
    "\n",
    "#Maximum Value\n",
    "    Continuous_Describe[Continuous_Head[8]] = columns.max()\n",
    "\n",
    "#Standard Deviation\n",
    "    Continuous_Describe[Continuous_Head[9]] = round(columns.std(),2)\n",
    "    \n",
    "    return Continuous_Describe\n",
    "\n",
    "for i in tqdm(course_programs):\n",
    "    print(f'{i}\\n\\n{DescribeContinuousFeatures(numeric_feat,course_programs[i])}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below may be commented for faster computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sns.set_theme(context='paper', style='whitegrid', font='Calibri', font_scale=2)\n",
    "\n",
    "# for i in tqdm(course_programs):\n",
    "    \n",
    "#     print(i)\n",
    "#     #perform a very simple pairplot\n",
    "#     g = sns.PairGrid(course_programs[i][numeric_feat], corner = True)\n",
    "    \n",
    "#     #frequency histogram on diagonal\n",
    "#     g.map_diag(sns.histplot, color = 'grey', stat = 'frequency', kde = True)\n",
    "    \n",
    "#     #regplot is a scatter with regression line\n",
    "#     g.map_offdiag(sns.regplot, fit_reg=True, x_jitter=.1, color = course_color,  marker = 'x')\n",
    "    \n",
    "#     # Layout\n",
    "#     plt.subplots_adjust(top=0.95)\n",
    "#     plt.title(\"Pairwise Relationship of Numerical Variables\", fontweight=\"bold\")\n",
    "    \n",
    "#     plt.savefig(f'../Images/R_Gonz_numerical_feats_{i}_pair.png', transparent=True, dpi=300)\n",
    "#     plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cor_heatmap(cor):\n",
    "#     plt.figure(figsize=(32,32))\n",
    "#     return sns.heatmap(data = cor, annot = True, cmap = standard_cmap, fmt='.1',\n",
    "#                 vmin = -1, vmax = 1,\n",
    "#                )\n",
    "    \n",
    "# sns.set_theme(context='paper', style='whitegrid', font='Calibri', rc={\"figure.figsize\":(32, 32)}, font_scale=2)\n",
    "\n",
    "# for i in tqdm(course_programs):\n",
    "#     print(i)\n",
    "#     cor_spearman = course_programs[i][numeric_feat].corr(method = 'spearman')\n",
    "#     g = cor_heatmap(cor_spearman)\n",
    "#     fig = g.get_figure()\n",
    "#     fig.savefig(f'../Images/R_gonz_numerical_feats_{i}_heat.png', transparent=True, dpi=300)\n",
    "#     plt.close(\"all\")\n",
    "    \n",
    "# del g, fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Immediate observations\n",
    "\n",
    "Very low correlations throughout most metric features, eith the exception of a couple.\n",
    "Still need to verify what to do concerning outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Call later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_table(dataset, target):\n",
    "\n",
    "    feature_df = dataset.copy()\n",
    "    \n",
    "    X = feature_df.drop(targets,axis=1)\n",
    "    y = feature_df[target]\n",
    "    num_feats=len(X.columns)\n",
    "    X_norm = StandardScaler().fit_transform(X)\n",
    "\n",
    "    #RFE\n",
    "    rfe_selector = RFE(estimator=DecisionTreeClassifier(), step=2)\n",
    "    rfe_selector.fit(X_norm, y)\n",
    "    rfe_support = rfe_selector.get_support()\n",
    "    rfe_feature = X.loc[:,rfe_support].columns.tolist()\n",
    "\n",
    "    #RFECV\n",
    "    rfecv_selector = RFECV(estimator=DecisionTreeClassifier(), step=1, cv=5, scoring='f1_weighted',\n",
    "                          min_features_to_select = 10)\n",
    "    rfecv_selector = rfecv_selector.fit(X_norm, y)\n",
    "    rfecv_support = rfecv_selector.support_\n",
    "    rfecv_feature = X.loc[:,rfecv_support].columns.tolist()\n",
    "\n",
    "    #Logistic regression\n",
    "    embeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l2\"), max_features=num_feats)\n",
    "    embeded_lr_selector.fit(X_norm, y)\n",
    "    embeded_lr_support = embeded_lr_selector.get_support()\n",
    "    embeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\n",
    "\n",
    "    #RandomForest \n",
    "    embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_feats)\n",
    "    embeded_rf_selector.fit(X, y)\n",
    "    embeded_rf_support = embeded_rf_selector.get_support()\n",
    "    embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\n",
    "\n",
    "\n",
    "    #LGBMClassifier\n",
    "    lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n",
    "                reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
    "    embeded_lgb_selector = SelectFromModel(lgbc, max_features=num_feats)\n",
    "    embeded_lgb_selector.fit(X, y)\n",
    "    embeded_lgb_support = embeded_lgb_selector.get_support()\n",
    "    embeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist()\n",
    "\n",
    "    #Lasso\n",
    "    reg = LassoCV(random_state=123) \n",
    "    reg.fit(X=X,y=y)\n",
    "    lasso_coef = pd.Series(reg.coef_,index = X.columns)\n",
    "\n",
    "    #Ridge\n",
    "    ridge = RidgeCV()\n",
    "    ridge.fit(X=X,y=y)\n",
    "    coef_ridge = pd.Series(ridge.coef_,index = X.columns)\n",
    "\n",
    "    #Elastic \n",
    "    elast = ElasticNetCV(cv=5, random_state=123)\n",
    "    elast.fit(X, y)\n",
    "    elast_coef = pd.Series(elast.coef_,index = X.columns)\n",
    "\n",
    "    # put all selection together\n",
    "    feature_selection_df = pd.DataFrame({'Feature':X.columns, 'RFE':rfe_support,'RFECV':rfecv_support, 'Logistics':embeded_lr_support,\n",
    "                                        'Random Forest':embeded_rf_support, 'LightGBM':embeded_lgb_support, 'Lasso':lasso_coef != 0, 'Ridge':coef_ridge != 0, 'Elastic':elast_coef != 0})\n",
    "    # count the selected times for each feature\n",
    "    feature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n",
    "    \n",
    "    # display the top 100\n",
    "    feature_selection_df = feature_selection_df.sort_values(['Total'] , ascending=False)\n",
    "    feature_selection_df.set_index('Feature',inplace=True)\n",
    "    feature_selection_df\n",
    "    \n",
    "    return feature_df,feature_selection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train, test, scaler):\n",
    "    \n",
    "    if scaler == 'MinMax':\n",
    "        pt = MinMaxScaler()\n",
    "    elif scaler == 'Standard':\n",
    "        pt = StandardScaler()\n",
    "    elif scaler == 'Robust':\n",
    "        pt = RobustScaler()\n",
    "    elif scaler == 'Quantile':\n",
    "        pt = QuantileTransformer()\n",
    "    else:\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "    \n",
    "    data_train = pt.fit_transform(train)\n",
    "    data_test = pt.transform(test)\n",
    "    # convert the array back to a dataframe\n",
    "    normalized_train = pd.DataFrame(data_train,columns=train.columns)\n",
    "    normalized_test = pd.DataFrame(data_test,columns=test.columns)\n",
    "        \n",
    "    return normalized_train, normalized_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TESTING & SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this section will hinge on defining the functions to use during model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run desired model\n",
    "def run_model(model_name, X, y):\n",
    "    \n",
    "    ###Baseline Classifier - most frequent class\n",
    "    if model_name == 'Baseline - Majority Class':\n",
    "        model = DummyClassifier(strategy=\"most_frequent\").fit(X, y)\n",
    "    if model_name == 'KNN':\n",
    "        #weights to attribute higher weight to closer neighbors; seems to improve score highly\n",
    "        model = KNeighborsClassifier(n_neighbors=10, weights='distance').fit(X, y) #\n",
    "    if model_name == 'LR':\n",
    "        model = LogisticRegression(tol=1e-05, solver='liblinear', penalty='l1', max_iter =200).fit(X, y) #\n",
    "    if model_name == 'NB': \n",
    "        model = GaussianNB().fit(X, y)\n",
    "    if model_name == 'BNB': #\n",
    "        model = BernoulliNB().fit(X, y)\n",
    "#     if model_name == 'MNB': #\n",
    "#         model = MultinomialNB().fit(X, y)\n",
    "    if model_name == 'MLP': #\n",
    "        model = MLPClassifier(alpha=0.01, hidden_layer_sizes = (20,20), activation = 'relu', solver = 'adam', learning_rate = 'adaptive', verbose = 0, learning_rate_init = 0.02).fit(X, y)\n",
    "    if model_name == 'CART DT':\n",
    "        model = DecisionTreeClassifier(criterion='gini', max_depth=10).fit(X, y)\n",
    "    if model_name == 'J48 DT':\n",
    "        model = DecisionTreeClassifier(criterion = \"entropy\", max_depth=10).fit(X, y)        \n",
    "    if model_name == 'SVM': #\n",
    "        model = svm.SVC(tol = 0.01, probability = True, gamma='scale', kernel='rbf', C = 1).fit(X, y)\n",
    "    \n",
    "    ###ENSEMBLES\n",
    "    if model_name == 'RF':\n",
    "        model = RandomForestClassifier(max_depth = 10, random_state = 15, n_estimators=500, min_samples_leaf = 3).fit(X, y) #max_features=6, #max_depth é super imp para reduzir overfitting! #min_samples_lead highly reduces overfitting!\n",
    "    if model_name == 'AdaBoost':\n",
    "        model = AdaBoostClassifier(n_estimators = 95, learning_rate = 0.8, random_state = 15).fit(X, y) #importante que GSearchCV em DT maximizado!\n",
    "    if model_name == 'GBoost':\n",
    "        model = GradientBoostingClassifier(n_estimators=175, learning_rate=0.1, random_state=15).fit(X, y)\n",
    "    if model_name == 'ExtraTree':\n",
    "        model = ExtraTreesClassifier(n_estimators=175, criterion='entropy', max_depth = 10, min_samples_split= 50).fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#averages scores of each run (for the present model) in each iteration of Repeated 10-fold CV that has been called\n",
    "def avg_score(method,X,y, model_name):\n",
    "    \n",
    "    f1micro_train = []\n",
    "    f1micro_val = []\n",
    "    precision_train = []\n",
    "    precision_val = []\n",
    "    recall_train = []\n",
    "    recall_val = []\n",
    "    timer = []\n",
    "    cm_holder = []\n",
    "    test_holder = []\n",
    "    auc_train = []\n",
    "    auc_val = []\n",
    "    features = []\n",
    "    averaged_confusion_matrix=None\n",
    "    \n",
    "    for train_index, val_index in method.split(X,y):\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        #perform feature selection\n",
    "        feature_df_dict,feature_selection_df_dict = feature_selection_table(data_prep_df.loc[X_train.index], k)\n",
    "        \n",
    "        #adjust train_val_total features and X_test features to the results of global feature selection\n",
    "        X_train = feature_df_dict[feature_selection_df_dict.loc[feature_selection_df_dict['Total'] >3].index.tolist()]\n",
    "        X_val = X_val.reindex(columns= X_train.columns)\n",
    "        \n",
    "        #append\n",
    "        features.append(X_train.columns)\n",
    "        \n",
    "        #scale with standard scaler\n",
    "        X_train, X_val = normalize(X_train, X_val,'Standard')\n",
    "        \n",
    "        #Oversampling with SMOTE\n",
    "        over = SMOTE()\n",
    "        X_train, y_train = over.fit_resample(X_train, y_train)\n",
    "        \n",
    "        begin = time.perf_counter()\n",
    "        model = run_model(model_name, X_train, y_train)\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        labels_train = model.predict(X_train)\n",
    "        labels_val = model.predict(X_val)\n",
    "        \n",
    "        f1micro_train.append(f1_score(y_train, labels_train, average='micro'))\n",
    "        f1micro_val.append(f1_score(y_val, labels_val, average='micro'))\n",
    "        \n",
    "        precision_train.append(precision_score(y_train, labels_train))\n",
    "        precision_val.append(precision_score(y_val, labels_val))\n",
    "        \n",
    "        recall_train.append(recall_score(y_train, labels_train))\n",
    "        recall_val.append(recall_score(y_val, labels_val))\n",
    "        \n",
    "        auc_train.append(roc_auc_score(y_train,model.predict_proba(X_train)[:,1]))\n",
    "        auc_val.append(roc_auc_score(y_val,model.predict_proba(X_val)[:,1]))\n",
    "        \n",
    "        timer.append(end-begin)\n",
    "        \n",
    "        cm_holder.append(confusion_matrix(y_val, labels_val))\n",
    "    \n",
    "    #scale with standard scaler\n",
    "    model = run_model(model_name, X_scaled,y_sampled)\n",
    "    labels_test = model.predict(X_test)\n",
    "    \n",
    "    f1micro_test = f1_score(y_test, labels_test, average='micro')\n",
    "    precision_test = precision_score(y_test, labels_test)\n",
    "    recall_test = recall_score(y_test, labels_test)\n",
    "    \n",
    "    #SVM does not allow probabilities\n",
    "    try:\n",
    "        auc_test = roc_auc_score(y_test,model.predict_proba(X_test)[:,1])\n",
    "        \n",
    "    except:\n",
    "        auc_test = np.nan\n",
    "    \n",
    "    print(f'Classification Report for {model_name}:\\nTest Data\\n{classification_report(y_test, labels_test)}\\n' + \n",
    "         f'Confusion Matrix:\\n {confusion_matrix(y_test, labels_test)}\\n')\n",
    "    \n",
    "    # calculate the average and the std for each measure (accuracy, time and number of iterations)\n",
    "    avg_time = round(np.mean(timer),3)\n",
    "    avg_f1_train = round(np.mean(f1micro_train),3)\n",
    "    avg_f1_val = round(np.mean(f1micro_val),3)\n",
    "    avg_f1_test = round(np.mean(f1micro_test),3)\n",
    "    avg_precision_train = round(np.mean(precision_train),3)\n",
    "    avg_precision_val = round(np.mean(precision_val),3)\n",
    "    avg_precision_test = round(precision_test,3)\n",
    "    avg_recall_train = round(np.mean(recall_train),3)\n",
    "    avg_recall_val = round(np.mean(recall_val),3)\n",
    "    avg_recall_test = round(recall_test,3)\n",
    "    avg_auc_train = round(np.mean(auc_train),3)\n",
    "    avg_auc_val = round(np.mean(auc_val),3)\n",
    "    avg_auc_test = round(auc_test,3)\n",
    "    \n",
    "    \n",
    "    std_time = round(np.std(timer),3)\n",
    "    std_f1_train = round(np.std(f1micro_train),3)\n",
    "    std_f1_val = round(np.std(f1micro_test),3)\n",
    "    std_precision_train = round(np.std(precision_train),3)\n",
    "    std_precision_val = round(np.std(precision_val),3)\n",
    "    std_recall_train = round(np.std(recall_train),3)\n",
    "    std_recall_val = round(np.std(recall_val),3)\n",
    "    std_auc_train = round(np.std(auc_train),3)\n",
    "    std_auc_val = round(np.std(auc_val),3)\n",
    "    \n",
    "    #from sklearn.metrics import cohen_kappa_score\n",
    "    \n",
    "    return str(avg_time) + '+/-' + str(std_time), str(avg_f1_train) + '+/-' + str(std_f1_train), str(avg_f1_val) + '+/-' + str(std_f1_val), str(avg_f1_test), str(avg_precision_train) + '+/-' + str(std_precision_train), str(avg_precision_val) + '+/-' + str(std_precision_val), str(avg_precision_test), str(avg_recall_train) + '+/-' + str(std_recall_train), str(avg_recall_val) + '+/-' + str(std_recall_val), str(avg_recall_test), str(avg_auc_train)+ '+/-' + str(std_auc_train), str(avg_auc_val)+ '+/-' + str(std_auc_val), str(avg_auc_test), features, model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_bar(models, f1micro_train, f1micro_val, f1micro_test, path, date, target):\n",
    "    \n",
    "    sns.set_theme(context='paper', style='whitegrid', font='Calibri', font_scale=2)\n",
    "    \n",
    "    #Creates a figure and a set of subplots\n",
    "    fig, ax = plt.subplots(figsize = (20, 12))\n",
    "\n",
    "    #set width of bar\n",
    "    barwidth = 0.3\n",
    "\n",
    "    #set position of bar on X axis\n",
    "    pos_train = np.arange(len(f1micro_test))\n",
    "    pos_val = np.arange(len(f1micro_test))+0.3\n",
    "    pos_test = np.arange(len(f1micro_test))+0.6\n",
    "    \n",
    "    #convert to number\n",
    "    f1micro_train = [float(i.split('+')[0]) for i in f1micro_train]\n",
    "    f1micro_val = [float(i.split('+')[0]) for i in f1micro_val]\n",
    "    f1micro_test = [float(i.split('+')[0]) for i in f1micro_test]\n",
    "    \n",
    "    #makes the plot\n",
    "    plt.bar(pos_train, f1micro_train, color= nova_ims_colors[0], width=barwidth, edgecolor='white', label='Train')\n",
    "    plt.bar(pos_val, f1micro_val, color=course_color, width=barwidth, edgecolor='white', label='Validation')\n",
    "    plt.bar(pos_test, f1micro_test, color=student_color, width=barwidth, edgecolor='white', label='Test')\n",
    "    \n",
    "    #sets x, y labels\n",
    "    ax.set(xlabel = 'Model', ylabel = 'Accuracy')\n",
    "\n",
    "    #sets x ticks locations and designation\n",
    "    ax.set_xticks((pos_train+pos_val+pos_test)/3)\n",
    "    ax.set_xticklabels(models, rotation='vertical')\n",
    "    \n",
    "    #ads title to the plot\n",
    "    plt.title(f'10-fold Repeated Cross-Validation Results\\nData: {date}, Target:{target}', fontweight=\"bold\")\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    #removes box to make the plot prettier\n",
    "    plt.box(on=None)\n",
    "\n",
    "    #Creates (pretty) legend\n",
    "    plt.legend(frameon=False)\n",
    "    \n",
    "    plt.savefig(path, transparent=True, dpi=300)\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_roc_pr(models, X, y, path_roc, path_pr, date, target):  \n",
    "    \n",
    "    sns.set_theme(context='paper', style='whitegrid', font='Calibri', font_scale=2)\n",
    "    # Below for loop iterates through your models list\n",
    "    for m in models:\n",
    "        model = m['model']\n",
    "        y_pred=model.predict(X) # predict the test data\n",
    "    #Compute False postive rate, and True positive rate\n",
    "        fpr, tpr, _ = roc_curve(y, model.predict_proba(X)[:,1])\n",
    "    #Calculate AUC\n",
    "        auc = roc_auc_score(y,model.predict_proba(X)[:,1])\n",
    "    #Plot\n",
    "        plt.plot(fpr, tpr, label='%s ROC (area = %0.4f)' % (m['label'], auc))\n",
    "    #Makes it pretty!\n",
    "    #plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Specificity (False Positive Rate)', fontweight = 'bold')\n",
    "    plt.ylabel('Sensitivity (True Positive Rate)', fontweight = 'bold')\n",
    "    plt.title(f'ROC Curve Test\\nData: {date}, Target:{target}', fontweight = 'bold')\n",
    "    plt.legend(loc=\"lower right\", frameon=False)\n",
    "    #save fig\n",
    "    plt.savefig(path_roc, transparent=True, dpi=300)\n",
    "    plt.close(\"all\")\n",
    "\n",
    "    \n",
    "    # Below for loop iterates through your models list\n",
    "    for m in models:\n",
    "        model = m['model']\n",
    "        y_pred=model.predict(X) # predict the test data\n",
    "    #Compute Precision and Recall\n",
    "        precision, recall, _ = precision_recall_curve(y, model.predict_proba(X)[:,1])\n",
    "    #Calculate AP\n",
    "        ap = average_precision_score(y, model.predict_proba(X)[:,1])\n",
    "    #Plot\n",
    "        plt.plot(recall, precision, label='%s AP (area = %0.4f)' % (m['label'], ap))\n",
    "    #Makes it pretty!\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall (Positive Predictive Value)', fontweight=\"bold\")\n",
    "    plt.ylabel('Precision (True Positive Rate)', fontweight=\"bold\")\n",
    "    plt.title(f'Precision-Recall Curve Test\\nData: {date}, Target:{target}', fontweight=\"bold\")\n",
    "    plt.legend(loc=\"lower left\", frameon=False)\n",
    "    \n",
    "    #save fig\n",
    "    plt.savefig(path_pr, transparent=True, dpi=300)\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One more thing to do is to give a proper name to each plot label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_names = {\n",
    "             'Date_threshold_10': '10% of Course Duration',   \n",
    "             'Date_threshold_25': '25% of Course Duration', \n",
    "             'Date_threshold_33': '33% of Course Duration', \n",
    "             'Date_threshold_50': '50% of Course Duration', \n",
    "             'Date_threshold_100':'100% of Course Duration', \n",
    "            }\n",
    "\n",
    "target_names = {\n",
    "                'exam_fail' : 'At risk - Exam Grade',\n",
    "                'final_fail' : 'At risk - Final Grade', \n",
    "                'exam_gifted' : 'High performer - Exam Grade', \n",
    "                'final_gifted': 'High performer - Final Grade'\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we try and arrange everything in order to train the models we get for every combination of Date Thresholds and Targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating main dicts\n",
    "#feature selection \n",
    "feature_df_dict = {}\n",
    "feature_selection_df_dict = {}\n",
    "feature_columns = {}\n",
    "\n",
    "#result storage\n",
    "results_normal_models = {}\n",
    "\n",
    "for i in tqdm(list(course_programs.keys())[1:]):\n",
    "    print(i)\n",
    "    \n",
    "    #create subordinate dicts\n",
    "    feature_df_dict[f'{(i)}'] = {}\n",
    "    feature_selection_df_dict[f'{(i)}'] = {}\n",
    "    results_normal_models[i] = {}\n",
    "    feature_columns[i] = {}\n",
    "    #prepare data for split\n",
    "    data_prep_df = course_programs[i].drop(columns = drop_feat).set_index(['course', 'userid'])\n",
    "    #for now, fill nans with 0\n",
    "    data_prep_df = data_prep_df.fillna(0)\n",
    "    \n",
    "    #set x and y\n",
    "    X = data_prep_df.drop(columns = targets)\n",
    "    y = data_prep_df[targets]\n",
    "\n",
    "    #first, we create dict where we will store results for every date_threshold and targe\n",
    "    #now, we start working with each instance of target\n",
    "    for k in tqdm(targets):\n",
    "        print(k)\n",
    "        \n",
    "        #start with train test split\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(X,\n",
    "                                                  y[k],\n",
    "                                                  test_size = 0.20,\n",
    "                                                  random_state = 15,\n",
    "                                                  shuffle=True,\n",
    "                                                  stratify=y)\n",
    "        \n",
    "        #feature selection first\n",
    "        feature_df_dict[i][k],feature_selection_df_dict[i][k] = feature_selection_table(data_prep_df.loc[X_train_val.index], k)\n",
    "        \n",
    "        #adjust train_val_total features and X_test features to the results of global feature selection\n",
    "        X_train_val_total = feature_df_dict[i][k][feature_selection_df_dict[i][k].loc[feature_selection_df_dict[i][k]['Total'] >3].index.tolist()]\n",
    "        X_test = X_test.reindex(columns= X_train_val_total.columns)\n",
    "        \n",
    "        #scaling to use onlyinside function - not on cv\n",
    "        X_scaled, X_test = normalize(X_train_val_total, X_test, 'Standard')\n",
    "        \n",
    "        #Oversampling with SMOTE\n",
    "        overs = SMOTE()\n",
    "        X_scaled, y_sampled = overs.fit_resample(X_scaled, y_train_val)\n",
    "        \n",
    "        #runs each Repeated (10) 10-fold Cross-Validation in all tested models by calling function avg_score and prints resutls with multiple metrics for each\n",
    "        method_cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=replicas, random_state = 15)\n",
    "        \n",
    "        results_Dummy = avg_score(method_cv, X_train_val, y_train_val, 'Baseline - Majority Class')\n",
    "        results_KNN = avg_score(method_cv, X_train_val, y_train_val, 'KNN')\n",
    "        results_LR = avg_score(method_cv, X_train_val, y_train_val, 'LR')\n",
    "        results_NB = avg_score(method_cv, X_train_val, y_train_val, 'NB')\n",
    "        results_BNB = avg_score(method_cv, X_train_val, y_train_val, 'BNB')\n",
    "        results_NN = avg_score(method_cv, X_train_val, y_train_val, 'MLP')\n",
    "        results_CART_DT = avg_score(method_cv, X_train_val, y_train_val, 'CART DT')\n",
    "        results_J48_DT = avg_score(method_cv, X_train_val, y_train_val, 'J48 DT')\n",
    "        resultsSVM = avg_score(method_cv, X_train_val, y_train_val, 'SVM')\n",
    "        results_RF = avg_score(method_cv, X_train_val, y_train_val, 'RF')\n",
    "        results_AdaBoost = avg_score(method_cv, X_train_val, y_train_val, 'AdaBoost')\n",
    "        results_GBoost = avg_score(method_cv, X_train_val, y_train_val, 'GBoost')\n",
    "        results_Xtra = avg_score(method_cv, X_train_val, y_train_val, 'ExtraTree')\n",
    "\n",
    "        results_models = [results_Dummy, results_KNN, results_LR, results_NB, results_BNB, results_NN, results_CART_DT, results_J48_DT, resultsSVM,\n",
    "                            results_RF, results_AdaBoost, results_GBoost, results_Xtra]\n",
    "\n",
    "        f1micro_train = []\n",
    "        f1micro_val = []\n",
    "        f1micro_test = []\n",
    "\n",
    "        precision_train = []\n",
    "        precision_val = []\n",
    "        precision_test = []\n",
    "\n",
    "        recall_train = []\n",
    "        recall_val = []\n",
    "        recall_test = []\n",
    "\n",
    "        auc_train = []\n",
    "        auc_val = []\n",
    "        auc_test = []\n",
    "        \n",
    "        times = []\n",
    "        feature_columns[i][k] = []\n",
    "        \n",
    "        #organizes data for futher plotting\n",
    "        for j, model in enumerate(results_models):\n",
    "            feature_columns[i][k].append(f'{results_models[j][14]}: {results_models[j][13]}',)\n",
    "            f1micro_train.append(results_models[j][1])\n",
    "            f1micro_val.append(results_models[j][2])\n",
    "            f1micro_test.append(results_models[j][3])\n",
    "    \n",
    "            precision_train.append(results_models[j][4])\n",
    "            precision_val.append(results_models[j][5])\n",
    "            precision_test.append(results_models[j][6])\n",
    "    \n",
    "            recall_train.append(results_models[j][7])\n",
    "            recall_val.append(results_models[j][8])\n",
    "            recall_test.append(results_models[j][9])\n",
    "            \n",
    "            auc_train.append(results_models[j][10])\n",
    "            auc_val.append(results_models[j][11])\n",
    "            auc_test.append(results_models[j][12])\n",
    "            \n",
    "            times.append(results_models[j][0])\n",
    "            \n",
    "        result = [f1micro_train, f1micro_val, f1micro_test, precision_train, precision_val, precision_test, recall_train, recall_val, recall_test, auc_train, auc_val, auc_test, times]\n",
    "\n",
    "        results_normal_models[i][k] = pd.DataFrame(result, index = ['Train Accuracy (F1 Score micro)', 'Validation Accuracy (F1 Score micro)', 'Test Accuracy (F1 Score micro)', 'Precision Train', 'Precision Validation', 'Precision Test', 'Recall Train', 'Recall Validation', 'Recall Test', \n",
    "                                                                    'AUC - Train', 'AUC - Validation', 'AUC - Test', 'Time'],  \n",
    "                                                   columns = ['Baseline - Majority Class', 'KNN', 'LR', 'NB', 'BNB', 'NN', 'CART DT', 'J48 DT', 'SVM', 'RF', 'AdaBoost', 'GBoost', 'ExtraTree'])\n",
    "            \n",
    "       \n",
    "        #setting up paths for plots\n",
    "        path_bar = f'../Images/R_Gonz/Non temporal models/{i}/normal_no_assign_results_bar_{k}_SMOTE.png'\n",
    "        path_pr = f'../Images/R_Gonz/Non temporal models/{i}/normal_no_assign_precision_recall_{k}_SMOTE.png'\n",
    "        path_roc = f'../Images/R_Gonz/Non temporal models/{i}/normal_no_assign_roc_{k}_SMOTE.png'\n",
    "        \n",
    "        #get roc and precision recall curves\n",
    "        # Add models to list of models to incorporte in ROC curve\n",
    "        models = [{'label': 'Majority Class', 'model': run_model('Baseline - Majority Class', X_scaled, y_sampled),},\n",
    "            {'label': 'KNN', 'model': run_model('KNN', X_scaled, y_sampled),},\n",
    "          {'label': 'LR', 'model': run_model('LR', X_scaled, y_sampled),},\n",
    "          {'label': 'NB','model': run_model('NB', X_scaled, y_sampled),},\n",
    "          {'label': 'BNB', 'model': run_model('BNB', X_scaled, y_sampled),},\n",
    "          {'label': 'MLP', 'model': run_model('MLP', X_scaled, y_sampled),},\n",
    "          {'label': 'CART DT', 'model': run_model('CART DT', X_scaled, y_sampled),},\n",
    "          {'label': 'J48 DT', 'model': run_model('J48 DT', X_scaled, y_sampled),},\n",
    "          {'label': 'RF', 'model': run_model('RF', X_scaled, y_sampled),},\n",
    "          {'label': 'AdaBoost', 'model': run_model('AdaBoost', X_scaled, y_sampled),},\n",
    "          {'label': 'GBoost', 'model': run_model('GBoost', X_scaled, y_sampled),},\n",
    "          {'label': 'ExtraTree', 'model': run_model('ExtraTree', X_scaled, y_sampled),}] \n",
    "            #SVM not included coz there's no probs!\n",
    "    \n",
    "        plot_roc_pr(models, X_test, y_test, path_roc, path_pr, date_names[i], target_names[k])\n",
    "        \n",
    "    # from pandas.io.parsers import ExcelWriter\n",
    "    with pd.ExcelWriter(f\"../Data/Modeling Stage/Results/R_Gonz/Non temporal models/simple_models_no_assign_results_{i}_{replicas}_replicas_SMOTE.xlsx\") as writer:  \n",
    "    #saving file for setor comercial\n",
    "    \n",
    "        for sheet in targets:\n",
    "            results_normal_models[i][sheet].to_excel(writer, sheet_name=str(sheet))\n",
    "            \n",
    "    #saving feature selection    \n",
    "    with pd.ExcelWriter(f\"../Data/Modeling Stage/Feature Selection/R_Gonz_no_assign_feature_selection_{i}_no_test_SMOTE.xlsx\") as writer:  \n",
    "        #saving file for setor comercial\n",
    "    \n",
    "        for sheet in targets:\n",
    "            feature_selection_df_dict[i][sheet].to_excel(writer, sheet_name=str(sheet))\n",
    "        \n",
    "    #saving feature selection    \n",
    "    pd.DataFrame(feature_columns[i]).to_excel(f\"../Data/Modeling Stage/Feature Selection/R_Gonz_no_assign_feature_selection_{i}_cross_val_SMOTE.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
