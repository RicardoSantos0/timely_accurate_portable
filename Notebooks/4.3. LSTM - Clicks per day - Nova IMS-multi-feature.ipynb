{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84eda90e",
   "metadata": {},
   "source": [
    "### Thesis notebook 4.3. - NOVA IMS\n",
    "\n",
    "#### LSTM - Temporal data representation\n",
    "\n",
    "In this notebook, we will finally start our application of temporal representation using LSTMs.\n",
    "The argument for the usage of Deep Learning stems from the fact that sequences themselves encode information that can be extracted using Recurrent Neural Networks and, more specifically, Long Short Term Memory Units.\n",
    "\n",
    "#### First Step: Setup a PyTorch environment that enables the use of GPU for training. \n",
    "\n",
    "The following cell wll confirm that the GPU will be the default device to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f27844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "cuda.init()\n",
    "## Get Id of default device\n",
    "torch.cuda.current_device()\n",
    "# 0\n",
    "cuda.Device(0).name() # '0' is the id of your GPU\n",
    "\n",
    "#set all tensors to gpu\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95429e",
   "metadata": {},
   "source": [
    "#### Second Step: Import the relevant packages and declare global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c2d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules/libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "#tqdm to monitor progress\n",
    "from tqdm.notebook import tqdm, trange\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "#time related features\n",
    "from datetime import timedelta\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "#vizualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#imblearn, scalers, kfold and metrics \n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer,PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, recall_score, classification_report, average_precision_score, precision_recall_curve\n",
    "\n",
    "#import torch related\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "#and optimizer of learning rate\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "#import pytorch modules\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c3f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables that may come in handy\n",
    "#course threshold sets the % duration that will be considered (1 = 100%)\n",
    "duration_threshold = [0.1, 0.25, 0.33, 0.5, 1]\n",
    "\n",
    "#colors for vizualizations\n",
    "nova_ims_colors = ['#BFD72F', '#5C666C']\n",
    "\n",
    "#standard color for student aggregates\n",
    "student_color = '#474838'\n",
    "\n",
    "#standard color for course aggragates\n",
    "course_color = '#1B3D2F'\n",
    "\n",
    "#standard continuous colormap\n",
    "standard_cmap = 'viridis_r'\n",
    "\n",
    "#Function designed to deal with multiindex and flatten it\n",
    "def flattenHierarchicalCol(col,sep = '_'):\n",
    "    '''converts multiindex columns into single index columns while retaining the hierarchical components'''\n",
    "    if not type(col) is tuple:\n",
    "        return col\n",
    "    else:\n",
    "        new_col = ''\n",
    "        for leveli,level in enumerate(col):\n",
    "            if not level == '':\n",
    "                if not leveli == 0:\n",
    "                    new_col += sep\n",
    "                new_col += level\n",
    "        return new_col\n",
    "    \n",
    "#number of replicas - number of repeats of stratified k fold - in this case 10\n",
    "replicas = 1\n",
    "\n",
    "#names to display on result figures\n",
    "date_names = {\n",
    "             'Date_threshold_10': '10% of Course Duration',   \n",
    "             'Date_threshold_25': '25% of Course Duration', \n",
    "             'Date_threshold_33': '33% of Course Duration', \n",
    "             'Date_threshold_50': '50% of Course Duration', \n",
    "             'Date_threshold_100':'100% of Course Duration', \n",
    "            }\n",
    "\n",
    "target_names = {\n",
    "                'exam_fail' : 'At risk - Exam Grade',\n",
    "                'final_fail' : 'At risk - Final Grade', \n",
    "                'exam_gifted' : 'High performer - Exam Grade', \n",
    "                'final_gifted': 'High performer - Final Grade'\n",
    "                }\n",
    "\n",
    "#targets\n",
    "targets = ['exam_fail', 'exam_gifted']\n",
    "\n",
    "#set the indexes to use for later\n",
    "index = [\"course_encoding\", \"cd_curso\", \"semestre\", \"courseid\", \"userid\", 'exam_gifted', 'exam_fail']\n",
    "\n",
    "#categories of objecctables\n",
    "objects = [\"course\", \"resource\", \"forum\", \"url\", \"folder\", \"quiz\", \"grade_grades\", \n",
    "           \"assignments\", \"groups\", \"user\", \"turnitintooltwo\", \"page\", \"choice\", \"other\"]          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c5ddb6",
   "metadata": {},
   "source": [
    "#### Step 3: Import data and take a preliminary look at it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8a23ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports dataframes\n",
    "course_programs = pd.read_excel(\"../Data/Modeling Stage/Nova_IMS_Temporal_Datasets_daily_clicks.xlsx\", \n",
    "                                dtype = {\n",
    "                                    'course_encoding' : int,\n",
    "                                    'userid' : int},\n",
    "                               sheet_name = None)\n",
    "\n",
    "#save tables \n",
    "student_list = pd.read_csv('../Data/Modeling Stage/Nova_IMS_Filtered_targets.csv', \n",
    "                         dtype = {\n",
    "                                   'course_encoding': int,\n",
    "                                   'userid' : int,\n",
    "                                   })\n",
    "\n",
    "#drop unnamed 0 column\n",
    "for i in course_programs:\n",
    "        \n",
    "    #merge with the targets we calculated on the other \n",
    "    course_programs[i] = course_programs[i].merge(student_list, on = ['course_encoding', 'userid'], how = 'inner')\n",
    "    course_programs[i].drop(['Unnamed: 0', 'exam_mark', 'final_mark'], axis = 1, inplace = True)\n",
    "    \n",
    "    #convert results to object and need to convert column names to string\n",
    "    course_programs[i]['course_encoding'], course_programs[i]['userid'] = course_programs[i]['course_encoding'].astype(object), course_programs[i]['userid'].astype(object)\n",
    "    course_programs[i].columns = course_programs[i].columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc3c84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 63171 entries, 0 to 63170\n",
      "Columns: 149 entries, course_encoding to exam_gifted\n",
      "dtypes: float64(95), int64(50), object(4)\n",
      "memory usage: 72.3+ MB\n"
     ]
    }
   ],
   "source": [
    "course_programs['Date_threshold_100'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a751ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_encoding</th>\n",
       "      <th>cd_curso</th>\n",
       "      <th>semestre</th>\n",
       "      <th>courseid</th>\n",
       "      <th>userid</th>\n",
       "      <th>objecttable</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>exam_fail</th>\n",
       "      <th>exam_gifted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>63171.0</td>\n",
       "      <td>63171.000000</td>\n",
       "      <td>63171</td>\n",
       "      <td>63171.000000</td>\n",
       "      <td>63171.0</td>\n",
       "      <td>63171</td>\n",
       "      <td>63171.000000</td>\n",
       "      <td>63171.000000</td>\n",
       "      <td>63171.000000</td>\n",
       "      <td>63171.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>41654.000000</td>\n",
       "      <td>39423.000000</td>\n",
       "      <td>37318.000000</td>\n",
       "      <td>34876.000000</td>\n",
       "      <td>32366.000000</td>\n",
       "      <td>27005.000000</td>\n",
       "      <td>19821.000000</td>\n",
       "      <td>1054.000000</td>\n",
       "      <td>63171.000000</td>\n",
       "      <td>63171.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>138.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1590.0</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6826.0</td>\n",
       "      <td>course</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1821.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.0</td>\n",
       "      <td>9295</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7906.809375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>185361.922290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019803</td>\n",
       "      <td>0.028716</td>\n",
       "      <td>0.045638</td>\n",
       "      <td>0.052113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522903</td>\n",
       "      <td>0.293027</td>\n",
       "      <td>0.235329</td>\n",
       "      <td>0.867387</td>\n",
       "      <td>0.250695</td>\n",
       "      <td>0.844844</td>\n",
       "      <td>0.339690</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>0.190784</td>\n",
       "      <td>0.287331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1986.226115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80819.428212</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.276235</td>\n",
       "      <td>0.308394</td>\n",
       "      <td>0.458564</td>\n",
       "      <td>0.550856</td>\n",
       "      <td>...</td>\n",
       "      <td>4.502103</td>\n",
       "      <td>2.288393</td>\n",
       "      <td>1.885921</td>\n",
       "      <td>9.354308</td>\n",
       "      <td>1.631869</td>\n",
       "      <td>8.256497</td>\n",
       "      <td>3.054003</td>\n",
       "      <td>0.043540</td>\n",
       "      <td>0.392922</td>\n",
       "      <td>0.452521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>859.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100001.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7512.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100091.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9155.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200165.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9434.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200193.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9435.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400131.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>376.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 149 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        course_encoding      cd_curso semestre       courseid   userid  \\\n",
       "count           63171.0  63171.000000    63171   63171.000000  63171.0   \n",
       "unique            138.0           NaN        6            NaN   1590.0   \n",
       "top               150.0           NaN       S1            NaN   6826.0   \n",
       "freq             1821.0           NaN    28407            NaN     93.0   \n",
       "mean                NaN   7906.809375      NaN  185361.922290      NaN   \n",
       "std                 NaN   1986.226115      NaN   80819.428212      NaN   \n",
       "min                 NaN    859.000000      NaN  100001.000000      NaN   \n",
       "25%                 NaN   7512.000000      NaN  100091.000000      NaN   \n",
       "50%                 NaN   9155.000000      NaN  200165.000000      NaN   \n",
       "75%                 NaN   9434.000000      NaN  200193.000000      NaN   \n",
       "max                 NaN   9435.000000      NaN  400131.000000      NaN   \n",
       "\n",
       "       objecttable             1             2             3             4  \\\n",
       "count        63171  63171.000000  63171.000000  63171.000000  63171.000000   \n",
       "unique          14           NaN           NaN           NaN           NaN   \n",
       "top         course           NaN           NaN           NaN           NaN   \n",
       "freq          9295           NaN           NaN           NaN           NaN   \n",
       "mean           NaN      0.019803      0.028716      0.045638      0.052113   \n",
       "std            NaN      0.276235      0.308394      0.458564      0.550856   \n",
       "min            NaN      0.000000      0.000000      0.000000      0.000000   \n",
       "25%            NaN      0.000000      0.000000      0.000000      0.000000   \n",
       "50%            NaN      0.000000      0.000000      0.000000      0.000000   \n",
       "75%            NaN      0.000000      0.000000      0.000000      0.000000   \n",
       "max            NaN     19.000000     17.000000     20.000000     30.000000   \n",
       "\n",
       "        ...           134           135           136           137  \\\n",
       "count   ...  41654.000000  39423.000000  37318.000000  34876.000000   \n",
       "unique  ...           NaN           NaN           NaN           NaN   \n",
       "top     ...           NaN           NaN           NaN           NaN   \n",
       "freq    ...           NaN           NaN           NaN           NaN   \n",
       "mean    ...      0.522903      0.293027      0.235329      0.867387   \n",
       "std     ...      4.502103      2.288393      1.885921      9.354308   \n",
       "min     ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%     ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%     ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%     ...      0.000000      0.000000      0.000000      0.000000   \n",
       "max     ...    214.000000    118.000000    169.000000    376.000000   \n",
       "\n",
       "                 138           139           140          141     exam_fail  \\\n",
       "count   32366.000000  27005.000000  19821.000000  1054.000000  63171.000000   \n",
       "unique           NaN           NaN           NaN          NaN           NaN   \n",
       "top              NaN           NaN           NaN          NaN           NaN   \n",
       "freq             NaN           NaN           NaN          NaN           NaN   \n",
       "mean        0.250695      0.844844      0.339690     0.001898      0.190784   \n",
       "std         1.631869      8.256497      3.054003     0.043540      0.392922   \n",
       "min         0.000000      0.000000      0.000000     0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000     0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000     0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000     0.000000      0.000000   \n",
       "max        58.000000    230.000000     96.000000     1.000000      1.000000   \n",
       "\n",
       "         exam_gifted  \n",
       "count   63171.000000  \n",
       "unique           NaN  \n",
       "top              NaN  \n",
       "freq             NaN  \n",
       "mean        0.287331  \n",
       "std         0.452521  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         1.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[11 rows x 149 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_programs['Date_threshold_100'].describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3291817",
   "metadata": {},
   "source": [
    "In our first attempt, we will use the absolute number of clicks made by each student - scaled using standard scaler. \n",
    "Therefore, we can start by immediately placing our course encoding/userid pairings into the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be722ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train, test, scaler):\n",
    "    \n",
    "    if scaler == 'MinMax':\n",
    "        pt = MinMaxScaler()\n",
    "    elif scaler == 'Standard':\n",
    "        pt = StandardScaler()\n",
    "    elif scaler == 'Robust':\n",
    "        pt = RobustScaler()\n",
    "    elif scaler == 'Quantile':\n",
    "        pt = QuantileTransformer()\n",
    "    else:\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "    \n",
    "    data_train = pt.fit_transform(train)\n",
    "    data_test = pt.transform(test)\n",
    "    # convert the array back to a dataframe\n",
    "    normalized_train = pd.DataFrame(data_train,columns=train.columns)\n",
    "    normalized_test = pd.DataFrame(data_test,columns=test.columns)\n",
    "        \n",
    "    return normalized_train, normalized_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85843427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "course             9295\n",
       "resource           8757\n",
       "forum              7005\n",
       "url                6769\n",
       "folder             5758\n",
       "quiz               4870\n",
       "grade_grades       4670\n",
       "assignments        3858\n",
       "groups             3525\n",
       "user               2344\n",
       "turnitintooltwo    1998\n",
       "page               1728\n",
       "choice             1373\n",
       "other              1221\n",
       "Name: objecttable, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_programs['Date_threshold_100'].objecttable.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e92abd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = course_programs['Date_threshold_100'].copy()\n",
    "\n",
    "#The first 6 columns are index - column 141 is fully empty - no other df has more columns than df_100\n",
    "columns = test.drop(targets, axis = 1).columns[6:146]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f99ddf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create first pivot\n",
    "placeholder_pivot = pd.pivot_table(test, index = index, values = columns, columns = \"objecttable\",\n",
    "                  aggfunc = 'first')\n",
    "\n",
    "\n",
    "#applies the function that removes multiindex\n",
    "placeholder_pivot.columns = placeholder_pivot.columns.map(flattenHierarchicalCol)\n",
    "\n",
    "#also saving index for reindexing of the remaining stuff\n",
    "save_index = placeholder_pivot.index.copy()\n",
    "\n",
    "#we will need to create the tensors multidimensional tensors\n",
    "placeholder_dict = {}\n",
    "\n",
    "#create dataset for targets\n",
    "df_targets = placeholder_pivot.reset_index().copy()[index]\n",
    "df_targets.set_index([\"course_encoding\", \"cd_curso\", \"semestre\", \"courseid\", \"userid\"], inplace = True)\n",
    "\n",
    "#initialize empty 3d array\n",
    "nd_array_100 = np.zeros((\n",
    "                               len(objects), #nbr of dimensions\n",
    "                               len(placeholder_pivot), #nbr of rows\n",
    "                               len(columns), #nbr of columns \n",
    "                              ))\n",
    "\n",
    "#likely inefficient, but should do the trick\n",
    "counter = 0\n",
    "\n",
    "#create multiple dataframes based on regex - this will create ndarray for the 100 duration\n",
    "for i in objects:\n",
    "    #create the objects\n",
    "    placeholder_dict[f'{i}'] = placeholder_pivot.filter(regex=f'_{i}')\n",
    "    \n",
    "    #remove text, convert column name back to numbers and sort numbers to ensure sequence\n",
    "    placeholder_dict[f'{i}'].columns = placeholder_dict[f'{i}'].columns.str.replace(r\"\\D+\", \"\", regex=True) \n",
    "    placeholder_dict[f'{i}'].columns = placeholder_dict[f'{i}'].columns.astype(int)\n",
    "    placeholder_dict[f'{i}'] = placeholder_dict[f'{i}'][sorted(placeholder_dict[f'{i}'].columns)].fillna(0)\n",
    "    \n",
    "    #converting df to nd array\n",
    "    nd_array_100[counter] = placeholder_dict[f'{i}'].values\n",
    "    counter += 1\n",
    "\n",
    "    #reshape to samples, rows, columns\n",
    "\n",
    "#switching to rows, columns, features\n",
    "nd_array_100 = nd_array_100.transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01e3c9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9296, 140, 14)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd_array_100.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4d5475",
   "metadata": {},
   "source": [
    "#### Implementing Cross-Validation with Deep Learning Model\n",
    "\n",
    "**1. Create the Deep Learning Model**\n",
    "\n",
    "In this instance, we will follow-up with on the approach used in Chen & Cui - CrossEntropyLoss with applied over a softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a16bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Uni(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM_Uni, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first = True) #lstm\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "    \n",
    "        self.fc = nn.Linear(self.hidden_size, num_classes) #fully connected last layer\n",
    "\n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        \n",
    "        #Xavier_init for both H_0 and C_0\n",
    "        torch.nn.init.xavier_normal_(h_0)\n",
    "        torch.nn.init.xavier_normal_(c_0)\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        lstm_out, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        last_output = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        \n",
    "        #we are interested in only keeping the last output\n",
    "        drop_out = self.dropout(last_output)\n",
    "        pre_softmax = self.fc(drop_out) #Final Output - dense\n",
    "        return pre_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c356bd",
   "metadata": {},
   "source": [
    "**2. Define the train and validation Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25b29a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,dataloader,loss_fn,optimizer):\n",
    "    \n",
    "    train_loss,train_correct=0.0,0 \n",
    "    model.train()\n",
    "    for X, labels in dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = loss_fn(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * X.size(0)\n",
    "        scores, predictions = torch.max(F.log_softmax(output.data), 1)\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "    return train_loss,train_correct\n",
    "  \n",
    "def valid_epoch(model,dataloader,loss_fn):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    targets = []\n",
    "    y_pred = []\n",
    "    probability_1 = []\n",
    "    \n",
    "    model.eval()\n",
    "    for X, labels in dataloader:\n",
    "\n",
    "        output = model(X)\n",
    "        loss=loss_fn(output,labels)\n",
    "        valid_loss+=loss.item()*X.size(0)\n",
    "        probability_1.append(F.softmax(output.data)[:,1])\n",
    "        predictions = torch.argmax(output, dim=1)\n",
    "        val_correct+=(predictions == labels).sum().item()\n",
    "        targets.append(labels)\n",
    "        y_pred.append(predictions)\n",
    "    \n",
    "    #concat all results\n",
    "    targets = torch.cat(targets).data.cpu().numpy()\n",
    "    y_pred = torch.cat(y_pred).data.cpu().numpy()\n",
    "    probability_1 = torch.cat(probability_1).data.cpu().numpy()\n",
    "    \n",
    "    #calculate precision, recall and AUC score\n",
    "    \n",
    "    precision = precision_score(targets, y_pred)\n",
    "    recall = recall_score(targets, y_pred)\n",
    "    f1 = f1_score(targets, y_pred, average = 'micro')\n",
    "    auroc = roc_auc_score(targets, probability_1)\n",
    "    \n",
    "    #return all\n",
    "    return valid_loss,val_correct, precision, recall, auroc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4543fb3",
   "metadata": {},
   "source": [
    "**3. Define main hyperparameters of the model, including splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcbbef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "num_epochs = 200 #200 epochs\n",
    "learning_rate = 0.001 #0.01 lr\n",
    "input_size = 14 #number of features\n",
    "hidden_size = 128 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "#Shape of Output as required for SoftMax Classifier\n",
    "num_classes = 2 #output shape\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "k=5\n",
    "splits= StratifiedKFold(n_splits=k, random_state=15, shuffle = True) #kfold of 10 with 30 replicas\n",
    "criterion = nn.CrossEntropyLoss()    # cross-entropy for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2414283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f0588de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7c97d183714d359c4d8e68a8a004d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exam_fail\n",
      "Replica 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a1bf26b1754ed28dcddded442102ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1c4bb298ce4e3b84dad13bea229188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best Accuracy found: 79.84%\n",
      "Epoch: 1\n",
      "New Best Accuracy found: 79.91%\n",
      "Epoch: 6\n",
      "New Best Accuracy found: 80.11%\n",
      "Epoch: 9\n",
      "Epoch:10/200 AVG Training Loss:0.480 AVG Validation Loss:0.511 AVG Training Acc 80.28 % AVG Validation Acc 79.97 %\n",
      "Epoch:20/200 AVG Training Loss:0.444 AVG Validation Loss:0.552 AVG Training Acc 81.62 % AVG Validation Acc 79.64 %\n",
      "Epoch:30/200 AVG Training Loss:0.397 AVG Validation Loss:0.561 AVG Training Acc 83.49 % AVG Validation Acc 79.57 %\n",
      "Epoch:40/200 AVG Training Loss:0.362 AVG Validation Loss:0.536 AVG Training Acc 84.85 % AVG Validation Acc 79.70 %\n",
      "Epoch:50/200 AVG Training Loss:0.325 AVG Validation Loss:0.502 AVG Training Acc 86.08 % AVG Validation Acc 80.11 %\n",
      "Epoch:60/200 AVG Training Loss:0.286 AVG Validation Loss:0.569 AVG Training Acc 88.16 % AVG Validation Acc 79.23 %\n",
      "Epoch:70/200 AVG Training Loss:0.273 AVG Validation Loss:0.525 AVG Training Acc 88.21 % AVG Validation Acc 79.91 %\n",
      "Epoch:80/200 AVG Training Loss:0.230 AVG Validation Loss:0.645 AVG Training Acc 90.64 % AVG Validation Acc 79.91 %\n",
      "Epoch:90/200 AVG Training Loss:0.230 AVG Validation Loss:0.577 AVG Training Acc 90.32 % AVG Validation Acc 79.97 %\n",
      "Epoch:100/200 AVG Training Loss:0.180 AVG Validation Loss:0.930 AVG Training Acc 92.82 % AVG Validation Acc 79.91 %\n",
      "Epoch:110/200 AVG Training Loss:0.184 AVG Validation Loss:0.739 AVG Training Acc 92.18 % AVG Validation Acc 79.91 %\n",
      "Epoch:120/200 AVG Training Loss:0.138 AVG Validation Loss:0.989 AVG Training Acc 94.49 % AVG Validation Acc 79.91 %\n",
      "Epoch:130/200 AVG Training Loss:0.132 AVG Validation Loss:0.612 AVG Training Acc 94.65 % AVG Validation Acc 72.31 %\n",
      "Epoch:140/200 AVG Training Loss:0.116 AVG Validation Loss:0.599 AVG Training Acc 95.19 % AVG Validation Acc 79.77 %\n",
      "Epoch:150/200 AVG Training Loss:0.131 AVG Validation Loss:0.547 AVG Training Acc 94.97 % AVG Validation Acc 79.64 %\n",
      "Epoch:160/200 AVG Training Loss:0.115 AVG Validation Loss:0.623 AVG Training Acc 95.29 % AVG Validation Acc 79.64 %\n",
      "Epoch:170/200 AVG Training Loss:0.105 AVG Validation Loss:0.896 AVG Training Acc 96.03 % AVG Validation Acc 79.77 %\n",
      "Epoch:180/200 AVG Training Loss:0.086 AVG Validation Loss:0.623 AVG Training Acc 96.89 % AVG Validation Acc 79.77 %\n",
      "Epoch:190/200 AVG Training Loss:0.084 AVG Validation Loss:0.809 AVG Training Acc 97.09 % AVG Validation Acc 79.77 %\n",
      "Epoch:200/200 AVG Training Loss:0.070 AVG Validation Loss:0.641 AVG Training Acc 97.48 % AVG Validation Acc 79.57 %\n",
      "Split 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc2cb0341c347fe9f453d4329198de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.480 AVG Validation Loss:0.507 AVG Training Acc 80.52 % AVG Validation Acc 79.83 %\n",
      "Epoch:20/200 AVG Training Loss:0.439 AVG Validation Loss:0.514 AVG Training Acc 81.95 % AVG Validation Acc 79.56 %\n",
      "Epoch:30/200 AVG Training Loss:0.382 AVG Validation Loss:0.508 AVG Training Acc 83.90 % AVG Validation Acc 79.69 %\n",
      "Epoch:40/200 AVG Training Loss:0.340 AVG Validation Loss:0.505 AVG Training Acc 85.41 % AVG Validation Acc 79.62 %\n",
      "Epoch:50/200 AVG Training Loss:0.312 AVG Validation Loss:0.511 AVG Training Acc 86.12 % AVG Validation Acc 79.62 %\n",
      "Epoch:60/200 AVG Training Loss:0.307 AVG Validation Loss:0.517 AVG Training Acc 86.89 % AVG Validation Acc 79.42 %\n",
      "Epoch:70/200 AVG Training Loss:0.242 AVG Validation Loss:0.762 AVG Training Acc 89.12 % AVG Validation Acc 25.22 %\n",
      "Epoch:80/200 AVG Training Loss:0.206 AVG Validation Loss:1.380 AVG Training Acc 90.54 % AVG Validation Acc 20.31 %\n",
      "Epoch:90/200 AVG Training Loss:0.197 AVG Validation Loss:0.610 AVG Training Acc 91.31 % AVG Validation Acc 74.78 %\n",
      "Epoch:100/200 AVG Training Loss:0.195 AVG Validation Loss:1.002 AVG Training Acc 91.36 % AVG Validation Acc 20.51 %\n",
      "Epoch:110/200 AVG Training Loss:0.179 AVG Validation Loss:1.655 AVG Training Acc 92.15 % AVG Validation Acc 20.44 %\n",
      "Epoch:120/200 AVG Training Loss:0.224 AVG Validation Loss:3.447 AVG Training Acc 90.38 % AVG Validation Acc 20.31 %\n",
      "Epoch:130/200 AVG Training Loss:0.141 AVG Validation Loss:1.534 AVG Training Acc 93.60 % AVG Validation Acc 20.44 %\n",
      "Epoch:140/200 AVG Training Loss:0.153 AVG Validation Loss:1.350 AVG Training Acc 93.41 % AVG Validation Acc 20.98 %\n",
      "Epoch:150/200 AVG Training Loss:0.120 AVG Validation Loss:0.741 AVG Training Acc 94.77 % AVG Validation Acc 30.67 %\n",
      "Epoch:160/200 AVG Training Loss:0.105 AVG Validation Loss:1.049 AVG Training Acc 95.51 % AVG Validation Acc 22.13 %\n",
      "Epoch:170/200 AVG Training Loss:0.113 AVG Validation Loss:0.665 AVG Training Acc 94.97 % AVG Validation Acc 77.67 %\n",
      "Epoch:180/200 AVG Training Loss:0.104 AVG Validation Loss:0.993 AVG Training Acc 95.58 % AVG Validation Acc 23.40 %\n",
      "Epoch:190/200 AVG Training Loss:0.095 AVG Validation Loss:0.627 AVG Training Acc 95.81 % AVG Validation Acc 79.62 %\n",
      "Epoch:200/200 AVG Training Loss:0.128 AVG Validation Loss:1.364 AVG Training Acc 94.49 % AVG Validation Acc 21.05 %\n",
      "Split 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e689af20ddb4413a356fc3eb7ddc3e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.482 AVG Validation Loss:0.632 AVG Training Acc 80.28 % AVG Validation Acc 79.29 %\n",
      "Epoch:20/200 AVG Training Loss:0.459 AVG Validation Loss:0.724 AVG Training Acc 81.36 % AVG Validation Acc 27.17 %\n",
      "Epoch:30/200 AVG Training Loss:0.408 AVG Validation Loss:0.842 AVG Training Acc 83.24 % AVG Validation Acc 20.51 %\n",
      "Epoch:40/200 AVG Training Loss:0.357 AVG Validation Loss:0.757 AVG Training Acc 84.72 % AVG Validation Acc 25.62 %\n",
      "Epoch:50/200 AVG Training Loss:0.308 AVG Validation Loss:1.255 AVG Training Acc 86.49 % AVG Validation Acc 20.24 %\n",
      "Epoch:60/200 AVG Training Loss:0.311 AVG Validation Loss:2.085 AVG Training Acc 86.33 % AVG Validation Acc 20.11 %\n",
      "Epoch:70/200 AVG Training Loss:0.243 AVG Validation Loss:0.773 AVG Training Acc 89.31 % AVG Validation Acc 28.18 %\n",
      "Epoch:80/200 AVG Training Loss:0.227 AVG Validation Loss:0.832 AVG Training Acc 89.61 % AVG Validation Acc 20.58 %\n",
      "Epoch:90/200 AVG Training Loss:0.194 AVG Validation Loss:0.512 AVG Training Acc 90.77 % AVG Validation Acc 79.69 %\n",
      "Epoch:100/200 AVG Training Loss:0.175 AVG Validation Loss:1.111 AVG Training Acc 92.10 % AVG Validation Acc 20.65 %\n",
      "Epoch:110/200 AVG Training Loss:0.148 AVG Validation Loss:0.531 AVG Training Acc 93.60 % AVG Validation Acc 79.62 %\n",
      "Epoch:120/200 AVG Training Loss:0.154 AVG Validation Loss:1.633 AVG Training Acc 93.60 % AVG Validation Acc 20.31 %\n",
      "Epoch:130/200 AVG Training Loss:0.165 AVG Validation Loss:1.469 AVG Training Acc 92.99 % AVG Validation Acc 20.17 %\n",
      "Epoch:140/200 AVG Training Loss:0.118 AVG Validation Loss:0.944 AVG Training Acc 94.99 % AVG Validation Acc 26.70 %\n",
      "Epoch:150/200 AVG Training Loss:0.103 AVG Validation Loss:1.654 AVG Training Acc 95.85 % AVG Validation Acc 20.17 %\n",
      "Epoch:160/200 AVG Training Loss:0.093 AVG Validation Loss:1.852 AVG Training Acc 96.29 % AVG Validation Acc 20.24 %\n",
      "Epoch:170/200 AVG Training Loss:0.114 AVG Validation Loss:3.527 AVG Training Acc 95.31 % AVG Validation Acc 20.11 %\n",
      "Epoch:180/200 AVG Training Loss:0.088 AVG Validation Loss:3.676 AVG Training Acc 96.49 % AVG Validation Acc 20.17 %\n",
      "Epoch:190/200 AVG Training Loss:0.063 AVG Validation Loss:4.909 AVG Training Acc 97.34 % AVG Validation Acc 20.17 %\n",
      "Epoch:200/200 AVG Training Loss:0.056 AVG Validation Loss:3.864 AVG Training Acc 97.90 % AVG Validation Acc 20.24 %\n",
      "Split 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcf99a301604ff3b650b5a9311132c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.478 AVG Validation Loss:0.751 AVG Training Acc 80.47 % AVG Validation Acc 29.99 %\n",
      "Epoch:20/200 AVG Training Loss:0.427 AVG Validation Loss:1.257 AVG Training Acc 81.98 % AVG Validation Acc 20.65 %\n",
      "Epoch:30/200 AVG Training Loss:0.385 AVG Validation Loss:1.894 AVG Training Acc 83.34 % AVG Validation Acc 20.11 %\n",
      "Epoch:40/200 AVG Training Loss:0.331 AVG Validation Loss:3.266 AVG Training Acc 86.01 % AVG Validation Acc 20.11 %\n",
      "Epoch:50/200 AVG Training Loss:0.297 AVG Validation Loss:3.175 AVG Training Acc 87.04 % AVG Validation Acc 20.11 %\n",
      "Epoch:60/200 AVG Training Loss:0.281 AVG Validation Loss:1.171 AVG Training Acc 87.49 % AVG Validation Acc 29.12 %\n",
      "Epoch:70/200 AVG Training Loss:0.207 AVG Validation Loss:1.863 AVG Training Acc 90.64 % AVG Validation Acc 20.31 %\n",
      "Epoch:80/200 AVG Training Loss:0.208 AVG Validation Loss:0.563 AVG Training Acc 90.55 % AVG Validation Acc 77.67 %\n",
      "Epoch:90/200 AVG Training Loss:0.180 AVG Validation Loss:1.099 AVG Training Acc 92.15 % AVG Validation Acc 29.79 %\n",
      "Epoch:100/200 AVG Training Loss:0.169 AVG Validation Loss:0.681 AVG Training Acc 92.32 % AVG Validation Acc 69.00 %\n",
      "Epoch:110/200 AVG Training Loss:0.119 AVG Validation Loss:3.034 AVG Training Acc 94.99 % AVG Validation Acc 20.11 %\n",
      "Epoch:120/200 AVG Training Loss:0.139 AVG Validation Loss:3.440 AVG Training Acc 94.47 % AVG Validation Acc 20.11 %\n",
      "Epoch:130/200 AVG Training Loss:0.190 AVG Validation Loss:6.030 AVG Training Acc 93.18 % AVG Validation Acc 20.11 %\n",
      "Epoch:140/200 AVG Training Loss:0.079 AVG Validation Loss:3.394 AVG Training Acc 97.14 % AVG Validation Acc 20.11 %\n",
      "Epoch:150/200 AVG Training Loss:0.142 AVG Validation Loss:4.393 AVG Training Acc 95.16 % AVG Validation Acc 20.11 %\n",
      "Epoch:160/200 AVG Training Loss:0.077 AVG Validation Loss:3.480 AVG Training Acc 97.50 % AVG Validation Acc 20.11 %\n",
      "Epoch:170/200 AVG Training Loss:0.069 AVG Validation Loss:5.752 AVG Training Acc 97.76 % AVG Validation Acc 20.11 %\n",
      "Epoch:180/200 AVG Training Loss:0.061 AVG Validation Loss:6.620 AVG Training Acc 98.12 % AVG Validation Acc 20.38 %\n",
      "Epoch:190/200 AVG Training Loss:0.060 AVG Validation Loss:5.295 AVG Training Acc 98.13 % AVG Validation Acc 20.38 %\n",
      "Epoch:200/200 AVG Training Loss:0.048 AVG Validation Loss:3.442 AVG Training Acc 98.59 % AVG Validation Acc 20.91 %\n",
      "Split 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f443a6823442d4ab8f823dfe5cbba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.485 AVG Validation Loss:0.511 AVG Training Acc 80.43 % AVG Validation Acc 79.76 %\n",
      "Epoch:20/200 AVG Training Loss:0.441 AVG Validation Loss:0.619 AVG Training Acc 81.64 % AVG Validation Acc 79.42 %\n",
      "Epoch:30/200 AVG Training Loss:0.378 AVG Validation Loss:0.558 AVG Training Acc 83.54 % AVG Validation Acc 79.62 %\n",
      "Epoch:40/200 AVG Training Loss:0.328 AVG Validation Loss:0.562 AVG Training Acc 85.09 % AVG Validation Acc 79.62 %\n",
      "Epoch:50/200 AVG Training Loss:0.363 AVG Validation Loss:0.565 AVG Training Acc 83.39 % AVG Validation Acc 79.29 %\n",
      "Epoch:60/200 AVG Training Loss:0.276 AVG Validation Loss:0.531 AVG Training Acc 86.15 % AVG Validation Acc 79.69 %\n",
      "Epoch:70/200 AVG Training Loss:0.245 AVG Validation Loss:0.517 AVG Training Acc 87.54 % AVG Validation Acc 79.56 %\n",
      "Epoch:80/200 AVG Training Loss:0.213 AVG Validation Loss:0.921 AVG Training Acc 89.23 % AVG Validation Acc 23.54 %\n",
      "Epoch:90/200 AVG Training Loss:0.215 AVG Validation Loss:0.652 AVG Training Acc 89.14 % AVG Validation Acc 76.87 %\n",
      "Epoch:100/200 AVG Training Loss:0.188 AVG Validation Loss:0.699 AVG Training Acc 90.79 % AVG Validation Acc 36.05 %\n",
      "Epoch:110/200 AVG Training Loss:0.168 AVG Validation Loss:1.198 AVG Training Acc 91.68 % AVG Validation Acc 23.34 %\n",
      "Epoch:120/200 AVG Training Loss:0.170 AVG Validation Loss:0.686 AVG Training Acc 91.80 % AVG Validation Acc 65.90 %\n",
      "Epoch:130/200 AVG Training Loss:0.130 AVG Validation Loss:0.715 AVG Training Acc 93.86 % AVG Validation Acc 33.62 %\n",
      "Epoch:140/200 AVG Training Loss:0.112 AVG Validation Loss:0.645 AVG Training Acc 95.13 % AVG Validation Acc 77.34 %\n",
      "Epoch:150/200 AVG Training Loss:0.108 AVG Validation Loss:0.539 AVG Training Acc 95.36 % AVG Validation Acc 79.35 %\n",
      "Epoch:160/200 AVG Training Loss:0.075 AVG Validation Loss:0.603 AVG Training Acc 97.06 % AVG Validation Acc 77.87 %\n",
      "Epoch:170/200 AVG Training Loss:0.078 AVG Validation Loss:0.569 AVG Training Acc 96.94 % AVG Validation Acc 78.95 %\n",
      "Epoch:180/200 AVG Training Loss:0.089 AVG Validation Loss:0.566 AVG Training Acc 96.45 % AVG Validation Acc 78.82 %\n",
      "Epoch:190/200 AVG Training Loss:0.055 AVG Validation Loss:0.610 AVG Training Acc 98.02 % AVG Validation Acc 79.49 %\n",
      "Epoch:200/200 AVG Training Loss:0.058 AVG Validation Loss:0.691 AVG Training Acc 97.88 % AVG Validation Acc 67.52 %\n",
      "exam_gifted\n",
      "Replica 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568d5d923fbf4b00b5817511f44b0565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e30e7ebfcfcf456bab05439925d3c1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best Accuracy found: 72.31%\n",
      "Epoch: 1\n",
      "Epoch:10/200 AVG Training Loss:0.578 AVG Validation Loss:0.656 AVG Training Acc 72.65 % AVG Validation Acc 71.71 %\n",
      "Epoch:20/200 AVG Training Loss:0.535 AVG Validation Loss:0.714 AVG Training Acc 74.53 % AVG Validation Acc 65.52 %\n",
      "Epoch:30/200 AVG Training Loss:0.501 AVG Validation Loss:0.752 AVG Training Acc 75.94 % AVG Validation Acc 65.99 %\n",
      "Epoch:40/200 AVG Training Loss:0.451 AVG Validation Loss:0.705 AVG Training Acc 78.60 % AVG Validation Acc 70.16 %\n",
      "Epoch:50/200 AVG Training Loss:0.439 AVG Validation Loss:0.630 AVG Training Acc 79.34 % AVG Validation Acc 71.91 %\n",
      "Epoch:60/200 AVG Training Loss:0.403 AVG Validation Loss:0.775 AVG Training Acc 80.87 % AVG Validation Acc 64.52 %\n",
      "Epoch:70/200 AVG Training Loss:0.378 AVG Validation Loss:0.760 AVG Training Acc 82.21 % AVG Validation Acc 39.05 %\n",
      "Epoch:80/200 AVG Training Loss:0.391 AVG Validation Loss:0.741 AVG Training Acc 81.67 % AVG Validation Acc 45.83 %\n",
      "Epoch:90/200 AVG Training Loss:0.389 AVG Validation Loss:0.671 AVG Training Acc 81.79 % AVG Validation Acc 71.71 %\n",
      "Epoch:100/200 AVG Training Loss:0.317 AVG Validation Loss:0.780 AVG Training Acc 84.70 % AVG Validation Acc 71.91 %\n",
      "Epoch:110/200 AVG Training Loss:0.324 AVG Validation Loss:0.637 AVG Training Acc 84.80 % AVG Validation Acc 70.09 %\n",
      "Epoch:120/200 AVG Training Loss:0.293 AVG Validation Loss:0.748 AVG Training Acc 86.37 % AVG Validation Acc 66.06 %\n",
      "Epoch:130/200 AVG Training Loss:0.246 AVG Validation Loss:0.851 AVG Training Acc 88.70 % AVG Validation Acc 71.84 %\n",
      "Epoch:140/200 AVG Training Loss:0.267 AVG Validation Loss:0.782 AVG Training Acc 87.59 % AVG Validation Acc 65.99 %\n",
      "Epoch:150/200 AVG Training Loss:0.221 AVG Validation Loss:0.953 AVG Training Acc 90.16 % AVG Validation Acc 71.84 %\n",
      "Epoch:160/200 AVG Training Loss:0.292 AVG Validation Loss:0.794 AVG Training Acc 86.99 % AVG Validation Acc 69.96 %\n",
      "Epoch:170/200 AVG Training Loss:0.206 AVG Validation Loss:0.863 AVG Training Acc 91.19 % AVG Validation Acc 66.20 %\n",
      "Epoch:180/200 AVG Training Loss:0.166 AVG Validation Loss:0.736 AVG Training Acc 93.49 % AVG Validation Acc 70.09 %\n",
      "Epoch:190/200 AVG Training Loss:0.160 AVG Validation Loss:0.919 AVG Training Acc 93.76 % AVG Validation Acc 64.31 %\n",
      "Epoch:200/200 AVG Training Loss:0.182 AVG Validation Loss:1.008 AVG Training Acc 92.77 % AVG Validation Acc 29.64 %\n",
      "Split 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc085d154e1477eba8b5da4d27462cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best Accuracy found: 72.36%\n",
      "Epoch: 1\n",
      "Epoch:10/200 AVG Training Loss:0.580 AVG Validation Loss:0.747 AVG Training Acc 72.73 % AVG Validation Acc 27.91 %\n",
      "Epoch:20/200 AVG Training Loss:0.550 AVG Validation Loss:0.634 AVG Training Acc 73.76 % AVG Validation Acc 72.23 %\n",
      "Epoch:30/200 AVG Training Loss:0.508 AVG Validation Loss:1.139 AVG Training Acc 75.74 % AVG Validation Acc 27.64 %\n",
      "Epoch:40/200 AVG Training Loss:0.480 AVG Validation Loss:0.698 AVG Training Acc 77.12 % AVG Validation Acc 37.86 %\n",
      "New Best Accuracy found: 72.43%\n",
      "Epoch: 45\n",
      "Epoch:50/200 AVG Training Loss:0.455 AVG Validation Loss:0.629 AVG Training Acc 78.25 % AVG Validation Acc 70.95 %\n",
      "Epoch:60/200 AVG Training Loss:0.435 AVG Validation Loss:0.604 AVG Training Acc 78.97 % AVG Validation Acc 72.43 %\n",
      "New Best Accuracy found: 72.63%\n",
      "Epoch: 67\n",
      "Epoch:70/200 AVG Training Loss:0.440 AVG Validation Loss:0.709 AVG Training Acc 79.00 % AVG Validation Acc 72.36 %\n",
      "Epoch:80/200 AVG Training Loss:0.381 AVG Validation Loss:0.601 AVG Training Acc 81.63 % AVG Validation Acc 72.29 %\n",
      "Epoch:90/200 AVG Training Loss:0.396 AVG Validation Loss:0.617 AVG Training Acc 81.46 % AVG Validation Acc 72.02 %\n",
      "Epoch:100/200 AVG Training Loss:0.334 AVG Validation Loss:0.683 AVG Training Acc 84.01 % AVG Validation Acc 48.49 %\n",
      "Epoch:110/200 AVG Training Loss:0.300 AVG Validation Loss:1.227 AVG Training Acc 85.81 % AVG Validation Acc 27.84 %\n",
      "Epoch:120/200 AVG Training Loss:0.314 AVG Validation Loss:1.667 AVG Training Acc 84.96 % AVG Validation Acc 27.77 %\n",
      "Epoch:130/200 AVG Training Loss:0.293 AVG Validation Loss:1.917 AVG Training Acc 86.13 % AVG Validation Acc 27.77 %\n",
      "Epoch:140/200 AVG Training Loss:0.329 AVG Validation Loss:0.720 AVG Training Acc 84.62 % AVG Validation Acc 35.84 %\n",
      "Epoch:150/200 AVG Training Loss:0.285 AVG Validation Loss:0.740 AVG Training Acc 86.64 % AVG Validation Acc 35.64 %\n",
      "Epoch:160/200 AVG Training Loss:0.354 AVG Validation Loss:1.324 AVG Training Acc 83.16 % AVG Validation Acc 28.04 %\n",
      "Epoch:170/200 AVG Training Loss:0.242 AVG Validation Loss:1.727 AVG Training Acc 89.65 % AVG Validation Acc 31.20 %\n",
      "Epoch:180/200 AVG Training Loss:0.247 AVG Validation Loss:0.903 AVG Training Acc 89.65 % AVG Validation Acc 34.30 %\n",
      "Epoch:190/200 AVG Training Loss:0.232 AVG Validation Loss:1.682 AVG Training Acc 90.20 % AVG Validation Acc 28.78 %\n",
      "Epoch:200/200 AVG Training Loss:0.190 AVG Validation Loss:2.512 AVG Training Acc 92.76 % AVG Validation Acc 27.84 %\n",
      "Split 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38513fcf00f4e4eb53f8ab61c935732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.573 AVG Validation Loss:0.727 AVG Training Acc 72.77 % AVG Validation Acc 33.02 %\n",
      "Epoch:20/200 AVG Training Loss:0.532 AVG Validation Loss:0.702 AVG Training Acc 74.57 % AVG Validation Acc 36.18 %\n",
      "Epoch:30/200 AVG Training Loss:0.497 AVG Validation Loss:0.597 AVG Training Acc 76.06 % AVG Validation Acc 72.23 %\n",
      "Epoch:40/200 AVG Training Loss:0.515 AVG Validation Loss:0.663 AVG Training Acc 75.12 % AVG Validation Acc 72.29 %\n",
      "Epoch:50/200 AVG Training Loss:0.445 AVG Validation Loss:0.631 AVG Training Acc 78.55 % AVG Validation Acc 72.16 %\n",
      "Epoch:60/200 AVG Training Loss:0.506 AVG Validation Loss:0.601 AVG Training Acc 76.16 % AVG Validation Acc 72.36 %\n",
      "Epoch:70/200 AVG Training Loss:0.466 AVG Validation Loss:0.669 AVG Training Acc 77.73 % AVG Validation Acc 71.89 %\n",
      "Epoch:80/200 AVG Training Loss:0.445 AVG Validation Loss:0.767 AVG Training Acc 78.89 % AVG Validation Acc 33.69 %\n",
      "Epoch:90/200 AVG Training Loss:0.432 AVG Validation Loss:0.885 AVG Training Acc 79.63 % AVG Validation Acc 33.56 %\n",
      "Epoch:100/200 AVG Training Loss:0.424 AVG Validation Loss:0.685 AVG Training Acc 80.20 % AVG Validation Acc 65.16 %\n",
      "Epoch:110/200 AVG Training Loss:0.415 AVG Validation Loss:0.638 AVG Training Acc 80.22 % AVG Validation Acc 71.82 %\n",
      "Epoch:120/200 AVG Training Loss:0.388 AVG Validation Loss:0.628 AVG Training Acc 81.59 % AVG Validation Acc 72.16 %\n",
      "Epoch:130/200 AVG Training Loss:0.366 AVG Validation Loss:0.716 AVG Training Acc 82.85 % AVG Validation Acc 72.23 %\n",
      "Epoch:140/200 AVG Training Loss:0.364 AVG Validation Loss:0.694 AVG Training Acc 82.96 % AVG Validation Acc 43.04 %\n",
      "Epoch:150/200 AVG Training Loss:0.349 AVG Validation Loss:0.686 AVG Training Acc 83.83 % AVG Validation Acc 70.07 %\n",
      "Epoch:160/200 AVG Training Loss:0.366 AVG Validation Loss:0.691 AVG Training Acc 82.99 % AVG Validation Acc 72.23 %\n",
      "Epoch:170/200 AVG Training Loss:0.561 AVG Validation Loss:1.055 AVG Training Acc 73.78 % AVG Validation Acc 29.19 %\n",
      "Epoch:180/200 AVG Training Loss:0.493 AVG Validation Loss:0.795 AVG Training Acc 76.11 % AVG Validation Acc 35.78 %\n",
      "Epoch:190/200 AVG Training Loss:0.466 AVG Validation Loss:0.766 AVG Training Acc 77.54 % AVG Validation Acc 34.84 %\n",
      "Epoch:200/200 AVG Training Loss:0.414 AVG Validation Loss:0.871 AVG Training Acc 80.74 % AVG Validation Acc 29.52 %\n",
      "Split 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb5019513e9441abc8fe6505ff5d17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.578 AVG Validation Loss:0.599 AVG Training Acc 72.70 % AVG Validation Acc 72.36 %\n",
      "Epoch:20/200 AVG Training Loss:0.550 AVG Validation Loss:0.605 AVG Training Acc 73.71 % AVG Validation Acc 72.29 %\n",
      "Epoch:30/200 AVG Training Loss:0.515 AVG Validation Loss:1.299 AVG Training Acc 75.02 % AVG Validation Acc 27.71 %\n",
      "Epoch:40/200 AVG Training Loss:0.466 AVG Validation Loss:0.925 AVG Training Acc 77.61 % AVG Validation Acc 28.45 %\n",
      "Epoch:50/200 AVG Training Loss:0.434 AVG Validation Loss:0.950 AVG Training Acc 79.21 % AVG Validation Acc 27.91 %\n",
      "Epoch:60/200 AVG Training Loss:0.407 AVG Validation Loss:1.228 AVG Training Acc 81.32 % AVG Validation Acc 28.04 %\n",
      "Epoch:70/200 AVG Training Loss:0.472 AVG Validation Loss:1.165 AVG Training Acc 77.26 % AVG Validation Acc 27.98 %\n",
      "Epoch:80/200 AVG Training Loss:0.376 AVG Validation Loss:2.010 AVG Training Acc 82.59 % AVG Validation Acc 27.91 %\n",
      "Epoch:90/200 AVG Training Loss:0.345 AVG Validation Loss:0.930 AVG Training Acc 83.43 % AVG Validation Acc 29.46 %\n",
      "Epoch:100/200 AVG Training Loss:0.343 AVG Validation Loss:1.576 AVG Training Acc 83.88 % AVG Validation Acc 27.98 %\n",
      "Epoch:110/200 AVG Training Loss:0.326 AVG Validation Loss:0.948 AVG Training Acc 85.34 % AVG Validation Acc 30.26 %\n",
      "Epoch:120/200 AVG Training Loss:0.292 AVG Validation Loss:1.209 AVG Training Acc 87.11 % AVG Validation Acc 28.58 %\n",
      "Epoch:130/200 AVG Training Loss:0.271 AVG Validation Loss:2.272 AVG Training Acc 88.20 % AVG Validation Acc 27.98 %\n",
      "Epoch:140/200 AVG Training Loss:0.317 AVG Validation Loss:0.609 AVG Training Acc 84.37 % AVG Validation Acc 71.69 %\n",
      "Epoch:150/200 AVG Training Loss:0.245 AVG Validation Loss:0.639 AVG Training Acc 88.44 % AVG Validation Acc 70.95 %\n",
      "Epoch:160/200 AVG Training Loss:0.228 AVG Validation Loss:0.818 AVG Training Acc 89.71 % AVG Validation Acc 72.09 %\n",
      "Epoch:170/200 AVG Training Loss:0.292 AVG Validation Loss:0.894 AVG Training Acc 86.54 % AVG Validation Acc 72.23 %\n",
      "Epoch:180/200 AVG Training Loss:0.257 AVG Validation Loss:1.226 AVG Training Acc 88.54 % AVG Validation Acc 72.16 %\n",
      "Epoch:190/200 AVG Training Loss:0.270 AVG Validation Loss:0.731 AVG Training Acc 87.71 % AVG Validation Acc 48.15 %\n",
      "Epoch:200/200 AVG Training Loss:0.279 AVG Validation Loss:0.643 AVG Training Acc 87.29 % AVG Validation Acc 70.34 %\n",
      "Split 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54501e3cdaa043e79303c665333f4cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.574 AVG Validation Loss:0.682 AVG Training Acc 73.00 % AVG Validation Acc 71.15 %\n",
      "Epoch:20/200 AVG Training Loss:0.535 AVG Validation Loss:0.779 AVG Training Acc 74.84 % AVG Validation Acc 29.59 %\n",
      "Epoch:30/200 AVG Training Loss:0.497 AVG Validation Loss:0.689 AVG Training Acc 76.43 % AVG Validation Acc 70.48 %\n",
      "Epoch:40/200 AVG Training Loss:0.470 AVG Validation Loss:0.888 AVG Training Acc 78.01 % AVG Validation Acc 70.48 %\n",
      "Epoch:50/200 AVG Training Loss:0.427 AVG Validation Loss:0.724 AVG Training Acc 79.83 % AVG Validation Acc 70.28 %\n",
      "Epoch:60/200 AVG Training Loss:0.413 AVG Validation Loss:1.456 AVG Training Acc 80.45 % AVG Validation Acc 72.16 %\n",
      "Epoch:70/200 AVG Training Loss:0.377 AVG Validation Loss:1.318 AVG Training Acc 82.18 % AVG Validation Acc 72.16 %\n",
      "Epoch:80/200 AVG Training Loss:0.402 AVG Validation Loss:1.136 AVG Training Acc 81.06 % AVG Validation Acc 71.76 %\n",
      "Epoch:90/200 AVG Training Loss:0.339 AVG Validation Loss:1.410 AVG Training Acc 84.03 % AVG Validation Acc 70.61 %\n",
      "Epoch:100/200 AVG Training Loss:0.343 AVG Validation Loss:0.810 AVG Training Acc 83.88 % AVG Validation Acc 71.89 %\n",
      "Epoch:110/200 AVG Training Loss:0.310 AVG Validation Loss:1.871 AVG Training Acc 85.06 % AVG Validation Acc 72.02 %\n",
      "Epoch:120/200 AVG Training Loss:0.297 AVG Validation Loss:1.444 AVG Training Acc 86.13 % AVG Validation Acc 71.49 %\n",
      "Epoch:130/200 AVG Training Loss:0.303 AVG Validation Loss:1.157 AVG Training Acc 86.28 % AVG Validation Acc 70.21 %\n",
      "Epoch:140/200 AVG Training Loss:0.273 AVG Validation Loss:1.362 AVG Training Acc 87.41 % AVG Validation Acc 71.55 %\n",
      "Epoch:150/200 AVG Training Loss:0.278 AVG Validation Loss:0.664 AVG Training Acc 87.33 % AVG Validation Acc 69.87 %\n",
      "Epoch:160/200 AVG Training Loss:0.236 AVG Validation Loss:0.957 AVG Training Acc 89.12 % AVG Validation Acc 70.01 %\n",
      "Epoch:170/200 AVG Training Loss:0.294 AVG Validation Loss:0.848 AVG Training Acc 87.09 % AVG Validation Acc 70.01 %\n",
      "Epoch:180/200 AVG Training Loss:0.315 AVG Validation Loss:0.689 AVG Training Acc 85.59 % AVG Validation Acc 70.21 %\n",
      "Epoch:190/200 AVG Training Loss:0.242 AVG Validation Loss:0.816 AVG Training Acc 88.65 % AVG Validation Acc 39.54 %\n",
      "Epoch:200/200 AVG Training Loss:0.221 AVG Validation Loss:0.850 AVG Training Acc 89.43 % AVG Validation Acc 34.70 %\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm(targets):\n",
    "    print(k)\n",
    "    \n",
    "    y = df_targets[k].values\n",
    "\n",
    "    #create a list containing one value per row\n",
    "    all_indices = list(range(len(df_targets)))\n",
    "    \n",
    "    #using train test split to later apply the rule accordingly\n",
    "    train_ind, test_ind = train_test_split(all_indices, test_size=0.2, \n",
    "                                           random_state = 5, stratify = y)\n",
    "    \n",
    "    #applied train_test_split rules accordingly\n",
    "    X_train_val = nd_array_100[train_ind,:,:]\n",
    "    y_train_val = y[train_ind]\n",
    "    \n",
    "    X_test = nd_array_100[test_ind, :, :]\n",
    "    y_test = y[test_ind]    \n",
    "        \n",
    "    #create dict to store fold performance\n",
    "    foldperf={}\n",
    "        \n",
    "    #reset \"best accuracy for treshold i and target k\"\n",
    "    best_accuracy = 0\n",
    "        \n",
    "    for repeat in range(replicas):\n",
    "        print('Replica {}'.format(repeat + 1))\n",
    "        \n",
    "        #make train_val split\n",
    "        for fold, (train_idx,val_idx) in tqdm(enumerate(splits.split(X_train_val, y_train_val))):\n",
    "            print('Split {}'.format(fold + 1))\n",
    "            \n",
    "            #make split between train and Val\n",
    "            X_train, y_train = X_train_val[train_idx], y_train_val[train_idx]\n",
    "            X_val, y_val = X_train_val[val_idx], y_train_val[val_idx]\n",
    "            \n",
    "            #scaling requires one scaler per channel (feature)\n",
    "            scalers = {}\n",
    "            for feature in range(X_train.shape[2]):\n",
    "                           \n",
    "                scalers[feature] = StandardScaler()\n",
    "                X_train[:, :, feature] = scalers[feature].fit_transform(X_train[:, :, feature]) \n",
    "\n",
    "            for col in range(X_val.shape[2]):\n",
    "                X_val[:, :, feature] = scalers[feature].transform(X_val[:, :, feature]) \n",
    "            \n",
    "            #second, convert everything to pytorch tensor - we will convert to tensor dataset and \n",
    "            X_train_tensors = torch.from_numpy(X_train)\n",
    "            X_val_tensors = torch.from_numpy(X_val)\n",
    "            \n",
    "            #convert X tensors to format FloatTensor\n",
    "            X_train_tensors = X_train_tensors.type(torch.cuda.FloatTensor)\n",
    "            X_val_tensors = X_val_tensors.type(torch.cuda.FloatTensor)\n",
    "            \n",
    "            #create y_tensor\n",
    "            y_train_tensors = torch.from_numpy(y_train)\n",
    "            y_val_tensors = torch.from_numpy(y_val)\n",
    "        \n",
    "            #convert y tensors to format longtensor\n",
    "            y_train_tensors = y_train_tensors.type(torch.cuda.LongTensor)\n",
    "            y_val_tensors = y_val_tensors.type(torch.cuda.LongTensor)\n",
    "            \n",
    "            #create Tensor Datasets and dataloaders for both Train and Val\n",
    "            train_dataset = TensorDataset(X_train_tensors, y_train_tensors)\n",
    "            val_dataset = TensorDataset(X_val_tensors, y_val_tensors)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "            #creates new model for each \n",
    "            model = LSTM_Uni(num_classes, input_size, hidden_size, num_layers, X_train_tensors.shape[1]).to('cuda') #our lstm class\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "            #scheduler = ReduceLROnPlateau(optimizer, \n",
    "            #                      'min', \n",
    "            #                      patience = 10,\n",
    "            #                      cooldown = 20,\n",
    "            #                     verbose = True)\n",
    "    \n",
    "            history = {'train_loss': [], 'val_loss': [],'train_acc':[],'val_acc':[], 'precision': [],\n",
    "                      'recall' : [], 'auroc': [], 'f1_score' : []}\n",
    "\n",
    "            for epoch in tqdm(range(num_epochs)):\n",
    "                train_loss, train_correct=train_epoch(model,train_loader,criterion,optimizer)\n",
    "                val_loss, val_correct, precision, recall, auroc, f1 = valid_epoch(model,val_loader,criterion)\n",
    "\n",
    "                train_loss = train_loss / len(train_loader.sampler)\n",
    "                train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "                val_loss = val_loss / len(val_loader.sampler)\n",
    "                val_acc = val_correct / len(val_loader.sampler) * 100\n",
    "        \n",
    "        \n",
    "                if (epoch+1) % 10 == 0: \n",
    "                 print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Validation Loss:{:.3f} AVG Training Acc {:.2f} % AVG Validation Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                             num_epochs,\n",
    "                                                                                                             train_loss,\n",
    "                                                                                                             val_loss,\n",
    "                                                                                                             train_acc,\n",
    "                                                                                                             val_acc))\n",
    "                history['train_loss'].append(train_loss)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['train_acc'].append(train_acc)\n",
    "                history['val_acc'].append(val_acc)\n",
    "                history['precision'].append(precision)\n",
    "                history['recall'].append(recall)\n",
    "                history['auroc'].append(auroc)\n",
    "                history['f1_score'].append(f1)\n",
    "                #scheduler.step(val_loss)\n",
    "    \n",
    "                if val_acc > best_accuracy:\n",
    "            \n",
    "                #replace best accuracy and save best model\n",
    "                    print(f'New Best Accuracy found: {val_acc:.2f}%\\nEpoch: {epoch + 1}')\n",
    "                    best_accuracy = val_acc\n",
    "                    best = deepcopy(model)\n",
    "                    curr_epoch = epoch + 1\n",
    "                    \n",
    "            #store fold performance\n",
    "            foldperf['fold{}'.format(fold+1)] = history\n",
    "        \n",
    "#     #saves fold performance for target \n",
    "#     threshold_dict[k] = pd.DataFrame.from_dict(foldperf, orient='index') # convert dict to dataframe\n",
    "        \n",
    "#      #explode to get eacxh epoch as a row\n",
    "#     threshold_dict[k] = threshold_dict[k].explode(list(threshold_dict[k].columns))\n",
    "#     torch.save(best,f\"../Models/{i}/Nova_IMS_best_{k}_{curr_epoch}_epochs.h\")\n",
    "        \n",
    "#     # from pandas.io.parsers import ExcelWriter\n",
    "#     with pd.ExcelWriter(f\"../Data/Modeling Stage/Results/IMS/Clicks per day/daily_clicks_{i}_{replicas}_replicas.xlsx\") as writer:  \n",
    "#         for sheet in targets:\n",
    "#             threshold_dict[sheet].to_excel(writer, sheet_name=str(sheet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b830f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49400eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe7680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0497596b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f8d28f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a20713d9",
   "metadata": {},
   "source": [
    "**4. Make the splits and Start Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45544589",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm(list(course_programs.keys())[1:]):\n",
    "    \n",
    "    print(i)\n",
    "    threshold_dict = {} #dict to store information in for each threshold\n",
    "    data = deepcopy(course_programs[i])\n",
    "    \n",
    "    data.set_index(['course_encoding', 'userid'], drop = True, inplace = True)\n",
    "    data.fillna(0, inplace = True)\n",
    "    \n",
    "    #set X and Y columns\n",
    "    X = data[data.columns[:25]] #different timesteps\n",
    "    y = data[data.columns[-4:]] #the 4 different putative targets\n",
    "    \n",
    "    for k in tqdm(targets):\n",
    "        print(k)\n",
    "        \n",
    "        #Start with train test split\n",
    "        X_train_val, X_test, y_train_val, y_test, = train_test_split(\n",
    "                                   X,\n",
    "                                   y[k], #replace when going for multi-target \n",
    "                                   test_size = 0.20,\n",
    "                                   random_state = 15,\n",
    "                                   shuffle=True,\n",
    "                                   stratify = y[k] #replace when going for multi-target\n",
    "                                    )\n",
    "        \n",
    "        #create dict to store fold performance\n",
    "        foldperf={}\n",
    "        \n",
    "        #reset \"best accuracy for treshold i and target k\"\n",
    "        best_accuracy = 0\n",
    "\n",
    "        #make train_val split\n",
    "        for fold, (train_idx,val_idx) in tqdm(enumerate(splits.split(X_train_val, y_train_val))):\n",
    "\n",
    "            print('Split {}'.format(fold + 1))\n",
    "            \n",
    "            #make split between train and Val\n",
    "            X_train, y_train = X_train_val.iloc[train_idx], y_train_val.iloc[train_idx]\n",
    "            X_val, y_val = X_train_val.iloc[val_idx], y_train_val.iloc[val_idx]\n",
    "            \n",
    "            #apply scaling after \n",
    "            X_train, X_val = normalize(X_train, X_val, 'MinMax')\n",
    "            \n",
    "            #second, convert everything to pytorch tensor - we will convert to tensor dataset and \n",
    "            X_train_tensors = Variable(torch.Tensor(X_train.values))\n",
    "            X_val_tensors = Variable(torch.Tensor(X_val.values))\n",
    "\n",
    "            y_train_tensors = Variable(torch.Tensor(y_train.values))\n",
    "            y_val_tensors = Variable(torch.Tensor(y_val.values)) \n",
    "\n",
    "            #reshaping to rows, timestamps, features \n",
    "            X_train_tensors = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], X_train_tensors.shape[1], 1))\n",
    "            X_val_tensors = torch.reshape(X_val_tensors,  (X_val_tensors.shape[0], X_val_tensors.shape[1], 1))\n",
    "        \n",
    "            #convert y tensors to format longtensor\n",
    "            y_train_tensors = y_train_tensors.type(torch.cuda.LongTensor)\n",
    "            y_val_tensors = y_val_tensors.type(torch.cuda.LongTensor)\n",
    "            \n",
    "            #create Tensor Datasets and dataloaders for both Train and Val\n",
    "            train_dataset = TensorDataset(X_train_tensors, y_train_tensors)\n",
    "            val_dataset = TensorDataset(X_val_tensors, y_val_tensors)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "            #creates new model for each \n",
    "            model = LSTM_Uni(num_classes, input_size, hidden_size, num_layers, X_train_tensors.shape[1]).to('cuda') #our lstm class\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "            scheduler = ReduceLROnPlateau(optimizer, \n",
    "                                  'min', \n",
    "                                  patience = 10,\n",
    "                                  cooldown = 20,\n",
    "                                 verbose = True)\n",
    "    \n",
    "            history = {'train_loss': [], 'val_loss': [],'train_acc':[],'val_acc':[], 'precision': [],\n",
    "                      'recall' : [], 'auroc': []}\n",
    "\n",
    "            for epoch in tqdm(range(num_epochs)):\n",
    "                train_loss, train_correct=train_epoch(model,train_loader,criterion,optimizer)\n",
    "                val_loss, val_correct, precision, recall, auroc = valid_epoch(model,val_loader,criterion)\n",
    "\n",
    "                train_loss = train_loss / len(train_loader.sampler)\n",
    "                train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "                val_loss = val_loss / len(val_loader.sampler)\n",
    "                val_acc = val_correct / len(val_loader.sampler) * 100\n",
    "        \n",
    "        \n",
    "                if (epoch+1) % 10 == 0: \n",
    "                    print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Validation Loss:{:.3f} AVG Training Acc {:.2f} % AVG Validation Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                             num_epochs,\n",
    "                                                                                                             train_loss,\n",
    "                                                                                                             val_loss,\n",
    "                                                                                                             train_acc,\n",
    "                                                                                                             val_acc))\n",
    "                history['train_loss'].append(train_loss)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['train_acc'].append(train_acc)\n",
    "                history['val_acc'].append(val_acc)\n",
    "                history['precision'].append(precision)\n",
    "                history['recall'].append(recall)\n",
    "                history['auroc'].append(auroc)\n",
    "                scheduler.step(val_loss)\n",
    "    \n",
    "                if val_acc > best_accuracy:\n",
    "            \n",
    "                #replace best accuracy and save best model\n",
    "                    print(f'New Best Accuracy found: {val_acc:.2f}%\\nEpoch: {epoch + 1}')\n",
    "                    best_accuracy = val_acc\n",
    "                    best = deepcopy(model)\n",
    "                    curr_epoch = epoch + 1\n",
    "                    \n",
    "            #store fold performance\n",
    "            foldperf['fold{}'.format(fold+1)] = history\n",
    "        \n",
    "        #saves fold performance for target \n",
    "        threshold_dict[k] = pd.DataFrame.from_dict(foldperf, orient='index') # convert dict to dataframe\n",
    "        \n",
    "        #explode to get eacxh epoch as a row\n",
    "        threshold_dict[k] = threshold_dict[k].explode(list(threshold_dict[k].columns))\n",
    "        torch.save(best,f\"../Models/{i}/Nova_IMS_best_{k}_{curr_epoch}_epochs.h\")\n",
    "        \n",
    "    # from pandas.io.parsers import ExcelWriter\n",
    "    with pd.ExcelWriter(f\"../Data/Modeling Stage/Results/IMS/Clicks per day/daily_clicks_{i}_{replicas}_replicas.xlsx\") as writer:  \n",
    "        for sheet in targets:\n",
    "                threshold_dict[sheet].to_excel(writer, sheet_name=str(sheet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9702df3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
