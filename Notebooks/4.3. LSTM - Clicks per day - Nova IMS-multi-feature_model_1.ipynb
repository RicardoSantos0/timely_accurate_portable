{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84eda90e",
   "metadata": {},
   "source": [
    "### Thesis notebook 4.3. - NOVA IMS\n",
    "\n",
    "#### LSTM - Temporal data representation\n",
    "\n",
    "In this notebook, we will finally start our application of temporal representation using LSTMs.\n",
    "The argument for the usage of Deep Learning stems from the fact that sequences themselves encode information that can be extracted using Recurrent Neural Networks and, more specifically, Long Short Term Memory Units.\n",
    "\n",
    "#### First Step: Setup a PyTorch environment that enables the use of GPU for training. \n",
    "\n",
    "The following cell wll confirm that the GPU will be the default device to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f27844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "cuda.init()\n",
    "## Get Id of default device\n",
    "torch.cuda.current_device()\n",
    "# 0\n",
    "cuda.Device(0).name() # '0' is the id of your GPU\n",
    "\n",
    "#set all tensors to gpu\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95429e",
   "metadata": {},
   "source": [
    "#### Second Step: Import the relevant packages and declare global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6c2d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules/libraries\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "\n",
    "#tqdm to monitor progress\n",
    "from tqdm.notebook import tqdm, trange\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "#time related features\n",
    "from datetime import timedelta\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "#vizualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#imblearn, scalers, kfold and metrics \n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer,PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, StratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, recall_score, classification_report, average_precision_score, precision_recall_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#import torch related\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "#and optimizer of learning rate\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "#import pytorch modules\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6c3f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables that may come in handy\n",
    "#course threshold sets the % duration that will be considered (1 = 100%)\n",
    "duration_threshold = [0.1, 0.25, 0.33, 0.5, 1]\n",
    "\n",
    "#colors for vizualizations\n",
    "nova_ims_colors = ['#BFD72F', '#5C666C']\n",
    "\n",
    "#standard color for student aggregates\n",
    "student_color = '#474838'\n",
    "\n",
    "#standard color for course aggragates\n",
    "course_color = '#1B3D2F'\n",
    "\n",
    "#standard continuous colormap\n",
    "standard_cmap = 'viridis_r'\n",
    "\n",
    "#Function designed to deal with multiindex and flatten it\n",
    "def flattenHierarchicalCol(col,sep = '_'):\n",
    "    '''converts multiindex columns into single index columns while retaining the hierarchical components'''\n",
    "    if not type(col) is tuple:\n",
    "        return col\n",
    "    else:\n",
    "        new_col = ''\n",
    "        for leveli,level in enumerate(col):\n",
    "            if not level == '':\n",
    "                if not leveli == 0:\n",
    "                    new_col += sep\n",
    "                new_col += level\n",
    "        return new_col\n",
    "    \n",
    "#number of replicas - number of repeats of stratified k fold - in this case 10\n",
    "replicas = 1\n",
    "\n",
    "#names to display on result figures\n",
    "date_names = {\n",
    "             'Date_threshold_10': '10% of Course Duration',   \n",
    "             'Date_threshold_25': '25% of Course Duration', \n",
    "             'Date_threshold_33': '33% of Course Duration', \n",
    "             'Date_threshold_50': '50% of Course Duration', \n",
    "             'Date_threshold_100':'100% of Course Duration', \n",
    "            }\n",
    "\n",
    "target_names = {\n",
    "                'exam_fail' : 'At risk - Exam Grade',\n",
    "                'final_fail' : 'At risk - Final Grade', \n",
    "                'exam_gifted' : 'High performer - Exam Grade', \n",
    "                'final_gifted': 'High performer - Final Grade'\n",
    "                }\n",
    "\n",
    "#targets\n",
    "targets = ['exam_fail', 'exam_gifted']\n",
    "\n",
    "#set the indexes to use for later\n",
    "index = [\"course_encoding\", \"cd_curso\", \"semestre\", \"courseid\", \"userid\", 'exam_gifted', 'exam_fail']\n",
    "\n",
    "#categories of objecctables\n",
    "objects = [\"course\", \"resource\", \"forum\", \"url\", \"folder\", \"quiz\", \"grade_grades\", \n",
    "           \"assignments\", \"groups\", \"user\", \"turnitintooltwo\", \"page\", \"choice\", \"other\"]          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c5ddb6",
   "metadata": {},
   "source": [
    "#### Step 3: Import data and take a preliminary look at it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8a23ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports dataframes\n",
    "course_programs = pd.read_excel(\"../Data/Modeling Stage/Nova_IMS_Temporal_Datasets_daily_clicks.xlsx\", \n",
    "                                dtype = {\n",
    "                                    'course_encoding' : int,\n",
    "                                    'userid' : int},\n",
    "                               sheet_name = 'Date_threshold_100')\n",
    "\n",
    "#save tables \n",
    "student_list = pd.read_csv('../Data/Modeling Stage/Nova_IMS_Filtered_targets.csv', \n",
    "                         dtype = {\n",
    "                                   'course_encoding': int,\n",
    "                                   'userid' : int,\n",
    "                                   })\n",
    "\n",
    "#drop unnamed 0 column\n",
    "#merge with the targets we calculated on the other \n",
    "course_programs = course_programs.merge(student_list, on = ['course_encoding', 'userid'], how = 'inner')\n",
    "course_programs.drop(['Unnamed: 0', 'exam_mark', 'final_mark'], axis = 1, inplace = True)\n",
    "    \n",
    "#convert results to object and need to convert column names to string\n",
    "course_programs['course_encoding'], course_programs['userid'] = course_programs['course_encoding'].astype(object), course_programs['userid'].astype(object)\n",
    "course_programs.columns = course_programs.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bc3c84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 63171 entries, 0 to 63170\n",
      "Columns: 149 entries, course_encoding to exam_gifted\n",
      "dtypes: float64(95), int64(50), object(4)\n",
      "memory usage: 72.3+ MB\n"
     ]
    }
   ],
   "source": [
    "course_programs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4a751ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_encoding</th>\n",
       "      <th>cd_curso</th>\n",
       "      <th>semestre</th>\n",
       "      <th>courseid</th>\n",
       "      <th>userid</th>\n",
       "      <th>objecttable</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>exam_fail</th>\n",
       "      <th>exam_gifted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>63171.0</td>\n",
       "      <td>63171.000000</td>\n",
       "      <td>63171</td>\n",
       "      <td>63171.000000</td>\n",
       "      <td>63171.0</td>\n",
       "      <td>63171</td>\n",
       "      <td>63171.000000</td>\n",
       "      <td>63171.000000</td>\n",
       "      <td>63171.000000</td>\n",
       "      <td>63171.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>41654.000000</td>\n",
       "      <td>39423.000000</td>\n",
       "      <td>37318.000000</td>\n",
       "      <td>34876.000000</td>\n",
       "      <td>32366.000000</td>\n",
       "      <td>27005.000000</td>\n",
       "      <td>19821.000000</td>\n",
       "      <td>1054.000000</td>\n",
       "      <td>63171.000000</td>\n",
       "      <td>63171.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>138.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1590.0</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6826.0</td>\n",
       "      <td>course</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1821.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.0</td>\n",
       "      <td>9295</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7906.809375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>185361.922290</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.019803</td>\n",
       "      <td>0.028716</td>\n",
       "      <td>0.045638</td>\n",
       "      <td>0.052113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522903</td>\n",
       "      <td>0.293027</td>\n",
       "      <td>0.235329</td>\n",
       "      <td>0.867387</td>\n",
       "      <td>0.250695</td>\n",
       "      <td>0.844844</td>\n",
       "      <td>0.339690</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>0.190784</td>\n",
       "      <td>0.287331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1986.226115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80819.428212</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.276235</td>\n",
       "      <td>0.308394</td>\n",
       "      <td>0.458564</td>\n",
       "      <td>0.550856</td>\n",
       "      <td>...</td>\n",
       "      <td>4.502103</td>\n",
       "      <td>2.288393</td>\n",
       "      <td>1.885921</td>\n",
       "      <td>9.354308</td>\n",
       "      <td>1.631869</td>\n",
       "      <td>8.256497</td>\n",
       "      <td>3.054003</td>\n",
       "      <td>0.043540</td>\n",
       "      <td>0.392922</td>\n",
       "      <td>0.452521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>859.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100001.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>7512.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100091.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9155.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200165.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9434.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200193.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9435.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400131.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>376.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows Ã— 149 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        course_encoding      cd_curso semestre       courseid   userid  \\\n",
       "count           63171.0  63171.000000    63171   63171.000000  63171.0   \n",
       "unique            138.0           NaN        6            NaN   1590.0   \n",
       "top               150.0           NaN       S1            NaN   6826.0   \n",
       "freq             1821.0           NaN    28407            NaN     93.0   \n",
       "mean                NaN   7906.809375      NaN  185361.922290      NaN   \n",
       "std                 NaN   1986.226115      NaN   80819.428212      NaN   \n",
       "min                 NaN    859.000000      NaN  100001.000000      NaN   \n",
       "25%                 NaN   7512.000000      NaN  100091.000000      NaN   \n",
       "50%                 NaN   9155.000000      NaN  200165.000000      NaN   \n",
       "75%                 NaN   9434.000000      NaN  200193.000000      NaN   \n",
       "max                 NaN   9435.000000      NaN  400131.000000      NaN   \n",
       "\n",
       "       objecttable             1             2             3             4  \\\n",
       "count        63171  63171.000000  63171.000000  63171.000000  63171.000000   \n",
       "unique          14           NaN           NaN           NaN           NaN   \n",
       "top         course           NaN           NaN           NaN           NaN   \n",
       "freq          9295           NaN           NaN           NaN           NaN   \n",
       "mean           NaN      0.019803      0.028716      0.045638      0.052113   \n",
       "std            NaN      0.276235      0.308394      0.458564      0.550856   \n",
       "min            NaN      0.000000      0.000000      0.000000      0.000000   \n",
       "25%            NaN      0.000000      0.000000      0.000000      0.000000   \n",
       "50%            NaN      0.000000      0.000000      0.000000      0.000000   \n",
       "75%            NaN      0.000000      0.000000      0.000000      0.000000   \n",
       "max            NaN     19.000000     17.000000     20.000000     30.000000   \n",
       "\n",
       "        ...           134           135           136           137  \\\n",
       "count   ...  41654.000000  39423.000000  37318.000000  34876.000000   \n",
       "unique  ...           NaN           NaN           NaN           NaN   \n",
       "top     ...           NaN           NaN           NaN           NaN   \n",
       "freq    ...           NaN           NaN           NaN           NaN   \n",
       "mean    ...      0.522903      0.293027      0.235329      0.867387   \n",
       "std     ...      4.502103      2.288393      1.885921      9.354308   \n",
       "min     ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%     ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%     ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%     ...      0.000000      0.000000      0.000000      0.000000   \n",
       "max     ...    214.000000    118.000000    169.000000    376.000000   \n",
       "\n",
       "                 138           139           140          141     exam_fail  \\\n",
       "count   32366.000000  27005.000000  19821.000000  1054.000000  63171.000000   \n",
       "unique           NaN           NaN           NaN          NaN           NaN   \n",
       "top              NaN           NaN           NaN          NaN           NaN   \n",
       "freq             NaN           NaN           NaN          NaN           NaN   \n",
       "mean        0.250695      0.844844      0.339690     0.001898      0.190784   \n",
       "std         1.631869      8.256497      3.054003     0.043540      0.392922   \n",
       "min         0.000000      0.000000      0.000000     0.000000      0.000000   \n",
       "25%         0.000000      0.000000      0.000000     0.000000      0.000000   \n",
       "50%         0.000000      0.000000      0.000000     0.000000      0.000000   \n",
       "75%         0.000000      0.000000      0.000000     0.000000      0.000000   \n",
       "max        58.000000    230.000000     96.000000     1.000000      1.000000   \n",
       "\n",
       "         exam_gifted  \n",
       "count   63171.000000  \n",
       "unique           NaN  \n",
       "top              NaN  \n",
       "freq             NaN  \n",
       "mean        0.287331  \n",
       "std         0.452521  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         1.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[11 rows x 149 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_programs.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3291817",
   "metadata": {},
   "source": [
    "In our first attempt, we will use the absolute number of clicks made by each student - scaled using standard scaler. \n",
    "Therefore, we can start by immediately placing our course encoding/userid pairings into the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be722ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train, test, scaler):\n",
    "    \n",
    "    if scaler == 'MinMax':\n",
    "        pt = MinMaxScaler()\n",
    "    elif scaler == 'Standard':\n",
    "        pt = StandardScaler()\n",
    "    elif scaler == 'Robust':\n",
    "        pt = RobustScaler()\n",
    "    elif scaler == 'Quantile':\n",
    "        pt = QuantileTransformer()\n",
    "    else:\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "    \n",
    "    data_train = pt.fit_transform(train)\n",
    "    data_test = pt.transform(test)\n",
    "    # convert the array back to a dataframe\n",
    "    normalized_train = pd.DataFrame(data_train,columns=train.columns)\n",
    "    normalized_test = pd.DataFrame(data_test,columns=test.columns)\n",
    "        \n",
    "    return normalized_train, normalized_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85843427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "course             9295\n",
       "resource           8757\n",
       "forum              7005\n",
       "url                6769\n",
       "folder             5758\n",
       "quiz               4870\n",
       "grade_grades       4670\n",
       "assignments        3858\n",
       "groups             3525\n",
       "user               2344\n",
       "turnitintooltwo    1998\n",
       "page               1728\n",
       "choice             1373\n",
       "other              1221\n",
       "Name: objecttable, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_programs.objecttable.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e92abd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = course_programs.copy()\n",
    "\n",
    "#The first 6 columns are index - column 141 is fully empty\n",
    "columns = test.drop(targets, axis = 1).columns[6:146]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f99ddf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create first pivot\n",
    "placeholder_pivot = pd.pivot_table(test, index = index, values = columns, columns = \"objecttable\",\n",
    "                  aggfunc = 'first')\n",
    "\n",
    "\n",
    "#applies the function that removes multiindex\n",
    "placeholder_pivot.columns = placeholder_pivot.columns.map(flattenHierarchicalCol)\n",
    "\n",
    "#also saving index for reindexing of the remaining stuff\n",
    "save_index = placeholder_pivot.index.copy()\n",
    "\n",
    "#we will need to create the multidimensional tensors\n",
    "placeholder_dict = {}\n",
    "\n",
    "#create dataset for targets\n",
    "df_targets = placeholder_pivot.reset_index().copy()[index]\n",
    "df_targets.set_index([\"course_encoding\", \"cd_curso\", \"semestre\", \"courseid\", \"userid\"], inplace = True)\n",
    "\n",
    "#initialize empty 3d array\n",
    "nd_array_100 = np.zeros((\n",
    "                               len(objects), #nbr of dimensions\n",
    "                               len(placeholder_pivot), #nbr of rows\n",
    "                               len(columns), #nbr of columns \n",
    "                              ))\n",
    "\n",
    "#likely inefficient, but should do the trick\n",
    "counter = 0\n",
    "\n",
    "#create multiple dataframes based on regex - this will create ndarray for the 100 duration\n",
    "for i in objects:\n",
    "    #create the objects\n",
    "    placeholder_dict[f'{i}'] = placeholder_pivot.filter(regex=f'_{i}')\n",
    "    \n",
    "    #remove text, convert column name back to numbers and sort numbers to ensure sequence\n",
    "    placeholder_dict[f'{i}'].columns = placeholder_dict[f'{i}'].columns.str.replace(r\"\\D+\", \"\", regex=True) \n",
    "    placeholder_dict[f'{i}'].columns = placeholder_dict[f'{i}'].columns.astype(int)\n",
    "    placeholder_dict[f'{i}'] = placeholder_dict[f'{i}'][sorted(placeholder_dict[f'{i}'].columns)].fillna(0)\n",
    "    \n",
    "    #converting df to nd array\n",
    "    nd_array_100[counter] = placeholder_dict[f'{i}'].values\n",
    "    counter += 1\n",
    "\n",
    "    #reshape to samples, rows, columns\n",
    "\n",
    "#switching to rows, columns, features\n",
    "nd_array_100 = nd_array_100.transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01e3c9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9296, 140, 14)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd_array_100.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4d5475",
   "metadata": {},
   "source": [
    "#### Implementing Cross-Validation with Deep Learning Model\n",
    "\n",
    "**1. Create the Deep Learning Model**\n",
    "\n",
    "In this instance, we will follow-up with on the approach used in Chen & Cui - CrossEntropyLoss with applied over a softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a16bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Uni(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length, dropout):\n",
    "        super(LSTM_Uni, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first = True) #lstm\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "    \n",
    "        self.fc = nn.Linear(self.hidden_size, num_classes) #fully connected last layer\n",
    "\n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        \n",
    "        #Xavier_init for both H_0 and C_0\n",
    "        torch.nn.init.xavier_normal_(h_0)\n",
    "        torch.nn.init.xavier_normal_(c_0)\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        lstm_out, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        last_output = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        \n",
    "        #we are interested in only keeping the last output\n",
    "        drop_out = self.dropout(last_output)\n",
    "        pre_bce = self.fc(drop_out) #Final Output - dense\n",
    "        return pre_bce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c356bd",
   "metadata": {},
   "source": [
    "**2. Define the train and validation Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25b29a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,dataloader,loss_fn,optimizer):\n",
    "    \n",
    "    train_loss,train_correct=0.0,0 \n",
    "    model.train()\n",
    "    for X, labels in dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = loss_fn(output,labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * X.size(0)\n",
    "        scores = F.sigmoid(output)\n",
    "        predictions = torch.round(scores)\n",
    "        #calculate % correct\n",
    "        train_correct += (predictions == labels.unsqueeze(1)).sum().item()\n",
    "        \n",
    "    return train_loss,train_correct\n",
    "  \n",
    "def valid_epoch(model,dataloader,loss_fn):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    targets = []\n",
    "    y_pred = []\n",
    "    probability_1 = []\n",
    "    \n",
    "    model.eval()\n",
    "    for X, labels in dataloader:\n",
    "\n",
    "        output = model(X)\n",
    "        loss=loss_fn(output,labels.unsqueeze(1))\n",
    "        valid_loss+=loss.item()*X.size(0)\n",
    "        scores = F.sigmoid(output)\n",
    "        predictions = torch.round(scores)\n",
    "        val_correct+=(predictions == labels.unsqueeze(1)).sum().item()\n",
    "        targets.append(labels.unsqueeze(1))\n",
    "        y_pred.append(predictions)\n",
    "        probability_1.append(scores)\n",
    "        \n",
    "    #concat all results\n",
    "    targets = torch.cat(targets).data.cpu().numpy()\n",
    "    y_pred = torch.cat(y_pred).data.cpu().numpy()\n",
    "    probability_1 = torch.cat(probability_1).data.cpu().numpy()\n",
    "    \n",
    "    #calculate precision, recall and AUC score\n",
    "    \n",
    "    precision = precision_score(targets, y_pred)\n",
    "    recall = recall_score(targets, y_pred)\n",
    "    f1 = f1_score(targets, y_pred)\n",
    "    auroc = roc_auc_score(targets, probability_1)\n",
    "    \n",
    "    #return all\n",
    "    return valid_loss,val_correct, precision, recall, auroc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4543fb3",
   "metadata": {},
   "source": [
    "**3. Define main hyperparameters of the model, including splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fcbbef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "#possible hyperparameters to test\n",
    "params = {\n",
    "    'lr': [0.001, 0.01, 0.005, 0.0005],\n",
    "    'hidden_size': [32, 64, 128, 256],\n",
    "    'batch_size': [32, 64, 128, 256, 512],\n",
    "    'num_layers': [1, 2, 3],\n",
    "    }\n",
    "\n",
    "num_epochs = 100 #100 epochs\n",
    "learning_rate = 0.001 #0.001 lr\n",
    "input_size = 14 #number of features\n",
    "hidden_size = 128 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "batch_size = 128\n",
    "dropout = 0.5\n",
    "\n",
    "\n",
    "#Shape of Output as required for Sigmoid Classifier\n",
    "num_classes = 1 #output shape\n",
    "\n",
    "k=2\n",
    "splits= StratifiedKFold(n_splits=k, random_state=15, shuffle = True) #kfold of 10 with 30 replicas\n",
    "criterion = nn.BCEWithLogitsLoss()    # cross-entropy for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89de5180",
   "metadata": {},
   "source": [
    "### Test on data - starting with the first 25 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f0588de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f836edd18e440aa5e8e970529d8893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exam_fail\n",
      "Replica 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ea3cdf8a374ee293b0942442ab2663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb3a9f75e7a48c1b04f7fe80790a3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best F1_score found: 0.27%\n",
      "Epoch: 2\n",
      " Accuracy: 79.80\n",
      "AUC: 57.41\n",
      "New Best F1_score found: 1.79%\n",
      "Epoch: 3\n",
      " Accuracy: 79.32\n",
      "AUC: 58.26\n",
      "New Best F1_score found: 30.23%\n",
      "Epoch: 4\n",
      " Accuracy: 72.81\n",
      "AUC: 58.83\n",
      "New Best F1_score found: 33.77%\n",
      "Epoch: 5\n",
      " Accuracy: 62.24\n",
      "AUC: 59.48\n",
      "New Best F1_score found: 34.84%\n",
      "Epoch: 6\n",
      " Accuracy: 52.61\n",
      "AUC: 60.04\n",
      "New Best F1_score found: 35.07%\n",
      "Epoch: 7\n",
      " Accuracy: 47.53\n",
      "AUC: 60.40\n",
      "New Best F1_score found: 35.78%\n",
      "Epoch: 8\n",
      " Accuracy: 53.55\n",
      "AUC: 59.10\n",
      "Epoch:10/100 AVG Training Loss:0.628 AVG Validation Loss:0.639 AVG Training Acc 65.07 % AVG Validation Acc 54.73 %\n",
      "New Best F1_score found: 35.98%\n",
      "Epoch: 10\n",
      " Accuracy: 54.73\n",
      "AUC: 60.41\n",
      "Epoch:20/100 AVG Training Loss:0.541 AVG Validation Loss:0.612 AVG Training Acc 75.09 % AVG Validation Acc 62.96 %\n",
      "Epoch:30/100 AVG Training Loss:0.432 AVG Validation Loss:0.595 AVG Training Acc 81.10 % AVG Validation Acc 71.60 %\n",
      "Epoch:40/100 AVG Training Loss:0.323 AVG Validation Loss:0.572 AVG Training Acc 84.64 % AVG Validation Acc 74.61 %\n",
      "Epoch:50/100 AVG Training Loss:0.253 AVG Validation Loss:0.595 AVG Training Acc 88.58 % AVG Validation Acc 74.66 %\n",
      "Epoch:60/100 AVG Training Loss:0.217 AVG Validation Loss:0.626 AVG Training Acc 90.60 % AVG Validation Acc 74.50 %\n",
      "Epoch    63: reducing learning rate of group 0 to 3.3000e-04.\n",
      "Epoch:70/100 AVG Training Loss:0.152 AVG Validation Loss:0.839 AVG Training Acc 92.98 % AVG Validation Acc 73.61 %\n",
      "Epoch:80/100 AVG Training Loss:0.123 AVG Validation Loss:0.992 AVG Training Acc 94.41 % AVG Validation Acc 71.19 %\n",
      "Epoch:90/100 AVG Training Loss:0.107 AVG Validation Loss:1.185 AVG Training Acc 95.08 % AVG Validation Acc 70.41 %\n",
      "Epoch:100/100 AVG Training Loss:0.082 AVG Validation Loss:1.235 AVG Training Acc 96.13 % AVG Validation Acc 72.59 %\n",
      "Split 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c590d4fdbc54d7f8e8668c01c96a483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/100 AVG Training Loss:0.674 AVG Validation Loss:0.656 AVG Training Acc 55.89 % AVG Validation Acc 52.02 %\n",
      "New Best F1_score found: 36.33%\n",
      "Epoch: 10\n",
      " Accuracy: 52.02\n",
      "AUC: 60.41\n",
      "New Best F1_score found: 36.52%\n",
      "Epoch: 12\n",
      " Accuracy: 49.14\n",
      "AUC: 61.09\n",
      "Epoch:20/100 AVG Training Loss:0.559 AVG Validation Loss:0.574 AVG Training Acc 75.42 % AVG Validation Acc 66.57 %\n",
      "Epoch:30/100 AVG Training Loss:0.447 AVG Validation Loss:0.681 AVG Training Acc 81.13 % AVG Validation Acc 67.70 %\n",
      "Epoch:40/100 AVG Training Loss:0.306 AVG Validation Loss:0.582 AVG Training Acc 86.08 % AVG Validation Acc 74.66 %\n",
      "Epoch:50/100 AVG Training Loss:0.253 AVG Validation Loss:0.784 AVG Training Acc 88.45 % AVG Validation Acc 69.53 %\n",
      "Epoch    52: reducing learning rate of group 0 to 3.3000e-04.\n",
      "Epoch:60/100 AVG Training Loss:0.206 AVG Validation Loss:0.676 AVG Training Acc 90.49 % AVG Validation Acc 74.42 %\n",
      "Epoch:70/100 AVG Training Loss:0.174 AVG Validation Loss:0.832 AVG Training Acc 92.42 % AVG Validation Acc 73.53 %\n",
      "Epoch:80/100 AVG Training Loss:0.150 AVG Validation Loss:0.901 AVG Training Acc 93.03 % AVG Validation Acc 73.08 %\n",
      "Epoch:90/100 AVG Training Loss:0.138 AVG Validation Loss:0.982 AVG Training Acc 93.94 % AVG Validation Acc 73.59 %\n",
      "Epoch:100/100 AVG Training Loss:0.115 AVG Validation Loss:1.077 AVG Training Acc 94.21 % AVG Validation Acc 72.30 %\n",
      "exam_gifted\n",
      "Replica 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7474e8377444beb995e0eebea44bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eeb071c297a440381c7bbc0507b8370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best F1_score found: 1.90%\n",
      "Epoch: 1\n",
      " Accuracy: 72.24\n",
      "AUC: 56.32\n",
      "New Best F1_score found: 7.10%\n",
      "Epoch: 2\n",
      " Accuracy: 71.84\n",
      "AUC: 55.90\n",
      "New Best F1_score found: 13.41%\n",
      "Epoch: 3\n",
      " Accuracy: 70.82\n",
      "AUC: 55.91\n",
      "New Best F1_score found: 18.18%\n",
      "Epoch: 4\n",
      " Accuracy: 69.02\n",
      "AUC: 55.88\n",
      "New Best F1_score found: 21.01%\n",
      "Epoch: 5\n",
      " Accuracy: 67.24\n",
      "AUC: 55.74\n",
      "New Best F1_score found: 23.85%\n",
      "Epoch: 6\n",
      " Accuracy: 65.65\n",
      "AUC: 55.66\n",
      "New Best F1_score found: 26.64%\n",
      "Epoch: 7\n",
      " Accuracy: 64.90\n",
      "AUC: 55.74\n",
      "New Best F1_score found: 30.24%\n",
      "Epoch: 8\n",
      " Accuracy: 62.78\n",
      "AUC: 55.87\n",
      "New Best F1_score found: 33.25%\n",
      "Epoch: 9\n",
      " Accuracy: 61.89\n",
      "AUC: 56.03\n",
      "Epoch:10/100 AVG Training Loss:0.689 AVG Validation Loss:0.683 AVG Training Acc 53.39 % AVG Validation Acc 59.98 %\n",
      "New Best F1_score found: 35.42%\n",
      "Epoch: 10\n",
      " Accuracy: 59.98\n",
      "AUC: 56.24\n",
      "New Best F1_score found: 37.76%\n",
      "Epoch: 11\n",
      " Accuracy: 59.39\n",
      "AUC: 56.46\n",
      "New Best F1_score found: 38.42%\n",
      "Epoch: 13\n",
      " Accuracy: 53.87\n",
      "AUC: 55.03\n",
      "New Best F1_score found: 40.96%\n",
      "Epoch: 14\n",
      " Accuracy: 56.11\n",
      "AUC: 57.01\n",
      "Epoch:20/100 AVG Training Loss:0.659 AVG Validation Loss:0.671 AVG Training Acc 65.31 % AVG Validation Acc 64.04 %\n",
      "Epoch    22: reducing learning rate of group 0 to 3.3000e-04.\n",
      "Epoch:30/100 AVG Training Loss:0.459 AVG Validation Loss:0.621 AVG Training Acc 78.59 % AVG Validation Acc 70.74 %\n",
      "Epoch:40/100 AVG Training Loss:0.431 AVG Validation Loss:0.610 AVG Training Acc 79.74 % AVG Validation Acc 71.36 %\n",
      "Epoch:50/100 AVG Training Loss:0.416 AVG Validation Loss:0.614 AVG Training Acc 80.30 % AVG Validation Acc 71.46 %\n",
      "Epoch:60/100 AVG Training Loss:0.401 AVG Validation Loss:0.630 AVG Training Acc 81.32 % AVG Validation Acc 71.36 %\n",
      "Epoch:70/100 AVG Training Loss:0.389 AVG Validation Loss:0.629 AVG Training Acc 82.09 % AVG Validation Acc 71.33 %\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0890e-04.\n",
      "Epoch:80/100 AVG Training Loss:0.367 AVG Validation Loss:0.669 AVG Training Acc 83.09 % AVG Validation Acc 70.82 %\n",
      "Epoch:90/100 AVG Training Loss:0.359 AVG Validation Loss:0.685 AVG Training Acc 83.82 % AVG Validation Acc 70.36 %\n",
      "Epoch:100/100 AVG Training Loss:0.352 AVG Validation Loss:0.700 AVG Training Acc 83.80 % AVG Validation Acc 69.82 %\n",
      "Split 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b53e972d6953485fb1ee1023fd77c111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best F1_score found: 43.65%\n",
      "Epoch: 2\n",
      " Accuracy: 35.42\n",
      "AUC: 54.60\n",
      "Epoch:10/100 AVG Training Loss:0.693 AVG Validation Loss:0.705 AVG Training Acc 52.34 % AVG Validation Acc 48.22 %\n",
      "Epoch:20/100 AVG Training Loss:0.648 AVG Validation Loss:0.689 AVG Training Acc 64.19 % AVG Validation Acc 59.52 %\n",
      "Epoch:30/100 AVG Training Loss:0.446 AVG Validation Loss:0.653 AVG Training Acc 79.27 % AVG Validation Acc 68.59 %\n",
      "Epoch:40/100 AVG Training Loss:0.397 AVG Validation Loss:0.645 AVG Training Acc 81.61 % AVG Validation Acc 70.01 %\n",
      "Epoch:50/100 AVG Training Loss:0.343 AVG Validation Loss:0.715 AVG Training Acc 85.09 % AVG Validation Acc 68.69 %\n",
      "Epoch    55: reducing learning rate of group 0 to 3.3000e-04.\n",
      "Epoch:60/100 AVG Training Loss:0.285 AVG Validation Loss:0.818 AVG Training Acc 88.29 % AVG Validation Acc 67.48 %\n",
      "Epoch:70/100 AVG Training Loss:0.244 AVG Validation Loss:0.967 AVG Training Acc 90.39 % AVG Validation Acc 67.37 %\n",
      "Epoch:80/100 AVG Training Loss:0.247 AVG Validation Loss:0.877 AVG Training Acc 90.39 % AVG Validation Acc 68.13 %\n",
      "Epoch:90/100 AVG Training Loss:0.200 AVG Validation Loss:1.167 AVG Training Acc 92.43 % AVG Validation Acc 64.71 %\n",
      "Epoch:100/100 AVG Training Loss:0.177 AVG Validation Loss:1.330 AVG Training Acc 93.55 % AVG Validation Acc 64.55 %\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm(targets):\n",
    "    print(k)\n",
    "    \n",
    "    y = df_targets[k].values\n",
    "\n",
    "    #create a list containing one value per row\n",
    "    all_indices = list(range(len(df_targets)))\n",
    "    \n",
    "    #using train test split to later apply the rule accordingly\n",
    "    train_ind, test_ind = train_test_split(all_indices, test_size=0.2, \n",
    "                                           random_state = 5, stratify = y)\n",
    "    \n",
    "    #applied train_test_split rules accordingly\n",
    "    X_train_val = nd_array_100[train_ind,:26,:]\n",
    "    y_train_val = y[train_ind]\n",
    "    \n",
    "    X_test = nd_array_100[test_ind, :26, :]\n",
    "    y_test = y[test_ind]    \n",
    "        \n",
    "    #create dict to store fold performance\n",
    "    repeatperf={}\n",
    "        \n",
    "    #reset \"best accuracy for treshold i and target k\"\n",
    "    best_f1_score = 0\n",
    "        \n",
    "    for repeat in range(replicas):\n",
    "        print('Replica {}'.format(repeat + 1))\n",
    "        \n",
    "        foldperf={}\n",
    "        \n",
    "        #make train_val split\n",
    "        for fold, (train_idx,val_idx) in tqdm(enumerate(splits.split(X_train_val, y_train_val))):\n",
    "            print('Split {}'.format(fold + 1))\n",
    "            \n",
    "            #make split between train and Val\n",
    "            X_train, y_train = X_train_val[train_idx], y_train_val[train_idx]\n",
    "            X_val, y_val = X_train_val[val_idx], y_train_val[val_idx]\n",
    "            \n",
    "            #scaling requires one scaler per channel (feature)\n",
    "            scalers = {}\n",
    "            for feature in range(X_train.shape[2]):\n",
    "                           \n",
    "                scalers[feature] = RobustScaler()\n",
    "                X_train[:, :, feature] = scalers[feature].fit_transform(X_train[:, :, feature]) \n",
    "\n",
    "            for col in range(X_val.shape[2]):\n",
    "                X_val[:, :, feature] = scalers[feature].transform(X_val[:, :, feature]) \n",
    "            \n",
    "            #need to oversample - will use smote\n",
    "            #will also require oneoversampler per channel (feature)\n",
    "            samplers = {}\n",
    "            \n",
    "            #create new nd_array with the correct size - 2 * majority class \n",
    "            X_train_res = np.zeros(shape = (2* (int(sc.stats.mode(y_train)[1])), X_train.shape[1], X_train.shape[2]))\n",
    "            for feature in range(X_train.shape[2]):\n",
    "                           \n",
    "                samplers[feature] = SMOTE()\n",
    "                X_train_res[:, :, feature], y_train_res = samplers[feature].fit_resample(X_train[:, :, feature], y_train) \n",
    "            \n",
    "            #second, convert everything to pytorch tensor - we will convert to tensor dataset and \n",
    "            X_train_tensors = torch.from_numpy(X_train_res)\n",
    "            X_val_tensors = torch.from_numpy(X_val)\n",
    "            \n",
    "            #convert X tensors to format FloatTensor\n",
    "            X_train_tensors = X_train_tensors.type(torch.cuda.FloatTensor)\n",
    "            X_val_tensors = X_val_tensors.type(torch.cuda.FloatTensor)\n",
    "            \n",
    "            #create y_tensor\n",
    "            y_train_tensors = torch.from_numpy(y_train_res)\n",
    "            y_val_tensors = torch.from_numpy(y_val)\n",
    "            \n",
    "            #convert y tensors to format longtensor\n",
    "            y_train_tensors = y_train_tensors.type(torch.cuda.FloatTensor)\n",
    "            y_val_tensors = y_val_tensors.type(torch.cuda.FloatTensor)\n",
    "            \n",
    "            #create Tensor Datasets and dataloaders for both Train and Val\n",
    "            train_dataset = TensorDataset(X_train_tensors, y_train_tensors)\n",
    "            val_dataset = TensorDataset(X_val_tensors, y_val_tensors)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "            #creates new model for each \n",
    "            model = LSTM_Uni(num_classes, input_size, hidden_size, num_layers, X_train_tensors.shape[1], dropout).to('cuda') #our lstm class\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "            scheduler = ReduceLROnPlateau(optimizer, \n",
    "                                 'min', \n",
    "                                 patience = 20,\n",
    "                                 cooldown = 30,\n",
    "                                threshold=0.00001,\n",
    "                                factor = 0.33,\n",
    "                                verbose = True)\n",
    "    \n",
    "            history = {'train_loss': [], 'val_loss': [],'train_acc':[],'val_acc':[], 'precision': [],\n",
    "                      'recall' : [], 'auroc': [], 'f1_score' : []}\n",
    "\n",
    "            for epoch in tqdm(range(num_epochs)):\n",
    "                train_loss, train_correct=train_epoch(model,train_loader,criterion,optimizer)\n",
    "                val_loss, val_correct, precision, recall, auroc, f1 = valid_epoch(model,val_loader,criterion)\n",
    "\n",
    "                train_loss = train_loss / len(train_loader.sampler)\n",
    "                train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "                val_loss = val_loss / len(val_loader.sampler)\n",
    "                val_acc = val_correct / len(val_loader.sampler) * 100\n",
    "        \n",
    "        \n",
    "                if (epoch+1) % 10 == 0: \n",
    "                 print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Validation Loss:{:.3f} AVG Training Acc {:.2f} % AVG Validation Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                             num_epochs,\n",
    "                                                                                                             train_loss,\n",
    "                                                                                                             val_loss,\n",
    "                                                                                                             train_acc,\n",
    "                                                                                                             val_acc))\n",
    "                history['train_loss'].append(train_loss)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['train_acc'].append(train_acc)\n",
    "                history['val_acc'].append(val_acc)\n",
    "                history['precision'].append(precision)\n",
    "                history['recall'].append(recall)\n",
    "                history['auroc'].append(auroc)\n",
    "                history['f1_score'].append(f1)\n",
    "                scheduler.step(val_loss)\n",
    "    \n",
    "                if f1 > best_f1_score:\n",
    "            \n",
    "                #replace best accuracy and save best model\n",
    "                    print(f'New Best F1_score found: {f1*100:.2f}%\\nEpoch: {epoch + 1}\\n', \n",
    "                         f'Accuracy: {val_acc:.2f}\\nAUC: {auroc*100:.2f}')\n",
    "                    best_f1_score = f1\n",
    "                    best = deepcopy(model)\n",
    "                    curr_epoch = epoch + 1\n",
    "                    \n",
    "            #store fold performance\n",
    "            foldperf['fold{}'.format(fold+1)] = history\n",
    "    \n",
    "    #create dict to store fold performance\n",
    "    repeatperf['repeat{}'.format(fold+1)] = foldperf\n",
    "        \n",
    "#     #saves fold performance for target \n",
    "#     threshold_dict[k] = pd.DataFrame.from_dict(foldperf, orient='index') # convert dict to dataframe\n",
    "        \n",
    "#      #explode to get eacxh epoch as a row\n",
    "#     threshold_dict[k] = threshold_dict[k].explode(list(threshold_dict[k].columns))\n",
    "#     torch.save(best,f\"../Models/{i}/Nova_IMS_best_{k}_{curr_epoch}_epochs.h\")\n",
    "        \n",
    "#     # from pandas.io.parsers import ExcelWriter\n",
    "#     with pd.ExcelWriter(f\"../Data/Modeling Stage/Results/IMS/Clicks per day/daily_clicks_{i}_{replicas}_replicas.xlsx\") as writer:  \n",
    "#         for sheet in targets:\n",
    "#             threshold_dict[sheet].to_excel(writer, sheet_name=str(sheet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7362dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in tqdm(targets):\n",
    "    print(k)\n",
    "    \n",
    "    y = df_targets[k].values\n",
    "\n",
    "    #create a list containing one value per row\n",
    "    all_indices = list(range(len(df_targets)))\n",
    "    \n",
    "    #using train test split to later apply the rule accordingly\n",
    "    train_ind, test_ind = train_test_split(all_indices, test_size=0.2, \n",
    "                                           random_state = 5, stratify = y)\n",
    "    \n",
    "    #applied train_test_split rules accordingly\n",
    "    X_train_val = nd_array_100[train_ind,:,:]\n",
    "    y_train_val = y[train_ind]\n",
    "    \n",
    "    X_test = nd_array_100[test_ind, :, :]\n",
    "    y_test = y[test_ind]    \n",
    "        \n",
    "    #create dict to store fold performance\n",
    "    repeatperf={}\n",
    "        \n",
    "    #reset \"best accuracy for treshold i and target k\"\n",
    "    best_accuracy = 0\n",
    "        \n",
    "    for repeat in range(replicas):\n",
    "        print('Replica {}'.format(repeat + 1))\n",
    "        \n",
    "        foldperf={}\n",
    "        \n",
    "        #make train_val split\n",
    "        for fold, (train_idx,val_idx) in tqdm(enumerate(splits.split(X_train_val, y_train_val))):\n",
    "            print('Split {}'.format(fold + 1))\n",
    "            \n",
    "            #make split between train and Val\n",
    "            X_train, y_train = X_train_val[train_idx], y_train_val[train_idx]\n",
    "            X_val, y_val = X_train_val[val_idx], y_train_val[val_idx]\n",
    "            \n",
    "            #scaling requires one scaler per channel (feature)\n",
    "            scalers = {}\n",
    "            for feature in range(X_train.shape[2]):\n",
    "                           \n",
    "                scalers[feature] = RobustScaler()\n",
    "                X_train[:, :, feature] = scalers[feature].fit_transform(X_train[:, :, feature]) \n",
    "\n",
    "            for col in range(X_val.shape[2]):\n",
    "                X_val[:, :, feature] = scalers[feature].transform(X_val[:, :, feature]) \n",
    "            \n",
    "            #need to oversample - will use smote\n",
    "            #will also require oneoversampler per channel (feature)\n",
    "            samplers = {}\n",
    "            \n",
    "            #create new nd_array with the correct size - 2 * majority class \n",
    "            X_train_res = np.zeros(shape = (2* (int(sc.stats.mode(y_train)[1])), X_train.shape[1], X_train.shape[2]))\n",
    "            for feature in range(X_train.shape[2]):\n",
    "                           \n",
    "                samplers[feature] = SMOTE()\n",
    "                X_train_res[:, :, feature], y_train_res = samplers[feature].fit_resample(X_train[:, :, feature], y_train) \n",
    "            \n",
    "            #second, convert everything to pytorch tensor - we will convert to tensor dataset and \n",
    "            X_train_tensors = torch.from_numpy(X_train_res)\n",
    "            X_val_tensors = torch.from_numpy(X_val)\n",
    "            \n",
    "            #convert X tensors to format FloatTensor\n",
    "            X_train_tensors = X_train_tensors.type(torch.cuda.FloatTensor)\n",
    "            X_val_tensors = X_val_tensors.type(torch.cuda.FloatTensor)\n",
    "            \n",
    "            #create y_tensor\n",
    "            y_train_tensors = torch.from_numpy(y_train_res)\n",
    "            y_val_tensors = torch.from_numpy(y_val)\n",
    "            \n",
    "            #convert y tensors to format longtensor\n",
    "            y_train_tensors = y_train_tensors.type(torch.cuda.FloatTensor)\n",
    "            y_val_tensors = y_val_tensors.type(torch.cuda.FloatTensor)\n",
    "            \n",
    "            #create Tensor Datasets and dataloaders for both Train and Val\n",
    "            train_dataset = TensorDataset(X_train_tensors, y_train_tensors)\n",
    "            val_dataset = TensorDataset(X_val_tensors, y_val_tensors)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "            #creates new model for each \n",
    "            model = LSTM_Uni(num_classes, input_size, hidden_size, num_layers, X_train_tensors.shape[1]).to('cuda') #our lstm class\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "            scheduler = ReduceLROnPlateau(optimizer, \n",
    "                                 'min', \n",
    "                                 patience = 20,\n",
    "                                 cooldown = 30,\n",
    "                                threshold=0.00001,\n",
    "                                factor = 0.33,\n",
    "                                verbose = True)\n",
    "    \n",
    "            history = {'train_loss': [], 'val_loss': [],'train_acc':[],'val_acc':[], 'precision': [],\n",
    "                      'recall' : [], 'auroc': [], 'f1_score' : []}\n",
    "\n",
    "            for epoch in tqdm(range(num_epochs)):\n",
    "                train_loss, train_correct=train_epoch(model,train_loader,criterion,optimizer)\n",
    "                val_loss, val_correct, precision, recall, auroc, f1 = valid_epoch(model,val_loader,criterion)\n",
    "\n",
    "                train_loss = train_loss / len(train_loader.sampler)\n",
    "                train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "                val_loss = val_loss / len(val_loader.sampler)\n",
    "                val_acc = val_correct / len(val_loader.sampler) * 100\n",
    "        \n",
    "        \n",
    "                if (epoch+1) % 10 == 0: \n",
    "                 print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Validation Loss:{:.3f} AVG Training Acc {:.2f} % AVG Validation Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                             num_epochs,\n",
    "                                                                                                             train_loss,\n",
    "                                                                                                             val_loss,\n",
    "                                                                                                             train_acc,\n",
    "                                                                                                             val_acc))\n",
    "                history['train_loss'].append(train_loss)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['train_acc'].append(train_acc)\n",
    "                history['val_acc'].append(val_acc)\n",
    "                history['precision'].append(precision)\n",
    "                history['recall'].append(recall)\n",
    "                history['auroc'].append(auroc)\n",
    "                history['f1_score'].append(f1)\n",
    "                scheduler.step(val_loss)\n",
    "    \n",
    "                if val_acc > best_accuracy:\n",
    "            \n",
    "                #replace best accuracy and save best model\n",
    "                    print(f'New Best Accuracy found: {val_acc:.2f}%\\nEpoch: {epoch + 1}')\n",
    "                    best_accuracy = val_acc\n",
    "                    best = deepcopy(model)\n",
    "                    curr_epoch = epoch + 1\n",
    "                    \n",
    "            #store fold performance\n",
    "            foldperf['fold{}'.format(fold+1)] = history\n",
    "    \n",
    "    #create dict to store fold performance\n",
    "    repeatperf['repeat{}'.format(fold+1)] = foldperf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf03a093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca5a2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09088873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b830f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbca30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e54fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383ff5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18484985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452402e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b19b54c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49400eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensors.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe7680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0497596b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f8d28f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a20713d9",
   "metadata": {},
   "source": [
    "**4. Make the splits and Start Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45544589",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm(list(course_programs.keys())[1:]):\n",
    "    \n",
    "    print(i)\n",
    "    threshold_dict = {} #dict to store information in for each threshold\n",
    "    data = deepcopy(course_programs[i])\n",
    "    \n",
    "    data.set_index(['course_encoding', 'userid'], drop = True, inplace = True)\n",
    "    data.fillna(0, inplace = True)\n",
    "    \n",
    "    #set X and Y columns\n",
    "    X = data[data.columns[:25]] #different timesteps\n",
    "    y = data[data.columns[-4:]] #the 4 different putative targets\n",
    "    \n",
    "    for k in tqdm(targets):\n",
    "        print(k)\n",
    "        \n",
    "        #Start with train test split\n",
    "        X_train_val, X_test, y_train_val, y_test, = train_test_split(\n",
    "                                   X,\n",
    "                                   y[k], #replace when going for multi-target \n",
    "                                   test_size = 0.20,\n",
    "                                   random_state = 15,\n",
    "                                   shuffle=True,\n",
    "                                   stratify = y[k] #replace when going for multi-target\n",
    "                                    )\n",
    "        \n",
    "        #create dict to store fold performance\n",
    "        foldperf={}\n",
    "        \n",
    "        #reset \"best accuracy for treshold i and target k\"\n",
    "        best_accuracy = 0\n",
    "\n",
    "        #make train_val split\n",
    "        for fold, (train_idx,val_idx) in tqdm(enumerate(splits.split(X_train_val, y_train_val))):\n",
    "\n",
    "            print('Split {}'.format(fold + 1))\n",
    "            \n",
    "            #make split between train and Val\n",
    "            X_train, y_train = X_train_val.iloc[train_idx], y_train_val.iloc[train_idx]\n",
    "            X_val, y_val = X_train_val.iloc[val_idx], y_train_val.iloc[val_idx]\n",
    "            \n",
    "            #apply scaling after \n",
    "            X_train, X_val = normalize(X_train, X_val, 'MinMax')\n",
    "            \n",
    "            #second, convert everything to pytorch tensor - we will convert to tensor dataset and \n",
    "            X_train_tensors = Variable(torch.Tensor(X_train.values))\n",
    "            X_val_tensors = Variable(torch.Tensor(X_val.values))\n",
    "\n",
    "            y_train_tensors = Variable(torch.Tensor(y_train.values))\n",
    "            y_val_tensors = Variable(torch.Tensor(y_val.values)) \n",
    "\n",
    "            #reshaping to rows, timestamps, features \n",
    "            X_train_tensors = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], X_train_tensors.shape[1], 1))\n",
    "            X_val_tensors = torch.reshape(X_val_tensors,  (X_val_tensors.shape[0], X_val_tensors.shape[1], 1))\n",
    "        \n",
    "            #convert y tensors to format longtensor\n",
    "            y_train_tensors = y_train_tensors.type(torch.cuda.LongTensor)\n",
    "            y_val_tensors = y_val_tensors.type(torch.cuda.LongTensor)\n",
    "            \n",
    "            #create Tensor Datasets and dataloaders for both Train and Val\n",
    "            train_dataset = TensorDataset(X_train_tensors, y_train_tensors)\n",
    "            val_dataset = TensorDataset(X_val_tensors, y_val_tensors)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "            #creates new model for each \n",
    "            model = LSTM_Uni(num_classes, input_size, hidden_size, num_layers, X_train_tensors.shape[1]).to('cuda') #our lstm class\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "            scheduler = ReduceLROnPlateau(optimizer, \n",
    "                                  'min', \n",
    "                                  patience = 10,\n",
    "                                  cooldown = 20,\n",
    "                                 verbose = True)\n",
    "    \n",
    "            history = {'train_loss': [], 'val_loss': [],'train_acc':[],'val_acc':[], 'precision': [],\n",
    "                      'recall' : [], 'auroc': []}\n",
    "\n",
    "            for epoch in tqdm(range(num_epochs)):\n",
    "                train_loss, train_correct=train_epoch(model,train_loader,criterion,optimizer)\n",
    "                val_loss, val_correct, precision, recall, auroc = valid_epoch(model,val_loader,criterion)\n",
    "\n",
    "                train_loss = train_loss / len(train_loader.sampler)\n",
    "                train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "                val_loss = val_loss / len(val_loader.sampler)\n",
    "                val_acc = val_correct / len(val_loader.sampler) * 100\n",
    "        \n",
    "        \n",
    "                if (epoch+1) % 10 == 0: \n",
    "                    print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Validation Loss:{:.3f} AVG Training Acc {:.2f} % AVG Validation Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                             num_epochs,\n",
    "                                                                                                             train_loss,\n",
    "                                                                                                             val_loss,\n",
    "                                                                                                             train_acc,\n",
    "                                                                                                             val_acc))\n",
    "                history['train_loss'].append(train_loss)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['train_acc'].append(train_acc)\n",
    "                history['val_acc'].append(val_acc)\n",
    "                history['precision'].append(precision)\n",
    "                history['recall'].append(recall)\n",
    "                history['auroc'].append(auroc)\n",
    "                scheduler.step(val_loss)\n",
    "    \n",
    "                if val_acc > best_accuracy:\n",
    "            \n",
    "                #replace best accuracy and save best model\n",
    "                    print(f'New Best Accuracy found: {val_acc:.2f}%\\nEpoch: {epoch + 1}')\n",
    "                    best_accuracy = val_acc\n",
    "                    best = deepcopy(model)\n",
    "                    curr_epoch = epoch + 1\n",
    "                    \n",
    "            #store fold performance\n",
    "            foldperf['fold{}'.format(fold+1)] = history\n",
    "        \n",
    "        #saves fold performance for target \n",
    "        threshold_dict[k] = pd.DataFrame.from_dict(foldperf, orient='index') # convert dict to dataframe\n",
    "        \n",
    "        #explode to get eacxh epoch as a row\n",
    "        threshold_dict[k] = threshold_dict[k].explode(list(threshold_dict[k].columns))\n",
    "        torch.save(best,f\"../Models/{i}/Nova_IMS_best_{k}_{curr_epoch}_epochs.h\")\n",
    "        \n",
    "    # from pandas.io.parsers import ExcelWriter\n",
    "    with pd.ExcelWriter(f\"../Data/Modeling Stage/Results/IMS/Clicks per day/daily_clicks_{i}_{replicas}_replicas.xlsx\") as writer:  \n",
    "        for sheet in targets:\n",
    "                threshold_dict[sheet].to_excel(writer, sheet_name=str(sheet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9702df3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
