{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84eda90e",
   "metadata": {},
   "source": [
    "### Thesis notebook 4.4. - NOVA IMS\n",
    "\n",
    "#### LSTM - Temporal data representation\n",
    "\n",
    "In this notebook, we will finally start our application of temporal representation using LSTMs and bi-directional LSTMs.\n",
    "The argument for the usage of Deep Learning stems from the fact that sequences themselves encode information that can be extracted using Recurrent Neural Networks and, more specifically, Long Short Term Memory Units.\n",
    "\n",
    "#### First Step: Setup a PyTorch environment that enables the use of GPU for training. \n",
    "\n",
    "The following cell wll confirm that the GPU will be the default device to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f27844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "cuda.init()\n",
    "## Get Id of default device\n",
    "torch.cuda.current_device()\n",
    "# 0\n",
    "cuda.Device(0).name() # '0' is the id of your GPU\n",
    "\n",
    "#set all tensors to gpu\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95429e",
   "metadata": {},
   "source": [
    "#### Second Step: Import the relevant packages and declare global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c2d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules/libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "#tqdm to monitor progress\n",
    "from tqdm.notebook import tqdm, trange\n",
    "tqdm.pandas(desc=\"Progress\")\n",
    "\n",
    "#time related features\n",
    "from datetime import timedelta\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "#vizualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#imblearn, scalers, kfold and metrics\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer,PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, RepeatedStratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, recall_score, classification_report, average_precision_score, precision_recall_curve\n",
    "\n",
    "#import torch related\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "#and optimizer of learning rate\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "#import pytorch modules\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c3f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables that may come in handy\n",
    "#course threshold sets the % duration that will be considered (1 = 100%)\n",
    "duration_threshold = [0.1, 0.25, 0.33, 0.5, 1]\n",
    "\n",
    "#colors for vizualizations\n",
    "nova_ims_colors = ['#BFD72F', '#5C666C']\n",
    "\n",
    "#standard color for student aggregates\n",
    "student_color = '#474838'\n",
    "\n",
    "#standard color for course aggragates\n",
    "course_color = '#1B3D2F'\n",
    "\n",
    "#standard continuous colormap\n",
    "standard_cmap = 'viridis_r'\n",
    "\n",
    "#Function designed to deal with multiindex and flatten it\n",
    "def flattenHierarchicalCol(col,sep = '_'):\n",
    "    '''converts multiindex columns into single index columns while retaining the hierarchical components'''\n",
    "    if not type(col) is tuple:\n",
    "        return col\n",
    "    else:\n",
    "        new_col = ''\n",
    "        for leveli,level in enumerate(col):\n",
    "            if not level == '':\n",
    "                if not leveli == 0:\n",
    "                    new_col += sep\n",
    "                new_col += level\n",
    "        return new_col\n",
    "    \n",
    "#number of replicas - number of repeats of stratified k fold - in this case 10\n",
    "replicas = 1\n",
    "\n",
    "#names to display on result figures\n",
    "date_names = {\n",
    "             'Date_threshold_10': '10% of Course Duration',   \n",
    "             'Date_threshold_25': '25% of Course Duration', \n",
    "             'Date_threshold_33': '33% of Course Duration', \n",
    "             'Date_threshold_50': '50% of Course Duration', \n",
    "             'Date_threshold_100':'100% of Course Duration', \n",
    "            }\n",
    "\n",
    "target_names = {\n",
    "                'exam_fail' : 'At risk - Exam Grade',\n",
    "                'final_fail' : 'At risk - Final Grade', \n",
    "                'exam_gifted' : 'High performer - Exam Grade', \n",
    "                'final_gifted': 'High performer - Final Grade'\n",
    "                }\n",
    "\n",
    "#targets\n",
    "targets = ['exam_fail' , 'final_fail' , 'exam_gifted' , 'final_gifted']\n",
    "temporal_columns = ['0 to 4%', '4 to 8%', '8 to 12%', '12 to 16%', '16 to 20%', '20 to 24%',\n",
    "       '24 to 28%', '28 to 32%', '32 to 36%', '36 to 40%', '40 to 44%',\n",
    "       '44 to 48%', '48 to 52%', '52 to 56%', '56 to 60%', '60 to 64%',\n",
    "       '64 to 68%', '68 to 72%', '72 to 76%', '76 to 80%', '80 to 84%',\n",
    "       '84 to 88%', '88 to 92%', '92 to 96%', '96 to 100%']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c5ddb6",
   "metadata": {},
   "source": [
    "#### Step 3: Import data and take a preliminary look at it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8a23ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports dataframes\n",
    "course_programs = pd.read_excel(\"../Data/Modeling Stage/Nova_IMS_Temporal_Datasets_25_splits.xlsx\", \n",
    "                                dtype = {\n",
    "                                    'course_encoding' : int,\n",
    "                                    'userid' : int},\n",
    "                               sheet_name = None)\n",
    "\n",
    "#save tables \n",
    "student_list = pd.read_csv('../Data/Modeling Stage/Nova_IMS_Filtered_targets.csv', \n",
    "                         dtype = {\n",
    "                                   'course_encoding': int,\n",
    "                                   'userid' : int,\n",
    "                                   })\n",
    "\n",
    "#drop unnamed 0 column\n",
    "for i in course_programs:\n",
    "        \n",
    "    #merge with the targets we calculated on the other \n",
    "    course_programs[i] = course_programs[i].merge(student_list, on = ['course_encoding', 'userid'], how = 'inner')\n",
    "    course_programs[i].drop(['Unnamed: 0', 'exam_mark', 'final_mark'], axis = 1, inplace = True)\n",
    "    \n",
    "    #convert results to object\n",
    "    course_programs[i]['course_encoding'], course_programs[i]['userid'] = course_programs[i]['course_encoding'].astype(object), course_programs[i]['userid'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc3c84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9296 entries, 0 to 9295\n",
      "Data columns (total 31 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   course_encoding  9296 non-null   object\n",
      " 1   userid           9296 non-null   object\n",
      " 2   0 to 4%          9296 non-null   int64 \n",
      " 3   4 to 8%          9296 non-null   int64 \n",
      " 4   8 to 12%         9296 non-null   int64 \n",
      " 5   12 to 16%        9296 non-null   int64 \n",
      " 6   16 to 20%        9296 non-null   int64 \n",
      " 7   20 to 24%        9296 non-null   int64 \n",
      " 8   24 to 28%        9296 non-null   int64 \n",
      " 9   28 to 32%        9296 non-null   int64 \n",
      " 10  32 to 36%        9296 non-null   int64 \n",
      " 11  36 to 40%        9296 non-null   int64 \n",
      " 12  40 to 44%        9296 non-null   int64 \n",
      " 13  44 to 48%        9296 non-null   int64 \n",
      " 14  48 to 52%        9296 non-null   int64 \n",
      " 15  52 to 56%        9296 non-null   int64 \n",
      " 16  56 to 60%        9296 non-null   int64 \n",
      " 17  60 to 64%        9296 non-null   int64 \n",
      " 18  64 to 68%        9296 non-null   int64 \n",
      " 19  68 to 72%        9296 non-null   int64 \n",
      " 20  72 to 76%        9296 non-null   int64 \n",
      " 21  76 to 80%        9296 non-null   int64 \n",
      " 22  80 to 84%        9296 non-null   int64 \n",
      " 23  84 to 88%        9296 non-null   int64 \n",
      " 24  88 to 92%        9296 non-null   int64 \n",
      " 25  92 to 96%        9296 non-null   int64 \n",
      " 26  96 to 100%       9296 non-null   int64 \n",
      " 27  exam_fail        9296 non-null   int64 \n",
      " 28  final_fail       9296 non-null   int64 \n",
      " 29  exam_gifted      9296 non-null   int64 \n",
      " 30  final_gifted     9296 non-null   int64 \n",
      "dtypes: int64(29), object(2)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "course_programs['Date_threshold_100'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a751ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_encoding</th>\n",
       "      <th>userid</th>\n",
       "      <th>0 to 4%</th>\n",
       "      <th>4 to 8%</th>\n",
       "      <th>8 to 12%</th>\n",
       "      <th>12 to 16%</th>\n",
       "      <th>16 to 20%</th>\n",
       "      <th>20 to 24%</th>\n",
       "      <th>24 to 28%</th>\n",
       "      <th>28 to 32%</th>\n",
       "      <th>...</th>\n",
       "      <th>76 to 80%</th>\n",
       "      <th>80 to 84%</th>\n",
       "      <th>84 to 88%</th>\n",
       "      <th>88 to 92%</th>\n",
       "      <th>92 to 96%</th>\n",
       "      <th>96 to 100%</th>\n",
       "      <th>exam_fail</th>\n",
       "      <th>final_fail</th>\n",
       "      <th>exam_gifted</th>\n",
       "      <th>final_gifted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9296.0</td>\n",
       "      <td>9296.0</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.0</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "      <td>9296.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>138.0</td>\n",
       "      <td>1590.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>150.0</td>\n",
       "      <td>3178.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>178.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.081863</td>\n",
       "      <td>8.307874</td>\n",
       "      <td>10.752797</td>\n",
       "      <td>11.193739</td>\n",
       "      <td>10.127797</td>\n",
       "      <td>8.966652</td>\n",
       "      <td>10.545396</td>\n",
       "      <td>11.445245</td>\n",
       "      <td>...</td>\n",
       "      <td>11.718051</td>\n",
       "      <td>13.136403</td>\n",
       "      <td>22.827883</td>\n",
       "      <td>27.341007</td>\n",
       "      <td>12.599613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.201377</td>\n",
       "      <td>0.149957</td>\n",
       "      <td>0.276893</td>\n",
       "      <td>0.308090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.526351</td>\n",
       "      <td>13.580025</td>\n",
       "      <td>13.626754</td>\n",
       "      <td>16.400023</td>\n",
       "      <td>14.291254</td>\n",
       "      <td>12.180177</td>\n",
       "      <td>13.507892</td>\n",
       "      <td>15.932226</td>\n",
       "      <td>...</td>\n",
       "      <td>28.186874</td>\n",
       "      <td>36.690068</td>\n",
       "      <td>47.158607</td>\n",
       "      <td>54.963959</td>\n",
       "      <td>35.194597</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.401051</td>\n",
       "      <td>0.357048</td>\n",
       "      <td>0.447487</td>\n",
       "      <td>0.461729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>619.000000</td>\n",
       "      <td>315.000000</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>614.000000</td>\n",
       "      <td>1091.000000</td>\n",
       "      <td>604.000000</td>\n",
       "      <td>747.000000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        course_encoding  userid      0 to 4%      4 to 8%     8 to 12%  \\\n",
       "count            9296.0  9296.0  9296.000000  9296.000000  9296.000000   \n",
       "unique            138.0  1590.0          NaN          NaN          NaN   \n",
       "top               150.0  3178.0          NaN          NaN          NaN   \n",
       "freq              178.0    14.0          NaN          NaN          NaN   \n",
       "mean                NaN     NaN     1.081863     8.307874    10.752797   \n",
       "std                 NaN     NaN     3.526351    13.580025    13.626754   \n",
       "min                 NaN     NaN     0.000000     0.000000     0.000000   \n",
       "25%                 NaN     NaN     0.000000     0.000000     1.000000   \n",
       "50%                 NaN     NaN     0.000000     2.000000     7.000000   \n",
       "75%                 NaN     NaN     1.000000    12.000000    15.000000   \n",
       "max                 NaN     NaN    66.000000   269.000000   360.000000   \n",
       "\n",
       "          12 to 16%    16 to 20%    20 to 24%    24 to 28%    28 to 32%  ...  \\\n",
       "count   9296.000000  9296.000000  9296.000000  9296.000000  9296.000000  ...   \n",
       "unique          NaN          NaN          NaN          NaN          NaN  ...   \n",
       "top             NaN          NaN          NaN          NaN          NaN  ...   \n",
       "freq            NaN          NaN          NaN          NaN          NaN  ...   \n",
       "mean      11.193739    10.127797     8.966652    10.545396    11.445245  ...   \n",
       "std       16.400023    14.291254    12.180177    13.507892    15.932226  ...   \n",
       "min        0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%        2.000000     2.000000     1.000000     2.000000     3.000000  ...   \n",
       "50%        7.000000     6.000000     5.000000     7.000000     7.000000  ...   \n",
       "75%       15.000000    13.000000    13.000000    14.000000    14.000000  ...   \n",
       "max      619.000000   315.000000   248.000000   268.000000   237.000000  ...   \n",
       "\n",
       "          76 to 80%    80 to 84%    84 to 88%    88 to 92%    92 to 96%  \\\n",
       "count   9296.000000  9296.000000  9296.000000  9296.000000  9296.000000   \n",
       "unique          NaN          NaN          NaN          NaN          NaN   \n",
       "top             NaN          NaN          NaN          NaN          NaN   \n",
       "freq            NaN          NaN          NaN          NaN          NaN   \n",
       "mean      11.718051    13.136403    22.827883    27.341007    12.599613   \n",
       "std       28.186874    36.690068    47.158607    54.963959    35.194597   \n",
       "min        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%        0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%        2.000000     2.000000     4.000000     2.000000     0.000000   \n",
       "75%       10.000000    10.000000    23.000000    27.000000     5.000000   \n",
       "max      614.000000  1091.000000   604.000000   747.000000   407.000000   \n",
       "\n",
       "        96 to 100%    exam_fail   final_fail  exam_gifted  final_gifted  \n",
       "count       9296.0  9296.000000  9296.000000  9296.000000   9296.000000  \n",
       "unique         NaN          NaN          NaN          NaN           NaN  \n",
       "top            NaN          NaN          NaN          NaN           NaN  \n",
       "freq           NaN          NaN          NaN          NaN           NaN  \n",
       "mean           0.0     0.201377     0.149957     0.276893      0.308090  \n",
       "std            0.0     0.401051     0.357048     0.447487      0.461729  \n",
       "min            0.0     0.000000     0.000000     0.000000      0.000000  \n",
       "25%            0.0     0.000000     0.000000     0.000000      0.000000  \n",
       "50%            0.0     0.000000     0.000000     0.000000      0.000000  \n",
       "75%            0.0     0.000000     0.000000     1.000000      1.000000  \n",
       "max            0.0     1.000000     1.000000     1.000000      1.000000  \n",
       "\n",
       "[11 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_programs['Date_threshold_100'].describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3291817",
   "metadata": {},
   "source": [
    "In our second attempt, we are looking to obtain a different result. Instead of using the absolute number of clicks used in each instance, we are instead looking to use the percent number of clicks made by each student relative to the the total number of clicks performed in the curricular unit.\n",
    "\n",
    "For that we will use transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ea43db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc60087f0234bfabf4935fb35444109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a38ec6e95a4413a4cd9edf59db8b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c30252161424c97a48de219d8125133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f72df43fe914141b3132cca600fd611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec63297636b943629472ed587339e89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350a73b416bd4cf886cefcbfdbfce231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(course_programs.keys()):\n",
    "    \n",
    "    for j in tqdm(temporal_columns):\n",
    "            course_programs[i][j] = np.where(course_programs[i].fillna(0).groupby('course_encoding')[j].transform('sum') != 0, #where valid operations occur\n",
    "                                             course_programs[i][j].fillna(0) / course_programs[i].fillna(0).groupby('course_encoding')[j].transform('sum') * 100, #calculate percentage\n",
    "                                             0) #otherwise, its 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be722ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train, test, scaler):\n",
    "    \n",
    "    if scaler == 'MinMax':\n",
    "        pt = MinMaxScaler()\n",
    "    elif scaler == 'Standard':\n",
    "        pt = StandardScaler()\n",
    "    elif scaler == 'Robust':\n",
    "        pt = RobustScaler()\n",
    "    elif scaler == 'Quantile':\n",
    "        pt = QuantileTransformer()\n",
    "    else:\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "    \n",
    "    data_train = pt.fit_transform(train)\n",
    "    data_test = pt.transform(test)\n",
    "    # convert the array back to a dataframe\n",
    "    normalized_train = pd.DataFrame(data_train,columns=train.columns)\n",
    "    normalized_test = pd.DataFrame(data_test,columns=test.columns)\n",
    "        \n",
    "    return normalized_train, normalized_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4d5475",
   "metadata": {},
   "source": [
    "#### Implementing Cross-Validation with Deep Learning Model\n",
    "\n",
    "**1. Create the Deep Learning Model**\n",
    "\n",
    "In this instance, we will follow-up with on the approach used in Chen & Cui - CrossEntropyLoss with applied over a softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a16bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Uni(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM_Uni, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first = True) #lstm\n",
    "        \n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "    \n",
    "        self.fc = nn.Linear(self.hidden_size, num_classes) #fully connected last layer\n",
    "\n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        \n",
    "        #Xavier_init for both H_0 and C_0\n",
    "        torch.nn.init.xavier_normal_(h_0)\n",
    "        torch.nn.init.xavier_normal_(c_0)\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        lstm_out, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        last_output = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        \n",
    "        drop_out = self.dropout(last_output)\n",
    "        pre_softmax = self.fc(drop_out) #Final Output - dense\n",
    "        return pre_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c356bd",
   "metadata": {},
   "source": [
    "**2. Define the train and validation Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b29a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,dataloader,loss_fn,optimizer):\n",
    "    \n",
    "    train_loss,train_correct=0.0,0 \n",
    "    model.train()\n",
    "    for X, labels in dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = loss_fn(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * X.size(0)\n",
    "        scores, predictions = torch.max(F.log_softmax(output.data), 1)\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "    return train_loss,train_correct\n",
    "  \n",
    "def valid_epoch(model,dataloader,loss_fn):\n",
    "    valid_loss, val_correct = 0.0, 0\n",
    "    targets = []\n",
    "    y_pred = []\n",
    "    probability_1 = []\n",
    "    \n",
    "    model.eval()\n",
    "    for X, labels in dataloader:\n",
    "\n",
    "        output = model(X)\n",
    "        loss=loss_fn(output,labels)\n",
    "        valid_loss+=loss.item()*X.size(0)\n",
    "        probability_1.append(F.softmax(output.data)[:,1])\n",
    "        predictions = torch.argmax(output, dim=1)\n",
    "        val_correct+=(predictions == labels).sum().item()\n",
    "        targets.append(labels)\n",
    "        y_pred.append(predictions)\n",
    "    \n",
    "    #concat all results\n",
    "    targets = torch.cat(targets).data.cpu().numpy()\n",
    "    y_pred = torch.cat(y_pred).data.cpu().numpy()\n",
    "    probability_1 = torch.cat(probability_1).data.cpu().numpy()\n",
    "    \n",
    "    #calculate precision, recall and AUC score\n",
    "    \n",
    "    precision = precision_score(targets, y_pred)\n",
    "    recall = recall_score(targets, y_pred)\n",
    "    auroc = roc_auc_score(targets, probability_1)\n",
    "    \n",
    "    #return all\n",
    "    return valid_loss,val_correct, precision, recall, auroc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4543fb3",
   "metadata": {},
   "source": [
    "**3. Define main hyperparameters of the model, including splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcbbef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "num_epochs = 200 #50 epochs\n",
    "learning_rate = 0.01 #0.001 lr\n",
    "input_size = 1 #number of features\n",
    "hidden_size = 40 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "#Shape of Output as required for SoftMax Classifier\n",
    "num_classes = 2 #output shape\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "k=10\n",
    "splits= RepeatedStratifiedKFold(n_splits=k, n_repeats=replicas, random_state=15) #kfold of 10 with 30 replicas\n",
    "criterion = nn.CrossEntropyLoss()    # cross-entropy for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20713d9",
   "metadata": {},
   "source": [
    "**4. Make the splits and Start Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45544589",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a58bab04964ab2b69f9283c356fee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date_threshold_10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffbea8ddb5c54c6e909e4d0ac306cfff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exam_fail\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfa9ff31e2642b581ac042f79daeb42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb0b5bc3b1b4ecbab51f11e8149eaad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best Accuracy found: 20.16%\n",
      "Epoch: 1\n",
      "Epoch:10/200 AVG Training Loss:0.499 AVG Validation Loss:5.719 AVG Training Acc 82.02 % AVG Validation Acc 20.16 %\n",
      "Epoch:20/200 AVG Training Loss:0.547 AVG Validation Loss:5.284 AVG Training Acc 74.43 % AVG Validation Acc 20.16 %\n",
      "Epoch:30/200 AVG Training Loss:0.570 AVG Validation Loss:5.057 AVG Training Acc 83.13 % AVG Validation Acc 20.16 %\n",
      "Epoch:40/200 AVG Training Loss:0.658 AVG Validation Loss:1.548 AVG Training Acc 64.76 % AVG Validation Acc 20.16 %\n",
      "Epoch:50/200 AVG Training Loss:0.645 AVG Validation Loss:3.198 AVG Training Acc 65.66 % AVG Validation Acc 20.16 %\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-03.\n",
      "New Best Accuracy found: 20.30%\n",
      "Epoch: 56\n",
      "New Best Accuracy found: 20.83%\n",
      "Epoch: 57\n",
      "New Best Accuracy found: 21.24%\n",
      "Epoch: 58\n",
      "New Best Accuracy found: 21.37%\n",
      "Epoch: 59\n",
      "Epoch:60/200 AVG Training Loss:0.702 AVG Validation Loss:0.790 AVG Training Acc 50.11 % AVG Validation Acc 21.64 %\n",
      "New Best Accuracy found: 21.64%\n",
      "Epoch: 60\n",
      "New Best Accuracy found: 21.91%\n",
      "Epoch: 64\n",
      "Epoch:70/200 AVG Training Loss:0.692 AVG Validation Loss:0.764 AVG Training Acc 50.80 % AVG Validation Acc 21.77 %\n",
      "New Best Accuracy found: 22.04%\n",
      "Epoch: 71\n",
      "Epoch:80/200 AVG Training Loss:0.689 AVG Validation Loss:0.765 AVG Training Acc 51.46 % AVG Validation Acc 21.77 %\n",
      "New Best Accuracy found: 22.18%\n",
      "Epoch: 81\n",
      "New Best Accuracy found: 22.31%\n",
      "Epoch: 82\n",
      "New Best Accuracy found: 22.98%\n",
      "Epoch: 84\n",
      "New Best Accuracy found: 23.66%\n",
      "Epoch: 88\n",
      "Epoch:90/200 AVG Training Loss:0.685 AVG Validation Loss:0.765 AVG Training Acc 53.17 % AVG Validation Acc 23.79 %\n",
      "New Best Accuracy found: 23.79%\n",
      "Epoch: 90\n",
      "New Best Accuracy found: 24.33%\n",
      "Epoch: 91\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-04.\n",
      "New Best Accuracy found: 24.73%\n",
      "Epoch: 94\n",
      "New Best Accuracy found: 25.67%\n",
      "Epoch: 95\n",
      "New Best Accuracy found: 26.34%\n",
      "Epoch: 96\n",
      "New Best Accuracy found: 27.15%\n",
      "Epoch: 97\n",
      "New Best Accuracy found: 28.09%\n",
      "Epoch: 98\n",
      "New Best Accuracy found: 28.49%\n",
      "Epoch: 99\n",
      "Epoch:100/200 AVG Training Loss:0.679 AVG Validation Loss:0.739 AVG Training Acc 54.78 % AVG Validation Acc 28.63 %\n",
      "New Best Accuracy found: 28.63%\n",
      "Epoch: 100\n",
      "New Best Accuracy found: 29.57%\n",
      "Epoch: 101\n",
      "New Best Accuracy found: 29.84%\n",
      "Epoch: 102\n",
      "New Best Accuracy found: 31.18%\n",
      "Epoch: 103\n",
      "New Best Accuracy found: 32.53%\n",
      "Epoch: 104\n",
      "New Best Accuracy found: 33.33%\n",
      "Epoch: 105\n",
      "New Best Accuracy found: 34.54%\n",
      "Epoch: 106\n",
      "New Best Accuracy found: 36.29%\n",
      "Epoch: 107\n",
      "New Best Accuracy found: 36.56%\n",
      "Epoch: 108\n",
      "New Best Accuracy found: 37.77%\n",
      "Epoch: 109\n",
      "Epoch:110/200 AVG Training Loss:0.677 AVG Validation Loss:0.720 AVG Training Acc 54.92 % AVG Validation Acc 38.04 %\n",
      "New Best Accuracy found: 38.04%\n",
      "Epoch: 110\n",
      "New Best Accuracy found: 38.31%\n",
      "Epoch: 111\n",
      "New Best Accuracy found: 39.38%\n",
      "Epoch: 112\n",
      "New Best Accuracy found: 39.65%\n",
      "Epoch: 113\n",
      "New Best Accuracy found: 39.78%\n",
      "Epoch: 114\n",
      "New Best Accuracy found: 40.46%\n",
      "Epoch: 115\n",
      "New Best Accuracy found: 40.99%\n",
      "Epoch: 116\n",
      "New Best Accuracy found: 41.67%\n",
      "Epoch: 117\n",
      "New Best Accuracy found: 41.94%\n",
      "Epoch: 118\n",
      "New Best Accuracy found: 42.07%\n",
      "Epoch: 119\n",
      "Epoch:120/200 AVG Training Loss:0.674 AVG Validation Loss:0.713 AVG Training Acc 56.10 % AVG Validation Acc 42.61 %\n",
      "New Best Accuracy found: 42.61%\n",
      "Epoch: 120\n",
      "New Best Accuracy found: 43.28%\n",
      "Epoch: 121\n",
      "New Best Accuracy found: 43.82%\n",
      "Epoch: 125\n",
      "New Best Accuracy found: 44.09%\n",
      "Epoch: 126\n",
      "New Best Accuracy found: 44.62%\n",
      "Epoch: 127\n",
      "New Best Accuracy found: 45.16%\n",
      "Epoch: 128\n",
      "New Best Accuracy found: 45.43%\n",
      "Epoch: 129\n",
      "Epoch:130/200 AVG Training Loss:0.674 AVG Validation Loss:0.708 AVG Training Acc 55.72 % AVG Validation Acc 45.43 %\n",
      "New Best Accuracy found: 45.56%\n",
      "Epoch: 133\n",
      "New Best Accuracy found: 45.70%\n",
      "Epoch: 135\n",
      "New Best Accuracy found: 46.10%\n",
      "Epoch: 136\n",
      "Epoch:140/200 AVG Training Loss:0.673 AVG Validation Loss:0.707 AVG Training Acc 55.72 % AVG Validation Acc 46.37 %\n",
      "New Best Accuracy found: 46.37%\n",
      "Epoch: 140\n",
      "New Best Accuracy found: 46.51%\n",
      "Epoch: 144\n",
      "New Best Accuracy found: 46.64%\n",
      "Epoch: 145\n",
      "New Best Accuracy found: 46.77%\n",
      "Epoch: 146\n",
      "New Best Accuracy found: 47.85%\n",
      "Epoch: 148\n",
      "New Best Accuracy found: 47.98%\n",
      "Epoch: 149\n",
      "Epoch:150/200 AVG Training Loss:0.673 AVG Validation Loss:0.706 AVG Training Acc 56.41 % AVG Validation Acc 47.85 %\n",
      "Epoch   155: reducing learning rate of group 0 to 1.0000e-05.\n",
      "New Best Accuracy found: 48.12%\n",
      "Epoch: 157\n",
      "New Best Accuracy found: 48.25%\n",
      "Epoch: 159\n",
      "Epoch:160/200 AVG Training Loss:0.672 AVG Validation Loss:0.705 AVG Training Acc 56.37 % AVG Validation Acc 47.98 %\n",
      "Epoch:170/200 AVG Training Loss:0.672 AVG Validation Loss:0.705 AVG Training Acc 56.22 % AVG Validation Acc 47.98 %\n",
      "Epoch:180/200 AVG Training Loss:0.673 AVG Validation Loss:0.705 AVG Training Acc 55.81 % AVG Validation Acc 48.12 %\n",
      "New Best Accuracy found: 48.39%\n",
      "Epoch: 185\n",
      "New Best Accuracy found: 48.52%\n",
      "Epoch: 189\n",
      "Epoch:190/200 AVG Training Loss:0.673 AVG Validation Loss:0.705 AVG Training Acc 55.89 % AVG Validation Acc 48.52 %\n",
      "Epoch:200/200 AVG Training Loss:0.672 AVG Validation Loss:0.704 AVG Training Acc 56.49 % AVG Validation Acc 48.52 %\n",
      "Split 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b635a5857c9b4157b92fdf23e56c7241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.501 AVG Validation Loss:4.009 AVG Training Acc 79.99 % AVG Validation Acc 20.16 %\n",
      "Epoch:20/200 AVG Training Loss:0.492 AVG Validation Loss:4.114 AVG Training Acc 80.53 % AVG Validation Acc 20.16 %\n",
      "Epoch:30/200 AVG Training Loss:0.603 AVG Validation Loss:3.138 AVG Training Acc 77.72 % AVG Validation Acc 20.16 %\n",
      "Epoch:40/200 AVG Training Loss:0.899 AVG Validation Loss:8.349 AVG Training Acc 77.97 % AVG Validation Acc 20.16 %\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:50/200 AVG Training Loss:0.694 AVG Validation Loss:1.150 AVG Training Acc 59.22 % AVG Validation Acc 20.30 %\n",
      "Epoch:60/200 AVG Training Loss:0.696 AVG Validation Loss:1.066 AVG Training Acc 57.59 % AVG Validation Acc 20.16 %\n",
      "Epoch:70/200 AVG Training Loss:0.700 AVG Validation Loss:0.982 AVG Training Acc 54.85 % AVG Validation Acc 20.70 %\n",
      "Epoch:80/200 AVG Training Loss:0.697 AVG Validation Loss:0.918 AVG Training Acc 54.28 % AVG Validation Acc 20.70 %\n",
      "Epoch:90/200 AVG Training Loss:0.639 AVG Validation Loss:1.535 AVG Training Acc 66.95 % AVG Validation Acc 21.10 %\n",
      "Epoch    96: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:100/200 AVG Training Loss:0.720 AVG Validation Loss:0.848 AVG Training Acc 51.16 % AVG Validation Acc 28.90 %\n",
      "Epoch:110/200 AVG Training Loss:0.697 AVG Validation Loss:0.808 AVG Training Acc 52.06 % AVG Validation Acc 28.63 %\n",
      "Epoch:120/200 AVG Training Loss:0.696 AVG Validation Loss:0.771 AVG Training Acc 52.60 % AVG Validation Acc 32.26 %\n",
      "Epoch:130/200 AVG Training Loss:0.693 AVG Validation Loss:0.767 AVG Training Acc 53.54 % AVG Validation Acc 32.39 %\n",
      "Epoch:140/200 AVG Training Loss:0.691 AVG Validation Loss:0.759 AVG Training Acc 54.01 % AVG Validation Acc 34.95 %\n",
      "Epoch:150/200 AVG Training Loss:0.687 AVG Validation Loss:0.744 AVG Training Acc 55.54 % AVG Validation Acc 39.65 %\n",
      "Epoch:160/200 AVG Training Loss:0.687 AVG Validation Loss:0.742 AVG Training Acc 54.91 % AVG Validation Acc 39.78 %\n",
      "Epoch:170/200 AVG Training Loss:0.685 AVG Validation Loss:0.730 AVG Training Acc 55.40 % AVG Validation Acc 43.28 %\n",
      "Epoch:180/200 AVG Training Loss:0.685 AVG Validation Loss:0.723 AVG Training Acc 56.17 % AVG Validation Acc 44.49 %\n",
      "Epoch:190/200 AVG Training Loss:0.685 AVG Validation Loss:0.723 AVG Training Acc 55.87 % AVG Validation Acc 44.62 %\n",
      "Epoch   191: reducing learning rate of group 0 to 1.0000e-05.\n",
      "New Best Accuracy found: 50.13%\n",
      "Epoch: 197\n",
      "New Best Accuracy found: 50.67%\n",
      "Epoch: 198\n",
      "New Best Accuracy found: 51.34%\n",
      "Epoch: 199\n",
      "Epoch:200/200 AVG Training Loss:0.679 AVG Validation Loss:0.703 AVG Training Acc 56.30 % AVG Validation Acc 51.34 %\n",
      "Split 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9cdc24d44e400997eb90f428bdd181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.556 AVG Validation Loss:4.449 AVG Training Acc 74.26 % AVG Validation Acc 20.16 %\n",
      "Epoch:20/200 AVG Training Loss:0.476 AVG Validation Loss:4.392 AVG Training Acc 81.66 % AVG Validation Acc 20.16 %\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.703 AVG Validation Loss:0.814 AVG Training Acc 50.41 % AVG Validation Acc 20.30 %\n",
      "Epoch:40/200 AVG Training Loss:0.700 AVG Validation Loss:0.806 AVG Training Acc 50.60 % AVG Validation Acc 20.70 %\n",
      "Epoch:50/200 AVG Training Loss:0.699 AVG Validation Loss:0.801 AVG Training Acc 50.05 % AVG Validation Acc 20.97 %\n",
      "Epoch:60/200 AVG Training Loss:0.700 AVG Validation Loss:0.799 AVG Training Acc 50.47 % AVG Validation Acc 21.24 %\n",
      "Epoch:70/200 AVG Training Loss:0.699 AVG Validation Loss:0.788 AVG Training Acc 50.30 % AVG Validation Acc 22.18 %\n",
      "Epoch:80/200 AVG Training Loss:0.695 AVG Validation Loss:0.798 AVG Training Acc 51.50 % AVG Validation Acc 21.91 %\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:90/200 AVG Training Loss:0.692 AVG Validation Loss:0.738 AVG Training Acc 51.66 % AVG Validation Acc 24.73 %\n",
      "Epoch:100/200 AVG Training Loss:0.687 AVG Validation Loss:0.710 AVG Training Acc 52.42 % AVG Validation Acc 30.65 %\n",
      "Epoch:110/200 AVG Training Loss:0.686 AVG Validation Loss:0.705 AVG Training Acc 52.57 % AVG Validation Acc 34.41 %\n",
      "Epoch:120/200 AVG Training Loss:0.685 AVG Validation Loss:0.702 AVG Training Acc 52.85 % AVG Validation Acc 36.02 %\n",
      "Epoch:130/200 AVG Training Loss:0.686 AVG Validation Loss:0.702 AVG Training Acc 52.62 % AVG Validation Acc 36.96 %\n",
      "Epoch   132: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:140/200 AVG Training Loss:0.683 AVG Validation Loss:0.701 AVG Training Acc 53.83 % AVG Validation Acc 37.77 %\n",
      "Epoch:150/200 AVG Training Loss:0.684 AVG Validation Loss:0.700 AVG Training Acc 53.52 % AVG Validation Acc 38.31 %\n",
      "Epoch:160/200 AVG Training Loss:0.685 AVG Validation Loss:0.699 AVG Training Acc 53.48 % AVG Validation Acc 38.71 %\n",
      "Epoch:170/200 AVG Training Loss:0.683 AVG Validation Loss:0.698 AVG Training Acc 53.75 % AVG Validation Acc 39.11 %\n",
      "Epoch:180/200 AVG Training Loss:0.684 AVG Validation Loss:0.696 AVG Training Acc 53.61 % AVG Validation Acc 39.65 %\n",
      "Epoch   186: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:190/200 AVG Training Loss:0.684 AVG Validation Loss:0.696 AVG Training Acc 53.26 % AVG Validation Acc 40.05 %\n",
      "Epoch:200/200 AVG Training Loss:0.684 AVG Validation Loss:0.697 AVG Training Acc 53.63 % AVG Validation Acc 39.92 %\n",
      "Split 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5f244e5f6f4dd491a0d04fdfcf080e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.552 AVG Validation Loss:3.598 AVG Training Acc 79.41 % AVG Validation Acc 20.16 %\n",
      "Epoch:20/200 AVG Training Loss:0.586 AVG Validation Loss:3.245 AVG Training Acc 76.22 % AVG Validation Acc 20.16 %\n",
      "Epoch:30/200 AVG Training Loss:0.615 AVG Validation Loss:9.320 AVG Training Acc 64.13 % AVG Validation Acc 20.16 %\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.756 AVG Validation Loss:0.913 AVG Training Acc 50.06 % AVG Validation Acc 20.16 %\n",
      "Epoch:50/200 AVG Training Loss:0.697 AVG Validation Loss:0.750 AVG Training Acc 47.95 % AVG Validation Acc 21.64 %\n",
      "Epoch:60/200 AVG Training Loss:0.695 AVG Validation Loss:0.749 AVG Training Acc 48.47 % AVG Validation Acc 23.39 %\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.692 AVG Validation Loss:0.743 AVG Training Acc 51.15 % AVG Validation Acc 24.33 %\n",
      "Epoch:80/200 AVG Training Loss:0.690 AVG Validation Loss:0.724 AVG Training Acc 51.36 % AVG Validation Acc 27.42 %\n",
      "Epoch:90/200 AVG Training Loss:0.689 AVG Validation Loss:0.713 AVG Training Acc 51.69 % AVG Validation Acc 29.70 %\n",
      "Epoch:100/200 AVG Training Loss:0.688 AVG Validation Loss:0.707 AVG Training Acc 51.78 % AVG Validation Acc 32.93 %\n",
      "Epoch:110/200 AVG Training Loss:0.687 AVG Validation Loss:0.704 AVG Training Acc 51.81 % AVG Validation Acc 33.60 %\n",
      "Epoch:120/200 AVG Training Loss:0.687 AVG Validation Loss:0.702 AVG Training Acc 51.95 % AVG Validation Acc 34.68 %\n",
      "Epoch:130/200 AVG Training Loss:0.687 AVG Validation Loss:0.702 AVG Training Acc 52.22 % AVG Validation Acc 36.02 %\n",
      "Epoch:140/200 AVG Training Loss:0.687 AVG Validation Loss:0.701 AVG Training Acc 52.22 % AVG Validation Acc 36.83 %\n",
      "Epoch:150/200 AVG Training Loss:0.686 AVG Validation Loss:0.703 AVG Training Acc 52.79 % AVG Validation Acc 37.37 %\n",
      "Epoch   153: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:160/200 AVG Training Loss:0.686 AVG Validation Loss:0.703 AVG Training Acc 52.44 % AVG Validation Acc 37.50 %\n",
      "Epoch:170/200 AVG Training Loss:0.686 AVG Validation Loss:0.703 AVG Training Acc 52.60 % AVG Validation Acc 37.63 %\n",
      "Epoch:180/200 AVG Training Loss:0.686 AVG Validation Loss:0.703 AVG Training Acc 52.84 % AVG Validation Acc 37.63 %\n",
      "Epoch   184: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:190/200 AVG Training Loss:0.686 AVG Validation Loss:0.703 AVG Training Acc 52.29 % AVG Validation Acc 37.63 %\n",
      "Epoch:200/200 AVG Training Loss:0.686 AVG Validation Loss:0.703 AVG Training Acc 52.67 % AVG Validation Acc 37.63 %\n",
      "Split 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f20f6e6c712436ebb86b16183e5b7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.555 AVG Validation Loss:6.239 AVG Training Acc 77.54 % AVG Validation Acc 20.16 %\n",
      "Epoch:20/200 AVG Training Loss:0.534 AVG Validation Loss:9.657 AVG Training Acc 81.59 % AVG Validation Acc 20.16 %\n",
      "Epoch:30/200 AVG Training Loss:0.665 AVG Validation Loss:1.477 AVG Training Acc 63.84 % AVG Validation Acc 20.16 %\n",
      "Epoch:40/200 AVG Training Loss:0.912 AVG Validation Loss:9.830 AVG Training Acc 75.02 % AVG Validation Acc 20.16 %\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:50/200 AVG Training Loss:0.700 AVG Validation Loss:0.791 AVG Training Acc 49.87 % AVG Validation Acc 20.56 %\n",
      "Epoch:60/200 AVG Training Loss:0.695 AVG Validation Loss:0.792 AVG Training Acc 51.76 % AVG Validation Acc 21.51 %\n",
      "Epoch:70/200 AVG Training Loss:0.694 AVG Validation Loss:0.787 AVG Training Acc 52.69 % AVG Validation Acc 22.18 %\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.688 AVG Validation Loss:0.734 AVG Training Acc 53.36 % AVG Validation Acc 30.91 %\n",
      "Epoch:90/200 AVG Training Loss:0.684 AVG Validation Loss:0.708 AVG Training Acc 54.79 % AVG Validation Acc 39.92 %\n",
      "Epoch:100/200 AVG Training Loss:0.684 AVG Validation Loss:0.703 AVG Training Acc 54.93 % AVG Validation Acc 45.43 %\n",
      "Epoch:110/200 AVG Training Loss:0.683 AVG Validation Loss:0.700 AVG Training Acc 55.48 % AVG Validation Acc 46.10 %\n",
      "Epoch:120/200 AVG Training Loss:0.682 AVG Validation Loss:0.697 AVG Training Acc 55.47 % AVG Validation Acc 49.87 %\n",
      "Epoch:130/200 AVG Training Loss:0.680 AVG Validation Loss:0.696 AVG Training Acc 55.75 % AVG Validation Acc 50.40 %\n",
      "Epoch:140/200 AVG Training Loss:0.680 AVG Validation Loss:0.698 AVG Training Acc 55.96 % AVG Validation Acc 50.13 %\n",
      "Epoch   144: reducing learning rate of group 0 to 1.0000e-05.\n",
      "New Best Accuracy found: 51.48%\n",
      "Epoch: 144\n",
      "New Best Accuracy found: 51.75%\n",
      "Epoch: 145\n",
      "Epoch:150/200 AVG Training Loss:0.679 AVG Validation Loss:0.695 AVG Training Acc 56.59 % AVG Validation Acc 51.88 %\n",
      "New Best Accuracy found: 51.88%\n",
      "Epoch: 150\n",
      "New Best Accuracy found: 52.55%\n",
      "Epoch: 155\n",
      "New Best Accuracy found: 52.69%\n",
      "Epoch: 158\n",
      "Epoch:160/200 AVG Training Loss:0.677 AVG Validation Loss:0.692 AVG Training Acc 56.74 % AVG Validation Acc 52.15 %\n",
      "New Best Accuracy found: 52.82%\n",
      "Epoch: 167\n",
      "Epoch:170/200 AVG Training Loss:0.678 AVG Validation Loss:0.691 AVG Training Acc 56.40 % AVG Validation Acc 52.28 %\n",
      "New Best Accuracy found: 53.09%\n",
      "Epoch: 171\n",
      "New Best Accuracy found: 53.36%\n",
      "Epoch: 172\n",
      "New Best Accuracy found: 53.49%\n",
      "Epoch: 176\n",
      "New Best Accuracy found: 53.76%\n",
      "Epoch: 177\n",
      "Epoch:180/200 AVG Training Loss:0.678 AVG Validation Loss:0.690 AVG Training Acc 56.04 % AVG Validation Acc 52.82 %\n",
      "Epoch:190/200 AVG Training Loss:0.677 AVG Validation Loss:0.689 AVG Training Acc 56.63 % AVG Validation Acc 54.03 %\n",
      "New Best Accuracy found: 54.03%\n",
      "Epoch: 190\n",
      "New Best Accuracy found: 54.84%\n",
      "Epoch: 194\n",
      "Epoch:200/200 AVG Training Loss:0.678 AVG Validation Loss:0.687 AVG Training Acc 56.54 % AVG Validation Acc 54.30 %\n",
      "Split 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050de5e3e30f4364a3de3476134dd78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.515 AVG Validation Loss:4.748 AVG Training Acc 78.00 % AVG Validation Acc 20.16 %\n",
      "Epoch:20/200 AVG Training Loss:0.588 AVG Validation Loss:6.046 AVG Training Acc 74.64 % AVG Validation Acc 20.16 %\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.975 AVG Validation Loss:1.249 AVG Training Acc 50.00 % AVG Validation Acc 20.16 %\n",
      "Epoch:40/200 AVG Training Loss:0.699 AVG Validation Loss:0.760 AVG Training Acc 48.63 % AVG Validation Acc 20.83 %\n",
      "Epoch:50/200 AVG Training Loss:0.697 AVG Validation Loss:0.756 AVG Training Acc 48.82 % AVG Validation Acc 22.85 %\n",
      "Epoch:60/200 AVG Training Loss:0.696 AVG Validation Loss:0.754 AVG Training Acc 49.24 % AVG Validation Acc 22.72 %\n",
      "Epoch:70/200 AVG Training Loss:0.694 AVG Validation Loss:0.755 AVG Training Acc 49.75 % AVG Validation Acc 22.58 %\n",
      "Epoch:80/200 AVG Training Loss:0.693 AVG Validation Loss:0.753 AVG Training Acc 50.59 % AVG Validation Acc 23.66 %\n",
      "Epoch:90/200 AVG Training Loss:0.688 AVG Validation Loss:0.752 AVG Training Acc 52.74 % AVG Validation Acc 24.19 %\n",
      "Epoch    99: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:100/200 AVG Training Loss:0.686 AVG Validation Loss:0.752 AVG Training Acc 53.13 % AVG Validation Acc 28.49 %\n",
      "Epoch:110/200 AVG Training Loss:0.680 AVG Validation Loss:0.717 AVG Training Acc 55.14 % AVG Validation Acc 36.56 %\n",
      "Epoch:120/200 AVG Training Loss:0.677 AVG Validation Loss:0.701 AVG Training Acc 55.87 % AVG Validation Acc 41.40 %\n",
      "Epoch:130/200 AVG Training Loss:0.674 AVG Validation Loss:0.694 AVG Training Acc 56.64 % AVG Validation Acc 45.97 %\n",
      "Epoch:140/200 AVG Training Loss:0.674 AVG Validation Loss:0.692 AVG Training Acc 56.55 % AVG Validation Acc 47.18 %\n",
      "Epoch:150/200 AVG Training Loss:0.673 AVG Validation Loss:0.691 AVG Training Acc 56.84 % AVG Validation Acc 49.46 %\n",
      "Epoch:160/200 AVG Training Loss:0.671 AVG Validation Loss:0.689 AVG Training Acc 57.55 % AVG Validation Acc 50.67 %\n",
      "Epoch:170/200 AVG Training Loss:0.672 AVG Validation Loss:0.690 AVG Training Acc 57.06 % AVG Validation Acc 51.08 %\n",
      "Epoch   171: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:180/200 AVG Training Loss:0.670 AVG Validation Loss:0.689 AVG Training Acc 56.91 % AVG Validation Acc 51.34 %\n",
      "Epoch:190/200 AVG Training Loss:0.670 AVG Validation Loss:0.688 AVG Training Acc 57.14 % AVG Validation Acc 51.75 %\n",
      "Epoch:200/200 AVG Training Loss:0.671 AVG Validation Loss:0.688 AVG Training Acc 57.81 % AVG Validation Acc 51.75 %\n",
      "Split 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d66a718c3b4a46ad89d9b67f944efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.530 AVG Validation Loss:8.902 AVG Training Acc 79.41 % AVG Validation Acc 20.05 %\n",
      "Epoch:20/200 AVG Training Loss:0.665 AVG Validation Loss:1.490 AVG Training Acc 63.81 % AVG Validation Acc 20.05 %\n",
      "Epoch:30/200 AVG Training Loss:0.577 AVG Validation Loss:9.726 AVG Training Acc 81.33 % AVG Validation Acc 20.05 %\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.846 AVG Validation Loss:1.061 AVG Training Acc 49.96 % AVG Validation Acc 20.05 %\n",
      "Epoch:50/200 AVG Training Loss:0.699 AVG Validation Loss:0.752 AVG Training Acc 47.63 % AVG Validation Acc 20.05 %\n",
      "Epoch:60/200 AVG Training Loss:0.697 AVG Validation Loss:0.745 AVG Training Acc 48.39 % AVG Validation Acc 20.59 %\n",
      "Epoch:70/200 AVG Training Loss:0.693 AVG Validation Loss:0.748 AVG Training Acc 50.94 % AVG Validation Acc 22.07 %\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.687 AVG Validation Loss:0.742 AVG Training Acc 53.26 % AVG Validation Acc 36.74 %\n",
      "Epoch:90/200 AVG Training Loss:0.683 AVG Validation Loss:0.708 AVG Training Acc 55.74 % AVG Validation Acc 50.34 %\n",
      "New Best Accuracy found: 55.18%\n",
      "Epoch: 98\n",
      "New Best Accuracy found: 55.45%\n",
      "Epoch: 99\n",
      "Epoch:100/200 AVG Training Loss:0.680 AVG Validation Loss:0.694 AVG Training Acc 56.33 % AVG Validation Acc 55.85 %\n",
      "New Best Accuracy found: 55.85%\n",
      "Epoch: 100\n",
      "New Best Accuracy found: 56.12%\n",
      "Epoch: 101\n",
      "New Best Accuracy found: 56.39%\n",
      "Epoch: 102\n",
      "New Best Accuracy found: 56.66%\n",
      "Epoch: 104\n",
      "New Best Accuracy found: 56.80%\n",
      "Epoch: 105\n",
      "New Best Accuracy found: 57.20%\n",
      "Epoch: 106\n",
      "New Best Accuracy found: 57.34%\n",
      "Epoch: 107\n",
      "New Best Accuracy found: 57.47%\n",
      "Epoch: 109\n",
      "Epoch:110/200 AVG Training Loss:0.681 AVG Validation Loss:0.687 AVG Training Acc 56.72 % AVG Validation Acc 57.34 %\n",
      "New Best Accuracy found: 57.87%\n",
      "Epoch: 111\n",
      "New Best Accuracy found: 58.01%\n",
      "Epoch: 115\n",
      "New Best Accuracy found: 58.14%\n",
      "Epoch: 116\n",
      "New Best Accuracy found: 58.55%\n",
      "Epoch: 117\n",
      "New Best Accuracy found: 58.68%\n",
      "Epoch: 119\n",
      "Epoch:120/200 AVG Training Loss:0.679 AVG Validation Loss:0.684 AVG Training Acc 56.90 % AVG Validation Acc 58.41 %\n",
      "New Best Accuracy found: 59.22%\n",
      "Epoch: 124\n",
      "New Best Accuracy found: 59.35%\n",
      "Epoch: 125\n",
      "New Best Accuracy found: 59.49%\n",
      "Epoch: 126\n",
      "New Best Accuracy found: 59.76%\n",
      "Epoch: 129\n",
      "Epoch:130/200 AVG Training Loss:0.677 AVG Validation Loss:0.684 AVG Training Acc 57.35 % AVG Validation Acc 59.62 %\n",
      "New Best Accuracy found: 59.89%\n",
      "Epoch: 134\n",
      "New Best Accuracy found: 60.03%\n",
      "Epoch: 135\n",
      "New Best Accuracy found: 60.16%\n",
      "Epoch: 138\n",
      "Epoch:140/200 AVG Training Loss:0.677 AVG Validation Loss:0.683 AVG Training Acc 57.36 % AVG Validation Acc 60.16 %\n",
      "New Best Accuracy found: 60.30%\n",
      "Epoch: 141\n",
      "New Best Accuracy found: 60.43%\n",
      "Epoch: 143\n",
      "Epoch:150/200 AVG Training Loss:0.679 AVG Validation Loss:0.683 AVG Training Acc 57.34 % AVG Validation Acc 60.70 %\n",
      "New Best Accuracy found: 60.70%\n",
      "Epoch: 150\n",
      "Epoch:160/200 AVG Training Loss:0.678 AVG Validation Loss:0.683 AVG Training Acc 57.23 % AVG Validation Acc 60.57 %\n",
      "Epoch   161: reducing learning rate of group 0 to 1.0000e-05.\n",
      "New Best Accuracy found: 60.83%\n",
      "Epoch: 166\n",
      "Epoch:170/200 AVG Training Loss:0.677 AVG Validation Loss:0.682 AVG Training Acc 57.30 % AVG Validation Acc 60.70 %\n",
      "New Best Accuracy found: 60.97%\n",
      "Epoch: 177\n",
      "Epoch:180/200 AVG Training Loss:0.675 AVG Validation Loss:0.682 AVG Training Acc 57.34 % AVG Validation Acc 60.70 %\n",
      "New Best Accuracy found: 61.10%\n",
      "Epoch: 187\n",
      "Epoch:190/200 AVG Training Loss:0.676 AVG Validation Loss:0.681 AVG Training Acc 56.76 % AVG Validation Acc 60.97 %\n",
      "New Best Accuracy found: 61.24%\n",
      "Epoch: 195\n",
      "New Best Accuracy found: 61.37%\n",
      "Epoch: 199\n",
      "Epoch:200/200 AVG Training Loss:0.676 AVG Validation Loss:0.680 AVG Training Acc 57.29 % AVG Validation Acc 61.24 %\n",
      "Split 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d844894fe5b64f339bc7d0b1fdcc7be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.443 AVG Validation Loss:4.790 AVG Training Acc 82.11 % AVG Validation Acc 20.05 %\n",
      "Epoch:20/200 AVG Training Loss:0.654 AVG Validation Loss:2.210 AVG Training Acc 65.43 % AVG Validation Acc 20.05 %\n",
      "Epoch:30/200 AVG Training Loss:0.609 AVG Validation Loss:6.553 AVG Training Acc 78.78 % AVG Validation Acc 20.05 %\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.705 AVG Validation Loss:0.872 AVG Training Acc 51.71 % AVG Validation Acc 20.05 %\n",
      "Epoch:50/200 AVG Training Loss:0.702 AVG Validation Loss:0.842 AVG Training Acc 51.69 % AVG Validation Acc 20.05 %\n",
      "Epoch:60/200 AVG Training Loss:0.699 AVG Validation Loss:0.801 AVG Training Acc 51.09 % AVG Validation Acc 20.05 %\n",
      "Epoch:70/200 AVG Training Loss:0.696 AVG Validation Loss:0.780 AVG Training Acc 50.25 % AVG Validation Acc 21.53 %\n",
      "Epoch:80/200 AVG Training Loss:0.694 AVG Validation Loss:0.773 AVG Training Acc 50.21 % AVG Validation Acc 21.94 %\n",
      "Epoch:90/200 AVG Training Loss:0.691 AVG Validation Loss:0.771 AVG Training Acc 51.75 % AVG Validation Acc 22.48 %\n",
      "Epoch    96: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:100/200 AVG Training Loss:0.687 AVG Validation Loss:0.756 AVG Training Acc 52.06 % AVG Validation Acc 24.36 %\n",
      "Epoch:110/200 AVG Training Loss:0.682 AVG Validation Loss:0.719 AVG Training Acc 54.29 % AVG Validation Acc 40.65 %\n",
      "Epoch:120/200 AVG Training Loss:0.678 AVG Validation Loss:0.703 AVG Training Acc 56.16 % AVG Validation Acc 53.70 %\n",
      "Epoch:130/200 AVG Training Loss:0.677 AVG Validation Loss:0.699 AVG Training Acc 55.64 % AVG Validation Acc 57.20 %\n",
      "Epoch:140/200 AVG Training Loss:0.676 AVG Validation Loss:0.695 AVG Training Acc 56.29 % AVG Validation Acc 58.95 %\n",
      "Epoch:150/200 AVG Training Loss:0.674 AVG Validation Loss:0.694 AVG Training Acc 57.12 % AVG Validation Acc 60.03 %\n",
      "Epoch:160/200 AVG Training Loss:0.675 AVG Validation Loss:0.694 AVG Training Acc 56.78 % AVG Validation Acc 60.03 %\n",
      "Epoch   164: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:170/200 AVG Training Loss:0.675 AVG Validation Loss:0.694 AVG Training Acc 56.58 % AVG Validation Acc 60.16 %\n",
      "Epoch:180/200 AVG Training Loss:0.673 AVG Validation Loss:0.693 AVG Training Acc 57.02 % AVG Validation Acc 60.57 %\n",
      "Epoch:190/200 AVG Training Loss:0.672 AVG Validation Loss:0.692 AVG Training Acc 57.41 % AVG Validation Acc 61.10 %\n",
      "New Best Accuracy found: 61.51%\n",
      "Epoch: 194\n",
      "New Best Accuracy found: 61.64%\n",
      "Epoch: 197\n",
      "Epoch:200/200 AVG Training Loss:0.674 AVG Validation Loss:0.691 AVG Training Acc 56.68 % AVG Validation Acc 61.64 %\n",
      "Split 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e44b8e212d470ea08dda30559b6d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.542 AVG Validation Loss:3.439 AVG Training Acc 79.99 % AVG Validation Acc 20.05 %\n",
      "Epoch:20/200 AVG Training Loss:0.634 AVG Validation Loss:1.785 AVG Training Acc 68.41 % AVG Validation Acc 20.05 %\n",
      "Epoch:30/200 AVG Training Loss:0.635 AVG Validation Loss:5.859 AVG Training Acc 71.51 % AVG Validation Acc 20.05 %\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.725 AVG Validation Loss:0.844 AVG Training Acc 50.49 % AVG Validation Acc 21.53 %\n",
      "Epoch:50/200 AVG Training Loss:0.695 AVG Validation Loss:0.756 AVG Training Acc 48.92 % AVG Validation Acc 23.01 %\n",
      "Epoch:60/200 AVG Training Loss:0.693 AVG Validation Loss:0.755 AVG Training Acc 49.87 % AVG Validation Acc 23.01 %\n",
      "Epoch:70/200 AVG Training Loss:0.690 AVG Validation Loss:0.754 AVG Training Acc 51.02 % AVG Validation Acc 23.42 %\n",
      "Epoch:80/200 AVG Training Loss:0.688 AVG Validation Loss:0.752 AVG Training Acc 52.27 % AVG Validation Acc 24.09 %\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:90/200 AVG Training Loss:0.684 AVG Validation Loss:0.742 AVG Training Acc 53.05 % AVG Validation Acc 25.98 %\n",
      "Epoch:100/200 AVG Training Loss:0.679 AVG Validation Loss:0.712 AVG Training Acc 54.95 % AVG Validation Acc 38.76 %\n",
      "Epoch:110/200 AVG Training Loss:0.678 AVG Validation Loss:0.698 AVG Training Acc 55.33 % AVG Validation Acc 49.26 %\n",
      "Epoch:120/200 AVG Training Loss:0.677 AVG Validation Loss:0.692 AVG Training Acc 55.74 % AVG Validation Acc 54.64 %\n",
      "Epoch:130/200 AVG Training Loss:0.677 AVG Validation Loss:0.688 AVG Training Acc 56.07 % AVG Validation Acc 57.20 %\n",
      "Epoch:140/200 AVG Training Loss:0.675 AVG Validation Loss:0.687 AVG Training Acc 56.37 % AVG Validation Acc 58.28 %\n",
      "Epoch:150/200 AVG Training Loss:0.676 AVG Validation Loss:0.687 AVG Training Acc 56.26 % AVG Validation Acc 58.82 %\n",
      "Epoch:160/200 AVG Training Loss:0.674 AVG Validation Loss:0.686 AVG Training Acc 56.13 % AVG Validation Acc 59.08 %\n",
      "Epoch:170/200 AVG Training Loss:0.675 AVG Validation Loss:0.686 AVG Training Acc 56.59 % AVG Validation Acc 59.76 %\n",
      "Epoch   172: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:180/200 AVG Training Loss:0.675 AVG Validation Loss:0.685 AVG Training Acc 56.65 % AVG Validation Acc 59.89 %\n",
      "Epoch:190/200 AVG Training Loss:0.674 AVG Validation Loss:0.685 AVG Training Acc 56.76 % AVG Validation Acc 60.03 %\n",
      "Epoch:200/200 AVG Training Loss:0.673 AVG Validation Loss:0.684 AVG Training Acc 56.85 % AVG Validation Acc 60.43 %\n",
      "Split 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f6ca47f9da401f928711c0cf7dd5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.462 AVG Validation Loss:6.153 AVG Training Acc 82.24 % AVG Validation Acc 20.19 %\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:20/200 AVG Training Loss:2.097 AVG Validation Loss:1.529 AVG Training Acc 52.16 % AVG Validation Acc 20.19 %\n",
      "Epoch:30/200 AVG Training Loss:0.701 AVG Validation Loss:0.805 AVG Training Acc 50.46 % AVG Validation Acc 20.59 %\n",
      "Epoch:40/200 AVG Training Loss:0.697 AVG Validation Loss:0.784 AVG Training Acc 50.52 % AVG Validation Acc 21.53 %\n",
      "Epoch:50/200 AVG Training Loss:0.692 AVG Validation Loss:0.778 AVG Training Acc 50.65 % AVG Validation Acc 22.21 %\n",
      "Epoch:60/200 AVG Training Loss:0.690 AVG Validation Loss:0.771 AVG Training Acc 51.11 % AVG Validation Acc 23.28 %\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.686 AVG Validation Loss:0.750 AVG Training Acc 52.57 % AVG Validation Acc 26.78 %\n",
      "Epoch:80/200 AVG Training Loss:0.681 AVG Validation Loss:0.723 AVG Training Acc 53.97 % AVG Validation Acc 37.15 %\n",
      "Epoch:90/200 AVG Training Loss:0.679 AVG Validation Loss:0.712 AVG Training Acc 54.47 % AVG Validation Acc 43.74 %\n",
      "Epoch:100/200 AVG Training Loss:0.677 AVG Validation Loss:0.708 AVG Training Acc 55.12 % AVG Validation Acc 47.38 %\n",
      "Epoch:110/200 AVG Training Loss:0.678 AVG Validation Loss:0.710 AVG Training Acc 55.33 % AVG Validation Acc 49.39 %\n",
      "Epoch   111: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:120/200 AVG Training Loss:0.677 AVG Validation Loss:0.709 AVG Training Acc 55.17 % AVG Validation Acc 50.20 %\n",
      "Epoch:130/200 AVG Training Loss:0.676 AVG Validation Loss:0.708 AVG Training Acc 55.43 % AVG Validation Acc 50.61 %\n",
      "Epoch:140/200 AVG Training Loss:0.676 AVG Validation Loss:0.708 AVG Training Acc 55.71 % AVG Validation Acc 51.55 %\n",
      "Epoch:150/200 AVG Training Loss:0.676 AVG Validation Loss:0.707 AVG Training Acc 55.38 % AVG Validation Acc 52.36 %\n",
      "Epoch   153: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:160/200 AVG Training Loss:0.677 AVG Validation Loss:0.707 AVG Training Acc 55.46 % AVG Validation Acc 52.36 %\n",
      "Epoch:170/200 AVG Training Loss:0.676 AVG Validation Loss:0.707 AVG Training Acc 55.85 % AVG Validation Acc 52.49 %\n",
      "Epoch:180/200 AVG Training Loss:0.677 AVG Validation Loss:0.707 AVG Training Acc 55.36 % AVG Validation Acc 52.76 %\n",
      "Epoch   184: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:190/200 AVG Training Loss:0.675 AVG Validation Loss:0.707 AVG Training Acc 55.47 % AVG Validation Acc 52.62 %\n",
      "Epoch:200/200 AVG Training Loss:0.677 AVG Validation Loss:0.707 AVG Training Acc 55.53 % AVG Validation Acc 52.76 %\n",
      "final_fail\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440cbb6fab39459f97c5a13ebd58770c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513ec1ef3b84493abd90b4d059d5cdf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best Accuracy found: 14.92%\n",
      "Epoch: 1\n",
      "Epoch:10/200 AVG Training Loss:0.624 AVG Validation Loss:1.889 AVG Training Acc 69.28 % AVG Validation Acc 14.92 %\n",
      "Epoch:20/200 AVG Training Loss:0.574 AVG Validation Loss:4.359 AVG Training Acc 78.17 % AVG Validation Acc 14.92 %\n",
      "Epoch:30/200 AVG Training Loss:0.556 AVG Validation Loss:4.033 AVG Training Acc 82.16 % AVG Validation Acc 14.92 %\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.834 AVG Validation Loss:1.090 AVG Training Acc 50.11 % AVG Validation Acc 15.19 %\n",
      "New Best Accuracy found: 15.19%\n",
      "Epoch: 40\n",
      "New Best Accuracy found: 15.73%\n",
      "Epoch: 41\n",
      "New Best Accuracy found: 16.67%\n",
      "Epoch: 42\n",
      "New Best Accuracy found: 17.07%\n",
      "Epoch: 43\n",
      "New Best Accuracy found: 17.34%\n",
      "Epoch: 44\n",
      "New Best Accuracy found: 18.01%\n",
      "Epoch: 45\n",
      "New Best Accuracy found: 18.28%\n",
      "Epoch: 46\n",
      "New Best Accuracy found: 18.41%\n",
      "Epoch: 47\n",
      "Epoch:50/200 AVG Training Loss:0.697 AVG Validation Loss:0.772 AVG Training Acc 49.20 % AVG Validation Acc 18.41 %\n",
      "New Best Accuracy found: 18.55%\n",
      "Epoch: 53\n",
      "New Best Accuracy found: 18.68%\n",
      "Epoch: 54\n",
      "New Best Accuracy found: 19.49%\n",
      "Epoch: 58\n",
      "New Best Accuracy found: 19.62%\n",
      "Epoch: 59\n",
      "Epoch:60/200 AVG Training Loss:0.690 AVG Validation Loss:0.780 AVG Training Acc 52.98 % AVG Validation Acc 18.95 %\n",
      "New Best Accuracy found: 21.10%\n",
      "Epoch: 64\n",
      "New Best Accuracy found: 21.91%\n",
      "Epoch: 66\n",
      "New Best Accuracy found: 24.46%\n",
      "Epoch: 67\n",
      "New Best Accuracy found: 25.00%\n",
      "Epoch: 68\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.689 AVG Validation Loss:0.775 AVG Training Acc 54.11 % AVG Validation Acc 29.17 %\n",
      "New Best Accuracy found: 29.17%\n",
      "Epoch: 70\n",
      "New Best Accuracy found: 33.06%\n",
      "Epoch: 71\n",
      "New Best Accuracy found: 35.89%\n",
      "Epoch: 72\n",
      "New Best Accuracy found: 39.52%\n",
      "Epoch: 73\n",
      "New Best Accuracy found: 41.40%\n",
      "Epoch: 74\n",
      "New Best Accuracy found: 44.49%\n",
      "Epoch: 75\n",
      "New Best Accuracy found: 45.70%\n",
      "Epoch: 76\n",
      "New Best Accuracy found: 48.25%\n",
      "Epoch: 77\n",
      "New Best Accuracy found: 50.13%\n",
      "Epoch: 78\n",
      "New Best Accuracy found: 51.61%\n",
      "Epoch: 79\n",
      "Epoch:80/200 AVG Training Loss:0.680 AVG Validation Loss:0.721 AVG Training Acc 54.71 % AVG Validation Acc 53.23 %\n",
      "New Best Accuracy found: 53.23%\n",
      "Epoch: 80\n",
      "New Best Accuracy found: 54.44%\n",
      "Epoch: 81\n",
      "New Best Accuracy found: 55.65%\n",
      "Epoch: 82\n",
      "New Best Accuracy found: 56.72%\n",
      "Epoch: 83\n",
      "New Best Accuracy found: 57.80%\n",
      "Epoch: 84\n",
      "New Best Accuracy found: 58.33%\n",
      "Epoch: 85\n",
      "New Best Accuracy found: 59.14%\n",
      "Epoch: 86\n",
      "New Best Accuracy found: 59.95%\n",
      "Epoch: 87\n",
      "New Best Accuracy found: 60.75%\n",
      "Epoch: 88\n",
      "New Best Accuracy found: 61.56%\n",
      "Epoch: 89\n",
      "Epoch:90/200 AVG Training Loss:0.677 AVG Validation Loss:0.695 AVG Training Acc 56.51 % AVG Validation Acc 62.10 %\n",
      "New Best Accuracy found: 62.10%\n",
      "Epoch: 90\n",
      "New Best Accuracy found: 62.37%\n",
      "Epoch: 92\n",
      "New Best Accuracy found: 62.77%\n",
      "Epoch: 93\n",
      "New Best Accuracy found: 63.58%\n",
      "Epoch: 94\n",
      "New Best Accuracy found: 64.25%\n",
      "Epoch: 95\n",
      "New Best Accuracy found: 64.78%\n",
      "Epoch: 97\n",
      "New Best Accuracy found: 64.92%\n",
      "Epoch: 98\n",
      "New Best Accuracy found: 65.05%\n",
      "Epoch: 99\n",
      "Epoch:100/200 AVG Training Loss:0.675 AVG Validation Loss:0.687 AVG Training Acc 56.93 % AVG Validation Acc 65.46 %\n",
      "New Best Accuracy found: 65.46%\n",
      "Epoch: 100\n",
      "New Best Accuracy found: 65.59%\n",
      "Epoch: 103\n",
      "New Best Accuracy found: 65.73%\n",
      "Epoch: 105\n",
      "New Best Accuracy found: 65.99%\n",
      "Epoch: 106\n",
      "New Best Accuracy found: 66.13%\n",
      "Epoch: 107\n",
      "New Best Accuracy found: 66.53%\n",
      "Epoch: 108\n",
      "New Best Accuracy found: 67.07%\n",
      "Epoch: 109\n",
      "Epoch:110/200 AVG Training Loss:0.673 AVG Validation Loss:0.685 AVG Training Acc 57.61 % AVG Validation Acc 66.67 %\n",
      "Epoch:120/200 AVG Training Loss:0.674 AVG Validation Loss:0.685 AVG Training Acc 57.25 % AVG Validation Acc 66.80 %\n",
      "Epoch   122: reducing learning rate of group 0 to 1.0000e-05.\n",
      "New Best Accuracy found: 67.20%\n",
      "Epoch: 124\n",
      "Epoch:130/200 AVG Training Loss:0.673 AVG Validation Loss:0.683 AVG Training Acc 57.24 % AVG Validation Acc 67.34 %\n",
      "New Best Accuracy found: 67.34%\n",
      "Epoch: 130\n",
      "New Best Accuracy found: 67.47%\n",
      "Epoch: 132\n",
      "New Best Accuracy found: 67.88%\n",
      "Epoch: 136\n",
      "Epoch:140/200 AVG Training Loss:0.672 AVG Validation Loss:0.682 AVG Training Acc 57.27 % AVG Validation Acc 67.34 %\n",
      "Epoch:150/200 AVG Training Loss:0.672 AVG Validation Loss:0.680 AVG Training Acc 57.45 % AVG Validation Acc 67.47 %\n",
      "New Best Accuracy found: 68.01%\n",
      "Epoch: 154\n",
      "New Best Accuracy found: 68.15%\n",
      "Epoch: 155\n",
      "Epoch:160/200 AVG Training Loss:0.672 AVG Validation Loss:0.679 AVG Training Acc 57.79 % AVG Validation Acc 68.28 %\n",
      "New Best Accuracy found: 68.28%\n",
      "Epoch: 160\n",
      "Epoch:170/200 AVG Training Loss:0.672 AVG Validation Loss:0.678 AVG Training Acc 57.19 % AVG Validation Acc 68.01 %\n",
      "New Best Accuracy found: 68.55%\n",
      "Epoch: 177\n",
      "Epoch:180/200 AVG Training Loss:0.671 AVG Validation Loss:0.677 AVG Training Acc 57.40 % AVG Validation Acc 68.28 %\n",
      "New Best Accuracy found: 68.68%\n",
      "Epoch: 184\n",
      "Epoch:190/200 AVG Training Loss:0.671 AVG Validation Loss:0.676 AVG Training Acc 58.11 % AVG Validation Acc 68.55 %\n",
      "Epoch:200/200 AVG Training Loss:0.672 AVG Validation Loss:0.676 AVG Training Acc 58.09 % AVG Validation Acc 68.55 %\n",
      "Split 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25bb08b189864c059b132eedb36cae1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.509 AVG Validation Loss:5.683 AVG Training Acc 78.05 % AVG Validation Acc 15.05 %\n",
      "Epoch:20/200 AVG Training Loss:0.639 AVG Validation Loss:1.883 AVG Training Acc 67.59 % AVG Validation Acc 15.05 %\n",
      "Epoch:30/200 AVG Training Loss:0.582 AVG Validation Loss:6.381 AVG Training Acc 83.83 % AVG Validation Acc 15.05 %\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.709 AVG Validation Loss:0.809 AVG Training Acc 47.59 % AVG Validation Acc 15.59 %\n",
      "Epoch:50/200 AVG Training Loss:0.698 AVG Validation Loss:0.771 AVG Training Acc 49.02 % AVG Validation Acc 17.74 %\n",
      "Epoch:60/200 AVG Training Loss:0.694 AVG Validation Loss:0.771 AVG Training Acc 50.19 % AVG Validation Acc 18.95 %\n",
      "Epoch:70/200 AVG Training Loss:0.690 AVG Validation Loss:0.776 AVG Training Acc 51.69 % AVG Validation Acc 19.76 %\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.682 AVG Validation Loss:0.740 AVG Training Acc 54.90 % AVG Validation Acc 25.54 %\n",
      "Epoch:90/200 AVG Training Loss:0.678 AVG Validation Loss:0.712 AVG Training Acc 55.78 % AVG Validation Acc 45.97 %\n",
      "Epoch:100/200 AVG Training Loss:0.674 AVG Validation Loss:0.697 AVG Training Acc 56.28 % AVG Validation Acc 53.63 %\n",
      "Epoch:110/200 AVG Training Loss:0.673 AVG Validation Loss:0.689 AVG Training Acc 56.79 % AVG Validation Acc 57.80 %\n",
      "Epoch:120/200 AVG Training Loss:0.672 AVG Validation Loss:0.684 AVG Training Acc 57.73 % AVG Validation Acc 59.81 %\n",
      "Epoch:130/200 AVG Training Loss:0.671 AVG Validation Loss:0.681 AVG Training Acc 58.12 % AVG Validation Acc 61.69 %\n",
      "Epoch:140/200 AVG Training Loss:0.671 AVG Validation Loss:0.679 AVG Training Acc 58.09 % AVG Validation Acc 62.50 %\n",
      "Epoch:150/200 AVG Training Loss:0.670 AVG Validation Loss:0.678 AVG Training Acc 58.23 % AVG Validation Acc 63.71 %\n",
      "Epoch:160/200 AVG Training Loss:0.668 AVG Validation Loss:0.678 AVG Training Acc 58.90 % AVG Validation Acc 64.52 %\n",
      "Epoch   165: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:170/200 AVG Training Loss:0.665 AVG Validation Loss:0.676 AVG Training Acc 58.78 % AVG Validation Acc 64.92 %\n",
      "Epoch:180/200 AVG Training Loss:0.664 AVG Validation Loss:0.675 AVG Training Acc 59.19 % AVG Validation Acc 65.46 %\n",
      "Epoch:190/200 AVG Training Loss:0.665 AVG Validation Loss:0.672 AVG Training Acc 58.77 % AVG Validation Acc 66.67 %\n",
      "Epoch:200/200 AVG Training Loss:0.666 AVG Validation Loss:0.671 AVG Training Acc 58.98 % AVG Validation Acc 66.13 %\n",
      "Split 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb801eeb92e243acb22c21621aecd9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.405 AVG Validation Loss:5.401 AVG Training Acc 85.61 % AVG Validation Acc 15.19 %\n",
      "Epoch:20/200 AVG Training Loss:0.630 AVG Validation Loss:1.888 AVG Training Acc 69.55 % AVG Validation Acc 15.05 %\n",
      "Epoch:30/200 AVG Training Loss:0.588 AVG Validation Loss:10.570 AVG Training Acc 82.77 % AVG Validation Acc 15.05 %\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.769 AVG Validation Loss:0.967 AVG Training Acc 49.99 % AVG Validation Acc 15.19 %\n",
      "Epoch:50/200 AVG Training Loss:0.698 AVG Validation Loss:0.774 AVG Training Acc 49.09 % AVG Validation Acc 16.40 %\n",
      "Epoch:60/200 AVG Training Loss:0.694 AVG Validation Loss:0.768 AVG Training Acc 50.24 % AVG Validation Acc 17.61 %\n",
      "Epoch:70/200 AVG Training Loss:0.691 AVG Validation Loss:0.769 AVG Training Acc 51.84 % AVG Validation Acc 18.01 %\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.687 AVG Validation Loss:0.748 AVG Training Acc 54.17 % AVG Validation Acc 21.10 %\n",
      "Epoch:90/200 AVG Training Loss:0.681 AVG Validation Loss:0.716 AVG Training Acc 55.46 % AVG Validation Acc 39.92 %\n",
      "Epoch:100/200 AVG Training Loss:0.679 AVG Validation Loss:0.700 AVG Training Acc 56.02 % AVG Validation Acc 51.61 %\n",
      "Epoch:110/200 AVG Training Loss:0.679 AVG Validation Loss:0.692 AVG Training Acc 55.83 % AVG Validation Acc 55.91 %\n",
      "Epoch:120/200 AVG Training Loss:0.676 AVG Validation Loss:0.688 AVG Training Acc 56.33 % AVG Validation Acc 58.47 %\n",
      "Epoch:130/200 AVG Training Loss:0.674 AVG Validation Loss:0.686 AVG Training Acc 56.79 % AVG Validation Acc 60.62 %\n",
      "Epoch:140/200 AVG Training Loss:0.673 AVG Validation Loss:0.684 AVG Training Acc 57.60 % AVG Validation Acc 62.10 %\n",
      "Epoch:150/200 AVG Training Loss:0.671 AVG Validation Loss:0.684 AVG Training Acc 58.24 % AVG Validation Acc 63.31 %\n",
      "Epoch:160/200 AVG Training Loss:0.669 AVG Validation Loss:0.683 AVG Training Acc 58.81 % AVG Validation Acc 63.98 %\n",
      "Epoch   167: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:170/200 AVG Training Loss:0.668 AVG Validation Loss:0.683 AVG Training Acc 58.50 % AVG Validation Acc 63.84 %\n",
      "Epoch:180/200 AVG Training Loss:0.668 AVG Validation Loss:0.681 AVG Training Acc 58.78 % AVG Validation Acc 63.98 %\n",
      "Epoch:190/200 AVG Training Loss:0.666 AVG Validation Loss:0.679 AVG Training Acc 58.87 % AVG Validation Acc 64.65 %\n",
      "Epoch:200/200 AVG Training Loss:0.667 AVG Validation Loss:0.678 AVG Training Acc 59.21 % AVG Validation Acc 64.78 %\n",
      "Split 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a57852f86547ce9ea7557a78770071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.640 AVG Validation Loss:1.780 AVG Training Acc 67.53 % AVG Validation Acc 15.05 %\n",
      "Epoch:20/200 AVG Training Loss:0.645 AVG Validation Loss:1.743 AVG Training Acc 67.19 % AVG Validation Acc 15.05 %\n",
      "Epoch:30/200 AVG Training Loss:0.558 AVG Validation Loss:10.176 AVG Training Acc 77.73 % AVG Validation Acc 15.05 %\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.716 AVG Validation Loss:0.845 AVG Training Acc 49.23 % AVG Validation Acc 16.67 %\n",
      "Epoch:50/200 AVG Training Loss:0.696 AVG Validation Loss:0.793 AVG Training Acc 50.71 % AVG Validation Acc 16.26 %\n",
      "Epoch:60/200 AVG Training Loss:0.691 AVG Validation Loss:0.796 AVG Training Acc 53.60 % AVG Validation Acc 18.82 %\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.688 AVG Validation Loss:0.766 AVG Training Acc 53.34 % AVG Validation Acc 26.75 %\n",
      "Epoch:80/200 AVG Training Loss:0.677 AVG Validation Loss:0.707 AVG Training Acc 57.25 % AVG Validation Acc 44.22 %\n",
      "Epoch:90/200 AVG Training Loss:0.675 AVG Validation Loss:0.696 AVG Training Acc 57.61 % AVG Validation Acc 47.18 %\n",
      "Epoch:100/200 AVG Training Loss:0.675 AVG Validation Loss:0.693 AVG Training Acc 57.83 % AVG Validation Acc 48.12 %\n",
      "Epoch:110/200 AVG Training Loss:0.673 AVG Validation Loss:0.692 AVG Training Acc 58.22 % AVG Validation Acc 48.39 %\n",
      "Epoch:120/200 AVG Training Loss:0.673 AVG Validation Loss:0.692 AVG Training Acc 58.36 % AVG Validation Acc 48.52 %\n",
      "Epoch:130/200 AVG Training Loss:0.672 AVG Validation Loss:0.691 AVG Training Acc 58.65 % AVG Validation Acc 49.60 %\n",
      "Epoch:140/200 AVG Training Loss:0.672 AVG Validation Loss:0.691 AVG Training Acc 58.07 % AVG Validation Acc 50.00 %\n",
      "Epoch:150/200 AVG Training Loss:0.671 AVG Validation Loss:0.691 AVG Training Acc 57.97 % AVG Validation Acc 50.27 %\n",
      "Epoch   153: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:160/200 AVG Training Loss:0.670 AVG Validation Loss:0.689 AVG Training Acc 58.28 % AVG Validation Acc 50.94 %\n",
      "Epoch:170/200 AVG Training Loss:0.668 AVG Validation Loss:0.687 AVG Training Acc 58.42 % AVG Validation Acc 50.94 %\n",
      "Epoch:180/200 AVG Training Loss:0.671 AVG Validation Loss:0.685 AVG Training Acc 58.67 % AVG Validation Acc 51.21 %\n",
      "Epoch:190/200 AVG Training Loss:0.668 AVG Validation Loss:0.684 AVG Training Acc 58.46 % AVG Validation Acc 51.61 %\n",
      "Epoch:200/200 AVG Training Loss:0.668 AVG Validation Loss:0.683 AVG Training Acc 58.53 % AVG Validation Acc 51.75 %\n",
      "Split 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6fc076a7ebf454faa8eeda535abeac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.539 AVG Validation Loss:5.874 AVG Training Acc 76.74 % AVG Validation Acc 15.05 %\n",
      "Epoch:20/200 AVG Training Loss:0.633 AVG Validation Loss:1.870 AVG Training Acc 69.17 % AVG Validation Acc 15.05 %\n",
      "Epoch:30/200 AVG Training Loss:0.616 AVG Validation Loss:3.612 AVG Training Acc 72.68 % AVG Validation Acc 15.05 %\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.970 AVG Validation Loss:1.315 AVG Training Acc 50.00 % AVG Validation Acc 15.05 %\n",
      "Epoch:50/200 AVG Training Loss:0.697 AVG Validation Loss:0.772 AVG Training Acc 49.82 % AVG Validation Acc 17.74 %\n",
      "Epoch:60/200 AVG Training Loss:0.692 AVG Validation Loss:0.779 AVG Training Acc 52.72 % AVG Validation Acc 18.41 %\n",
      "Epoch:70/200 AVG Training Loss:0.684 AVG Validation Loss:0.797 AVG Training Acc 56.31 % AVG Validation Acc 18.82 %\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.678 AVG Validation Loss:0.714 AVG Training Acc 56.04 % AVG Validation Acc 55.65 %\n",
      "Epoch:90/200 AVG Training Loss:0.673 AVG Validation Loss:0.686 AVG Training Acc 57.93 % AVG Validation Acc 62.90 %\n",
      "Epoch:100/200 AVG Training Loss:0.672 AVG Validation Loss:0.677 AVG Training Acc 58.63 % AVG Validation Acc 64.25 %\n",
      "Epoch:110/200 AVG Training Loss:0.672 AVG Validation Loss:0.674 AVG Training Acc 59.16 % AVG Validation Acc 65.05 %\n",
      "Epoch:120/200 AVG Training Loss:0.671 AVG Validation Loss:0.673 AVG Training Acc 58.97 % AVG Validation Acc 64.92 %\n",
      "Epoch:130/200 AVG Training Loss:0.670 AVG Validation Loss:0.673 AVG Training Acc 58.83 % AVG Validation Acc 64.52 %\n",
      "Epoch   135: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:140/200 AVG Training Loss:0.668 AVG Validation Loss:0.672 AVG Training Acc 59.22 % AVG Validation Acc 65.05 %\n",
      "Epoch:150/200 AVG Training Loss:0.667 AVG Validation Loss:0.670 AVG Training Acc 59.15 % AVG Validation Acc 65.05 %\n",
      "Epoch:160/200 AVG Training Loss:0.668 AVG Validation Loss:0.668 AVG Training Acc 58.99 % AVG Validation Acc 65.73 %\n",
      "Epoch:170/200 AVG Training Loss:0.668 AVG Validation Loss:0.667 AVG Training Acc 59.45 % AVG Validation Acc 65.46 %\n",
      "Epoch:180/200 AVG Training Loss:0.668 AVG Validation Loss:0.666 AVG Training Acc 59.69 % AVG Validation Acc 65.99 %\n",
      "Epoch:190/200 AVG Training Loss:0.669 AVG Validation Loss:0.665 AVG Training Acc 59.65 % AVG Validation Acc 65.99 %\n",
      "Epoch:200/200 AVG Training Loss:0.667 AVG Validation Loss:0.664 AVG Training Acc 59.39 % AVG Validation Acc 65.99 %\n",
      "Split 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b0ae28b138485ebb8cc7df15bff771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.598 AVG Validation Loss:2.111 AVG Training Acc 72.60 % AVG Validation Acc 15.05 %\n",
      "Epoch:20/200 AVG Training Loss:0.591 AVG Validation Loss:3.861 AVG Training Acc 82.33 % AVG Validation Acc 15.05 %\n",
      "Epoch:30/200 AVG Training Loss:0.774 AVG Validation Loss:2.779 AVG Training Acc 72.57 % AVG Validation Acc 15.05 %\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.836 AVG Validation Loss:1.094 AVG Training Acc 50.10 % AVG Validation Acc 15.46 %\n",
      "Epoch:50/200 AVG Training Loss:0.700 AVG Validation Loss:0.773 AVG Training Acc 48.80 % AVG Validation Acc 16.67 %\n",
      "Epoch:60/200 AVG Training Loss:0.695 AVG Validation Loss:0.780 AVG Training Acc 50.52 % AVG Validation Acc 20.16 %\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.694 AVG Validation Loss:0.788 AVG Training Acc 52.63 % AVG Validation Acc 29.30 %\n",
      "Epoch:80/200 AVG Training Loss:0.684 AVG Validation Loss:0.741 AVG Training Acc 53.45 % AVG Validation Acc 35.62 %\n",
      "Epoch:90/200 AVG Training Loss:0.683 AVG Validation Loss:0.720 AVG Training Acc 55.13 % AVG Validation Acc 41.80 %\n",
      "Epoch:100/200 AVG Training Loss:0.681 AVG Validation Loss:0.709 AVG Training Acc 56.20 % AVG Validation Acc 47.18 %\n",
      "Epoch:110/200 AVG Training Loss:0.680 AVG Validation Loss:0.703 AVG Training Acc 56.16 % AVG Validation Acc 50.00 %\n",
      "Epoch:120/200 AVG Training Loss:0.680 AVG Validation Loss:0.699 AVG Training Acc 57.03 % AVG Validation Acc 52.96 %\n",
      "Epoch:130/200 AVG Training Loss:0.679 AVG Validation Loss:0.691 AVG Training Acc 57.23 % AVG Validation Acc 53.90 %\n",
      "Epoch:140/200 AVG Training Loss:0.677 AVG Validation Loss:0.691 AVG Training Acc 57.51 % AVG Validation Acc 55.78 %\n",
      "Epoch:150/200 AVG Training Loss:0.677 AVG Validation Loss:0.693 AVG Training Acc 56.94 % AVG Validation Acc 57.66 %\n",
      "Epoch   155: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:160/200 AVG Training Loss:0.676 AVG Validation Loss:0.691 AVG Training Acc 57.60 % AVG Validation Acc 58.60 %\n",
      "Epoch:170/200 AVG Training Loss:0.675 AVG Validation Loss:0.693 AVG Training Acc 57.94 % AVG Validation Acc 59.27 %\n",
      "Epoch:180/200 AVG Training Loss:0.675 AVG Validation Loss:0.691 AVG Training Acc 57.55 % AVG Validation Acc 59.81 %\n",
      "Epoch   186: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:190/200 AVG Training Loss:0.674 AVG Validation Loss:0.690 AVG Training Acc 57.75 % AVG Validation Acc 60.22 %\n",
      "Epoch:200/200 AVG Training Loss:0.675 AVG Validation Loss:0.690 AVG Training Acc 57.70 % AVG Validation Acc 60.22 %\n",
      "Split 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41389b60865d44c0b5822ae0e193ec4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.484 AVG Validation Loss:6.149 AVG Training Acc 83.20 % AVG Validation Acc 14.94 %\n",
      "Epoch:20/200 AVG Training Loss:0.633 AVG Validation Loss:3.884 AVG Training Acc 66.35 % AVG Validation Acc 14.94 %\n",
      "Epoch:30/200 AVG Training Loss:0.567 AVG Validation Loss:3.137 AVG Training Acc 78.06 % AVG Validation Acc 14.94 %\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.698 AVG Validation Loss:1.340 AVG Training Acc 54.56 % AVG Validation Acc 14.94 %\n",
      "Epoch:50/200 AVG Training Loss:0.700 AVG Validation Loss:0.970 AVG Training Acc 55.94 % AVG Validation Acc 15.21 %\n",
      "Epoch:60/200 AVG Training Loss:0.698 AVG Validation Loss:0.910 AVG Training Acc 54.54 % AVG Validation Acc 15.34 %\n",
      "Epoch:70/200 AVG Training Loss:0.686 AVG Validation Loss:0.957 AVG Training Acc 55.96 % AVG Validation Acc 15.48 %\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.718 AVG Validation Loss:0.879 AVG Training Acc 50.54 % AVG Validation Acc 16.15 %\n",
      "Epoch:90/200 AVG Training Loss:0.675 AVG Validation Loss:0.729 AVG Training Acc 56.06 % AVG Validation Acc 42.26 %\n",
      "Epoch:100/200 AVG Training Loss:0.671 AVG Validation Loss:0.709 AVG Training Acc 56.64 % AVG Validation Acc 47.38 %\n",
      "Epoch:110/200 AVG Training Loss:0.671 AVG Validation Loss:0.703 AVG Training Acc 56.71 % AVG Validation Acc 49.53 %\n",
      "Epoch:120/200 AVG Training Loss:0.668 AVG Validation Loss:0.700 AVG Training Acc 57.17 % AVG Validation Acc 50.87 %\n",
      "Epoch:130/200 AVG Training Loss:0.668 AVG Validation Loss:0.697 AVG Training Acc 57.65 % AVG Validation Acc 52.22 %\n",
      "Epoch:140/200 AVG Training Loss:0.667 AVG Validation Loss:0.698 AVG Training Acc 57.66 % AVG Validation Acc 53.30 %\n",
      "Epoch:150/200 AVG Training Loss:0.665 AVG Validation Loss:0.698 AVG Training Acc 57.73 % AVG Validation Acc 55.32 %\n",
      "Epoch   158: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:160/200 AVG Training Loss:0.664 AVG Validation Loss:0.693 AVG Training Acc 58.11 % AVG Validation Acc 57.34 %\n",
      "Epoch:170/200 AVG Training Loss:0.665 AVG Validation Loss:0.689 AVG Training Acc 58.64 % AVG Validation Acc 58.55 %\n",
      "Epoch:180/200 AVG Training Loss:0.665 AVG Validation Loss:0.682 AVG Training Acc 58.71 % AVG Validation Acc 59.08 %\n",
      "Epoch:190/200 AVG Training Loss:0.663 AVG Validation Loss:0.681 AVG Training Acc 59.03 % AVG Validation Acc 60.03 %\n",
      "Epoch:200/200 AVG Training Loss:0.663 AVG Validation Loss:0.679 AVG Training Acc 58.57 % AVG Validation Acc 60.70 %\n",
      "Split 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a4b3900312484aba4377c42d703809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.455 AVG Validation Loss:5.774 AVG Training Acc 85.60 % AVG Validation Acc 14.94 %\n",
      "Epoch:20/200 AVG Training Loss:0.645 AVG Validation Loss:1.731 AVG Training Acc 67.09 % AVG Validation Acc 14.94 %\n",
      "Epoch:30/200 AVG Training Loss:0.523 AVG Validation Loss:5.991 AVG Training Acc 85.64 % AVG Validation Acc 14.94 %\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.737 AVG Validation Loss:0.891 AVG Training Acc 50.01 % AVG Validation Acc 16.02 %\n",
      "Epoch:50/200 AVG Training Loss:0.701 AVG Validation Loss:0.777 AVG Training Acc 48.48 % AVG Validation Acc 15.88 %\n",
      "Epoch:60/200 AVG Training Loss:0.700 AVG Validation Loss:0.769 AVG Training Acc 48.07 % AVG Validation Acc 15.21 %\n",
      "Epoch:70/200 AVG Training Loss:0.698 AVG Validation Loss:0.764 AVG Training Acc 48.99 % AVG Validation Acc 16.15 %\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.695 AVG Validation Loss:0.761 AVG Training Acc 50.98 % AVG Validation Acc 17.63 %\n",
      "Epoch:90/200 AVG Training Loss:0.691 AVG Validation Loss:0.727 AVG Training Acc 51.79 % AVG Validation Acc 21.67 %\n",
      "Epoch:100/200 AVG Training Loss:0.689 AVG Validation Loss:0.706 AVG Training Acc 53.15 % AVG Validation Acc 31.63 %\n",
      "Epoch:110/200 AVG Training Loss:0.687 AVG Validation Loss:0.694 AVG Training Acc 54.50 % AVG Validation Acc 44.55 %\n",
      "Epoch:120/200 AVG Training Loss:0.685 AVG Validation Loss:0.687 AVG Training Acc 55.35 % AVG Validation Acc 53.97 %\n",
      "Epoch:130/200 AVG Training Loss:0.684 AVG Validation Loss:0.683 AVG Training Acc 56.09 % AVG Validation Acc 59.35 %\n",
      "Epoch:140/200 AVG Training Loss:0.685 AVG Validation Loss:0.681 AVG Training Acc 56.22 % AVG Validation Acc 63.66 %\n",
      "Epoch:150/200 AVG Training Loss:0.683 AVG Validation Loss:0.680 AVG Training Acc 56.07 % AVG Validation Acc 65.14 %\n",
      "Epoch:160/200 AVG Training Loss:0.680 AVG Validation Loss:0.679 AVG Training Acc 57.10 % AVG Validation Acc 67.29 %\n",
      "Epoch:170/200 AVG Training Loss:0.679 AVG Validation Loss:0.679 AVG Training Acc 56.85 % AVG Validation Acc 67.70 %\n",
      "Epoch:180/200 AVG Training Loss:0.678 AVG Validation Loss:0.680 AVG Training Acc 57.22 % AVG Validation Acc 67.29 %\n",
      "Epoch   181: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:190/200 AVG Training Loss:0.678 AVG Validation Loss:0.679 AVG Training Acc 56.49 % AVG Validation Acc 68.64 %\n",
      "New Best Accuracy found: 68.78%\n",
      "Epoch: 192\n",
      "New Best Accuracy found: 69.18%\n",
      "Epoch: 198\n",
      "Epoch:200/200 AVG Training Loss:0.677 AVG Validation Loss:0.677 AVG Training Acc 56.69 % AVG Validation Acc 69.04 %\n",
      "Split 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8deda896d83340778adef89857d77f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.486 AVG Validation Loss:9.806 AVG Training Acc 83.64 % AVG Validation Acc 14.94 %\n",
      "Epoch:20/200 AVG Training Loss:0.652 AVG Validation Loss:1.633 AVG Training Acc 65.68 % AVG Validation Acc 14.94 %\n",
      "Epoch:30/200 AVG Training Loss:0.713 AVG Validation Loss:5.550 AVG Training Acc 76.79 % AVG Validation Acc 14.94 %\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.702 AVG Validation Loss:0.811 AVG Training Acc 49.62 % AVG Validation Acc 15.61 %\n",
      "Epoch:50/200 AVG Training Loss:0.699 AVG Validation Loss:0.792 AVG Training Acc 50.16 % AVG Validation Acc 15.75 %\n",
      "Epoch:60/200 AVG Training Loss:0.693 AVG Validation Loss:0.784 AVG Training Acc 52.35 % AVG Validation Acc 16.15 %\n",
      "Epoch:70/200 AVG Training Loss:0.685 AVG Validation Loss:0.794 AVG Training Acc 55.62 % AVG Validation Acc 20.05 %\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:80/200 AVG Training Loss:0.679 AVG Validation Loss:0.737 AVG Training Acc 56.12 % AVG Validation Acc 45.49 %\n",
      "Epoch:90/200 AVG Training Loss:0.675 AVG Validation Loss:0.702 AVG Training Acc 57.35 % AVG Validation Acc 56.12 %\n",
      "Epoch:100/200 AVG Training Loss:0.673 AVG Validation Loss:0.688 AVG Training Acc 57.88 % AVG Validation Acc 59.35 %\n",
      "Epoch:110/200 AVG Training Loss:0.674 AVG Validation Loss:0.683 AVG Training Acc 57.34 % AVG Validation Acc 60.03 %\n",
      "Epoch:120/200 AVG Training Loss:0.673 AVG Validation Loss:0.680 AVG Training Acc 57.70 % AVG Validation Acc 61.37 %\n",
      "Epoch:130/200 AVG Training Loss:0.673 AVG Validation Loss:0.679 AVG Training Acc 57.37 % AVG Validation Acc 62.05 %\n",
      "Epoch:140/200 AVG Training Loss:0.672 AVG Validation Loss:0.679 AVG Training Acc 57.78 % AVG Validation Acc 61.91 %\n",
      "Epoch   146: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:150/200 AVG Training Loss:0.667 AVG Validation Loss:0.679 AVG Training Acc 58.75 % AVG Validation Acc 62.31 %\n",
      "Epoch:160/200 AVG Training Loss:0.670 AVG Validation Loss:0.678 AVG Training Acc 58.63 % AVG Validation Acc 62.31 %\n",
      "Epoch:170/200 AVG Training Loss:0.669 AVG Validation Loss:0.676 AVG Training Acc 58.34 % AVG Validation Acc 62.72 %\n",
      "Epoch:180/200 AVG Training Loss:0.669 AVG Validation Loss:0.675 AVG Training Acc 58.66 % AVG Validation Acc 63.26 %\n",
      "Epoch:190/200 AVG Training Loss:0.669 AVG Validation Loss:0.674 AVG Training Acc 57.90 % AVG Validation Acc 63.12 %\n",
      "Epoch:200/200 AVG Training Loss:0.667 AVG Validation Loss:0.673 AVG Training Acc 58.90 % AVG Validation Acc 63.93 %\n",
      "Split 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc10fc7b313341b29a2a79ffa2a9d46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10/200 AVG Training Loss:0.555 AVG Validation Loss:2.290 AVG Training Acc 77.59 % AVG Validation Acc 14.94 %\n",
      "Epoch:20/200 AVG Training Loss:0.654 AVG Validation Loss:3.060 AVG Training Acc 65.49 % AVG Validation Acc 14.94 %\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:30/200 AVG Training Loss:0.893 AVG Validation Loss:1.188 AVG Training Acc 49.91 % AVG Validation Acc 14.94 %\n",
      "Epoch:40/200 AVG Training Loss:0.701 AVG Validation Loss:0.794 AVG Training Acc 48.79 % AVG Validation Acc 16.42 %\n",
      "Epoch:50/200 AVG Training Loss:0.695 AVG Validation Loss:0.785 AVG Training Acc 51.59 % AVG Validation Acc 15.48 %\n",
      "Epoch:60/200 AVG Training Loss:0.685 AVG Validation Loss:0.802 AVG Training Acc 55.00 % AVG Validation Acc 18.57 %\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:70/200 AVG Training Loss:0.677 AVG Validation Loss:0.752 AVG Training Acc 55.82 % AVG Validation Acc 46.70 %\n",
      "Epoch:80/200 AVG Training Loss:0.673 AVG Validation Loss:0.722 AVG Training Acc 57.50 % AVG Validation Acc 53.84 %\n",
      "Epoch:90/200 AVG Training Loss:0.669 AVG Validation Loss:0.709 AVG Training Acc 58.47 % AVG Validation Acc 56.12 %\n",
      "Epoch:100/200 AVG Training Loss:0.671 AVG Validation Loss:0.707 AVG Training Acc 58.96 % AVG Validation Acc 56.93 %\n",
      "Epoch:110/200 AVG Training Loss:0.668 AVG Validation Loss:0.704 AVG Training Acc 58.98 % AVG Validation Acc 57.47 %\n",
      "Epoch:120/200 AVG Training Loss:0.669 AVG Validation Loss:0.704 AVG Training Acc 59.67 % AVG Validation Acc 57.87 %\n",
      "Epoch   121: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:130/200 AVG Training Loss:0.666 AVG Validation Loss:0.703 AVG Training Acc 59.77 % AVG Validation Acc 58.14 %\n",
      "Epoch:140/200 AVG Training Loss:0.669 AVG Validation Loss:0.702 AVG Training Acc 59.16 % AVG Validation Acc 58.28 %\n",
      "Epoch:150/200 AVG Training Loss:0.667 AVG Validation Loss:0.701 AVG Training Acc 59.76 % AVG Validation Acc 58.28 %\n",
      "Epoch:160/200 AVG Training Loss:0.667 AVG Validation Loss:0.700 AVG Training Acc 59.52 % AVG Validation Acc 58.28 %\n",
      "Epoch:170/200 AVG Training Loss:0.666 AVG Validation Loss:0.700 AVG Training Acc 59.38 % AVG Validation Acc 58.55 %\n",
      "Epoch:180/200 AVG Training Loss:0.667 AVG Validation Loss:0.699 AVG Training Acc 59.78 % AVG Validation Acc 58.82 %\n",
      "Epoch:190/200 AVG Training Loss:0.666 AVG Validation Loss:0.699 AVG Training Acc 59.76 % AVG Validation Acc 58.95 %\n",
      "Epoch:200/200 AVG Training Loss:0.666 AVG Validation Loss:0.698 AVG Training Acc 60.11 % AVG Validation Acc 59.08 %\n",
      "exam_gifted\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0662194c2ce54274bfb379c822285939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c4945ebce646928841b11e561507cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Best Accuracy found: 27.69%\n",
      "Epoch: 1\n",
      "Epoch:10/200 AVG Training Loss:0.557 AVG Validation Loss:4.057 AVG Training Acc 74.70 % AVG Validation Acc 27.69 %\n",
      "Epoch:20/200 AVG Training Loss:0.671 AVG Validation Loss:1.401 AVG Training Acc 62.92 % AVG Validation Acc 27.69 %\n",
      "Epoch:30/200 AVG Training Loss:0.671 AVG Validation Loss:4.202 AVG Training Acc 75.19 % AVG Validation Acc 27.69 %\n",
      "New Best Accuracy found: 27.82%\n",
      "Epoch: 32\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch:40/200 AVG Training Loss:0.825 AVG Validation Loss:0.955 AVG Training Acc 50.00 % AVG Validation Acc 27.69 %\n",
      "Epoch:50/200 AVG Training Loss:0.700 AVG Validation Loss:0.736 AVG Training Acc 48.01 % AVG Validation Acc 27.82 %\n",
      "Epoch:60/200 AVG Training Loss:0.699 AVG Validation Loss:0.733 AVG Training Acc 47.63 % AVG Validation Acc 27.82 %\n",
      "Epoch:70/200 AVG Training Loss:0.699 AVG Validation Loss:0.730 AVG Training Acc 47.60 % AVG Validation Acc 27.82 %\n",
      "Epoch:80/200 AVG Training Loss:0.698 AVG Validation Loss:0.728 AVG Training Acc 47.20 % AVG Validation Acc 27.82 %\n",
      "Epoch:90/200 AVG Training Loss:0.698 AVG Validation Loss:0.727 AVG Training Acc 47.55 % AVG Validation Acc 27.69 %\n",
      "Epoch:100/200 AVG Training Loss:0.697 AVG Validation Loss:0.726 AVG Training Acc 48.25 % AVG Validation Acc 27.55 %\n",
      "Epoch   109: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:110/200 AVG Training Loss:0.694 AVG Validation Loss:0.726 AVG Training Acc 50.94 % AVG Validation Acc 28.09 %\n",
      "New Best Accuracy found: 28.09%\n",
      "Epoch: 110\n",
      "New Best Accuracy found: 28.23%\n",
      "Epoch: 111\n",
      "New Best Accuracy found: 28.49%\n",
      "Epoch: 112\n",
      "New Best Accuracy found: 28.76%\n",
      "Epoch: 113\n",
      "New Best Accuracy found: 29.03%\n",
      "Epoch: 114\n",
      "New Best Accuracy found: 30.11%\n",
      "Epoch: 118\n",
      "New Best Accuracy found: 30.38%\n",
      "Epoch: 119\n",
      "Epoch:120/200 AVG Training Loss:0.692 AVG Validation Loss:0.712 AVG Training Acc 51.05 % AVG Validation Acc 30.78 %\n",
      "New Best Accuracy found: 30.78%\n",
      "Epoch: 120\n",
      "New Best Accuracy found: 31.05%\n",
      "Epoch: 122\n",
      "New Best Accuracy found: 31.32%\n",
      "Epoch: 123\n",
      "New Best Accuracy found: 31.45%\n",
      "Epoch: 124\n",
      "New Best Accuracy found: 31.59%\n",
      "Epoch: 127\n",
      "New Best Accuracy found: 31.72%\n",
      "Epoch: 128\n",
      "Epoch:130/200 AVG Training Loss:0.691 AVG Validation Loss:0.706 AVG Training Acc 50.77 % AVG Validation Acc 31.59 %\n",
      "New Best Accuracy found: 31.99%\n",
      "Epoch: 131\n",
      "New Best Accuracy found: 32.12%\n",
      "Epoch: 133\n",
      "New Best Accuracy found: 32.26%\n",
      "Epoch: 135\n",
      "New Best Accuracy found: 32.39%\n",
      "Epoch: 136\n",
      "New Best Accuracy found: 32.53%\n",
      "Epoch: 137\n",
      "New Best Accuracy found: 32.66%\n",
      "Epoch: 138\n",
      "New Best Accuracy found: 32.80%\n",
      "Epoch: 139\n",
      "Epoch:140/200 AVG Training Loss:0.690 AVG Validation Loss:0.702 AVG Training Acc 50.77 % AVG Validation Acc 33.20 %\n",
      "New Best Accuracy found: 33.20%\n",
      "Epoch: 140\n",
      "New Best Accuracy found: 33.33%\n",
      "Epoch: 142\n",
      "New Best Accuracy found: 34.01%\n",
      "Epoch: 143\n",
      "New Best Accuracy found: 34.27%\n",
      "Epoch: 145\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(course_programs.keys()):\n",
    "    \n",
    "    print(i)\n",
    "    threshold_dict = {} #dict to store information in for each threshold\n",
    "    data = deepcopy(course_programs[i])\n",
    "    \n",
    "    data.set_index(['course_encoding', 'userid'], drop = True, inplace = True)\n",
    "    data.fillna(0, inplace = True)\n",
    "    \n",
    "    #set X and Y columns\n",
    "    X = data[data.columns[:25]] #different timesteps\n",
    "    y = data[data.columns[-4:]] #the 4 different putative targets\n",
    "    \n",
    "    for k in tqdm(targets):\n",
    "        print(k)\n",
    "        \n",
    "        #Start with train test split\n",
    "        X_train_val, X_test, y_train_val, y_test, = train_test_split(\n",
    "                                    X,\n",
    "                                   y[k], #replace when going for multi-target \n",
    "                                   test_size = 0.20,\n",
    "                                   random_state = 15,\n",
    "                                   shuffle=True,\n",
    "                                   stratify = y[k] #replace when going for multi-target\n",
    "                                    )\n",
    "        \n",
    "        #create dict to store fold performance\n",
    "        foldperf={}\n",
    "        \n",
    "        #reset \"best accuracy for treshold i and target k\"\n",
    "        best_accuracy = 0\n",
    "\n",
    "        #make train_val split\n",
    "        for fold, (train_idx,val_idx) in tqdm(enumerate(splits.split(X_train_val, y_train_val))):\n",
    "\n",
    "            print('Split {}'.format(fold + 1))\n",
    "            \n",
    "            #make split between train and Val\n",
    "            X_train, y_train = X_train_val.iloc[train_idx], y_train_val.iloc[train_idx]\n",
    "            X_val, y_val = X_train_val.iloc[val_idx], y_train_val.iloc[val_idx]\n",
    "            \n",
    "            #apply SMOTE to training split\n",
    "            over = SMOTE()\n",
    "            X_train, y_train = over.fit_resample(X_train, y_train)\n",
    "            \n",
    "            #apply scaling after \n",
    "            X_train, X_val = normalize(X_train, X_val, 'Standard')\n",
    "            \n",
    "            #second, convert everything to pytorch tensor - we will convert to tensor dataset and \n",
    "            X_train_tensors = Variable(torch.Tensor(X_train.values))\n",
    "            X_val_tensors = Variable(torch.Tensor(X_val.values))\n",
    "\n",
    "            y_train_tensors = Variable(torch.Tensor(y_train.values))\n",
    "            y_val_tensors = Variable(torch.Tensor(y_val.values)) \n",
    "\n",
    "            #reshaping to rows, timestamps, features \n",
    "            X_train_tensors = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], X_train_tensors.shape[1], 1))\n",
    "            X_val_tensors = torch.reshape(X_val_tensors,  (X_val_tensors.shape[0], X_val_tensors.shape[1], 1))\n",
    "        \n",
    "            #convert y tensors to format longtensor\n",
    "            y_train_tensors = y_train_tensors.type(torch.cuda.LongTensor)\n",
    "            y_val_tensors = y_val_tensors.type(torch.cuda.LongTensor)\n",
    "            \n",
    "            #create Tensor Datasets and dataloaders for both Train and Val\n",
    "            train_dataset = TensorDataset(X_train_tensors, y_train_tensors)\n",
    "            val_dataset = TensorDataset(X_val_tensors, y_val_tensors)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "            #creates new model for each \n",
    "            model = LSTM_Uni(num_classes, input_size, hidden_size, num_layers, X_train_tensors.shape[1]).to('cuda') #our lstm class\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "            scheduler = ReduceLROnPlateau(optimizer, \n",
    "                                  'min', \n",
    "                                  patience = 10,\n",
    "                                  cooldown = 20,\n",
    "                                 verbose = True)\n",
    "    \n",
    "            history = {'train_loss': [], 'val_loss': [],'train_acc':[],'val_acc':[], 'precision': [],\n",
    "                      'recall' : [], 'auroc': []}\n",
    "\n",
    "            for epoch in tqdm(range(num_epochs)):\n",
    "                train_loss, train_correct=train_epoch(model,train_loader,criterion,optimizer)\n",
    "                val_loss, val_correct, precision, recall, auroc = valid_epoch(model,val_loader,criterion)\n",
    "\n",
    "                train_loss = train_loss / len(train_loader.sampler)\n",
    "                train_acc = train_correct / len(train_loader.sampler) * 100\n",
    "                val_loss = val_loss / len(val_loader.sampler)\n",
    "                val_acc = val_correct / len(val_loader.sampler) * 100\n",
    "        \n",
    "        \n",
    "                if (epoch+1) % 10 == 0: \n",
    "                    print(\"Epoch:{}/{} AVG Training Loss:{:.3f} AVG Validation Loss:{:.3f} AVG Training Acc {:.2f} % AVG Validation Acc {:.2f} %\".format(epoch + 1,\n",
    "                                                                                                             num_epochs,\n",
    "                                                                                                             train_loss,\n",
    "                                                                                                             val_loss,\n",
    "                                                                                                             train_acc,\n",
    "                                                                                                             val_acc))\n",
    "                history['train_loss'].append(train_loss)\n",
    "                history['val_loss'].append(val_loss)\n",
    "                history['train_acc'].append(train_acc)\n",
    "                history['val_acc'].append(val_acc)\n",
    "                history['precision'].append(precision)\n",
    "                history['recall'].append(recall)\n",
    "                history['auroc'].append(auroc)\n",
    "                scheduler.step(val_loss)\n",
    "    \n",
    "                if val_acc > best_accuracy:\n",
    "            \n",
    "                #replace best accuracy and save best model\n",
    "                    print(f'New Best Accuracy found: {val_acc:.2f}%\\nEpoch: {epoch + 1}')\n",
    "                    best_accuracy = val_acc\n",
    "                    best = deepcopy(model)\n",
    "                    curr_epoch = epoch + 1\n",
    "                    \n",
    "            #store fold performance\n",
    "            foldperf['fold{}'.format(fold+1)] = history\n",
    "        \n",
    "        #saves fold performance for target \n",
    "        threshold_dict[k] = pd.DataFrame.from_dict(foldperf, orient='index') # convert dict to dataframe\n",
    "        \n",
    "        #explode to get eacxh epoch as a row\n",
    "        threshold_dict[k] = threshold_dict[k].explode(list(threshold_dict[k].columns))\n",
    "        torch.save(best,f\"../Models/{i}/SMOTE_Nova_IMS_relative_clicks_best_{k}_{curr_epoch}_epochs.h\")\n",
    "        \n",
    "    # from pandas.io.parsers import ExcelWriter\n",
    "    with pd.ExcelWriter(f\"../Data/Modeling Stage/Results/IMS/Clicks_duration_relative/SMOTE_25_splits_{i}_{replicas}_replicas.xlsx\") as writer:  \n",
    "        for sheet in targets:\n",
    "                threshold_dict[sheet].to_excel(writer, sheet_name=str(sheet))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
